{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Post-Incident Reviews","text":"<p>Welcome to the internal Post-Incident Review (PIR) documentation site. This site contains detailed analyses of system incidents, root cause investigations, and lessons learned.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>Post-incident reviews are critical for:</p> <ul> <li>Learning from failures - Understanding what went wrong and why</li> <li>Preventing recurrence - Implementing safeguards and preventive measures</li> <li>Improving systems - Identifying architectural and operational improvements</li> <li>Knowledge sharing - Building team expertise and institutional memory</li> </ul>"},{"location":"#recent-incidents","title":"Recent Incidents","text":""},{"location":"#2026","title":"2026","text":"<ul> <li>2026-02-22 - Radarr Outage Due to OpenEBS Jiva Replica Divergence</li> <li>Severity: High</li> <li>Duration: ~16h30m silent failure + ~47m active recovery (~20h47m total outage)</li> <li> <p>Summary: Radarr became completely unavailable when all three OpenEBS Jiva storage replicas simultaneously entered CrashLoopBackOff following an ungraceful shutdown during an active rebuild, leaving the iSCSI-backed PVC unmountable due to ext4 journal corruption</p> </li> <li> <p>2026-01-06 - Cascading Kubernetes Cluster Failures</p> </li> <li>Severity: Critical</li> <li>Duration: ~8 hours (Phase 1-2) + 16.5 hours (Phase 3) + 12+ hours (Phase 5)</li> <li>Summary: Multi-phase cascading failure across microk8s cluster spanning 4 days, involving node reboots, kubelet failures, disk exhaustion, storage issues, job controller corruption, and container runtime corruption</li> </ul>"},{"location":"#pir-structure","title":"PIR Structure","text":"<p>Each post-incident review follows a standard structure:</p> <ol> <li>Executive Summary - High-level overview of the incident</li> <li>Timeline - Detailed chronological sequence of events</li> <li>Root Causes - Analysis of underlying issues</li> <li>Impact - Affected services, duration, and scope</li> <li>Resolution Steps - Actions taken to resolve the incident</li> <li>Verification - Confirmation of service restoration</li> <li>Preventive Measures - Immediate and long-term improvements</li> <li>Lessons Learned - Key takeaways and insights</li> <li>Action Items - Specific follow-up tasks with owners</li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>When creating a new PIR document:</p> <ol> <li>Use the naming convention: <code>YYYY-MM-DD-brief-description.md</code></li> <li>Place documents in the <code>docs/incidents/</code> directory</li> <li>Update the <code>mkdocs.yml</code> navigation section</li> <li>Follow the standard PIR structure template</li> <li>Include relevant technical details, commands, and verification steps</li> </ol>"},{"location":"#navigation","title":"Navigation","text":"<p>Use the navigation menu to browse incidents by date or search for specific topics using the search functionality.</p>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/","title":"Post Incident Review: Cascading Kubernetes Cluster Failures","text":"<p>Date: 2026-01-06 Duration: ~8 hours (estimated 09:00 - 17:00 AEST) Severity: Critical (Complete cluster instability, multiple service outages) Status: Resolved</p>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#executive-summary","title":"Executive Summary","text":"<p>A cascading failure across the microk8s Kubernetes cluster began with unplanned node reboots, leading to widespread kubelet failures, disk exhaustion, controller corruption, and ultimately service outages. The incident progressed through five distinct phases spanning January 6-9, 2026:</p> <p>Phase 1 (2026-01-06 09:00-12:30): Cascading node failures caused kubelet hangs on all three nodes due to disk pressure (97-100% usage), audit buffer overload, and orphaned pod accumulation. The cluster reached a critical state where pods could not be scheduled, started, or terminated. 571 orphaned GitHub Actions runner pods and 22 stuck OpenEBS replica pods contributed to resource exhaustion.</p> <p>Phase 2 (2026-01-06 12:30-15:35): After stabilizing node operations, secondary issues emerged: OpenEBS Jiva volume snapshot accumulation (1011+ snapshots), ingress controller endpoint caching failures, and volume capacity exhaustion. Multiple media services (Sonarr, Radarr, Overseerr) became inaccessible.</p> <p>Phase 3 (2026-01-08 02:00-18:25): Job controller corruption prevented all cluster-wide job creation for 16.5 hours. Required nuclear option (cluster restart with dqlite backup) to resolve persistent database state corruption originating from Phase 1.</p> <p>Phase 4 (2026-01-08 19:00-19:45): ArgoCD application recovery required manual finalizer removal and configuration fixes for GitHub Actions runner controllers and LinkAce cronjob.</p> <p>Phase 5 (2026-01-09 09:50-09:55): k8s01 container runtime corruption recurred 48+ hours after Phase 3 nuclear option, demonstrating that cluster restart cleared cluster-global state but not node-local container runtime issues. 4 runner pods stuck Pending for 12+ hours due to silent failure pattern.</p> <p>Resolution required systematic intervention across multiple infrastructure layers: node recovery, disk cleanup, pod force-deletion, storage subsystem repair, ingress refresh, database backup/restart, and multiple node-local container runtime restarts. All services restored to full functionality with complete volume replication (3/3 replicas).</p>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#timeline-aest-utc10","title":"Timeline (AEST - UTC+10)","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-1-cascading-node-and-kubelet-failures","title":"Phase 1: Cascading Node and Kubelet Failures","text":"Time Event ~09:00 INCIDENT START: Unplanned node reboots across k8s01, k8s02, k8s03 (likely power event or scheduled maintenance) 09:15-09:30 Cluster returns online but exhibits severe instability: pods not scheduling, not starting, not terminating 09:30-10:00 Initial diagnostics: Control plane components healthy, scheduler functioning, but kubelets not processing assigned pods 10:00-10:15 Identified k8s02 kubelet hung: pods assigned by scheduler but never reaching ContainerCreating state 10:15-10:20 RESOLUTION 1.1: Restarted kubelite on k8s02 (<code>systemctl restart snap.microk8s.daemon-kubelite</code>) 10:20-10:30 k8s01 kubelet repeatedly crashing: \"Kubelet stopped posting node status\" within minutes of restart 10:30-10:45 Root cause analysis k8s01: Disk at 97% usage + audit buffer overload (\"audit buffer queue blocked\" errors) 10:45-11:00 Database lock errors in kine (etcd replacement): \"database is locked\" preventing state updates 11:00-11:15 k8s03 diagnostics: Disk at 100% capacity with garbage collection failures 11:15-11:30 Discovered 571 orphaned GitHub Actions runner pods in ci namespace (deployment scaled to 0 but pods remained) 11:30-11:45 RESOLUTION 1.2: Disk cleanup on k8s01 (container images, logs) reducing from 97% \u2192 87% usage 11:45-12:00 RESOLUTION 1.3: Disk cleanup on k8s03 reducing from 100% \u2192 81% usage 12:00-12:15 k8s01 kubelet stabilized after disk cleanup, node maintaining Ready status 12:15-12:20 Deleted RunnerDeployment and HorizontalRunnerAutoscaler (GitHub Actions runner controller orphaned) 12:20-12:25 Force-deleted 22 OpenEBS replica pods stuck in Terminating state 12:25-12:30 Began aggressive force-deletion of 571 runner pods in batches (Pending, ContainerStatusUnknown, StartError, Completed)"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-2-storage-and-ingress-service-outages","title":"Phase 2: Storage and Ingress Service Outages","text":"Time Event ~12:30 PHASE 2 START: User reports 504 Gateway Timeout errors for Sonarr at https://sonarr.int.pgmac.net/ 12:30-12:45 Initial investigation: Examined ingress controller logs showing upstream timeouts to pods at old IP addresses (10.1.236.34:8989 for Sonarr, etc.) 12:45-13:00 Root cause analysis: Discovered Radarr pod in CrashLoopBackOff with \"No space left on device\" error. Sonarr pod Pending on k8s01 node. 13:00-13:15 Volume analysis: Identified OpenEBS Jiva volumes with excessive snapshots (1011 vs 500 threshold) affecting Radarr, Sonarr, and Overseerr 13:15-13:30 Node troubleshooting: Identified k8s01 node unable to start new containers despite being healthy (residual kubelet issues from Phase 1) 13:30-13:35 RESOLUTION 2.1: Restarted microk8s on k8s01, resolving pod scheduling issues 13:35-14:00 Snapshot cleanup: Triggered manual Jiva snapshot cleanup job (jiva-snapshot-cleanup-manual) 13:52 Cleanup job started processing Overseerr volume (pvc-05e03b60) 13:57 Sonarr volume (pvc-17e6e808) cleanup completed 14:10 RESOLUTION 2.2: Restarted all 3 ingress controller pods to clear stale endpoint cache 14:11 SERVICE RESTORED: Sonarr accessible at https://sonarr.int.pgmac.net/ (200 OK responses) 14:15 Overseerr confirmed accessible (200 OK responses) 14:20 Radarr volume (pvc-311bef00) cleanup completed 14:25 Radarr pod still crashing: volume at 100% capacity (958M/974M used) 14:28 RESOLUTION 2.3: Cleared 49M of old backups from Radarr volume, reducing to 95% usage 14:30 SERVICE RESTORED: Radarr accessible at https://radarr.int.pgmac.net/ 14:35 Identified 8-9 Jiva replica pods stuck in Pending state on k8s03 (residual from Phase 1) ~15:30 RESOLUTION 2.4: Restarted microk8s on k8s03, resolving all Pending replica pods 15:35 INCIDENT END: All services operational, all replicas running (3/3), no problematic pods"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#cleanup-operations-parallel-with-phase-2","title":"Cleanup Operations (Parallel with Phase 2)","text":"Time Event 12:30-12:45 Force-deleted 299 Pending runner pods 12:45-13:00 Force-deleted 110 ContainerStatusUnknown runner pods 13:00-13:15 Force-deleted 58 StartError/RunContainerError runner pods 13:15-13:30 Force-deleted 79 Completed runner pods 13:30-14:00 Force-deleted final batch of 247 non-Running/non-Terminating runner pods 14:00 Runner pod cleanup substantially complete: 393 pods remain (128 Terminating, 18 Running, 247 deleted)"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-3-linkace-cronjob-controller-corruption-2026-01-08-0200-1825","title":"Phase 3: LinkAce CronJob Controller Corruption (2026-01-08 02:00-18:25)","text":"Time Event 2026-01-08 ~02:00 PHASE 3 START: LinkAce cronjob (<code>* * * * *</code> schedule) begins failing to create jobs successfully 02:00-05:00 Cronjob creates job objects but pods orphaned (parent job deleted before pod creation) 05:00-05:30 44+ jobs stuck in Running state (0/1 completions, 6min-11h old), 24+ pods Pending 05:30-06:00 Investigation reveals job controller stuck syncing deleted job <code>linkace-cronjob-29463021</code> 06:00-06:15 Job controller logs: \"syncing job: tracking status: jobs.batch not found\" errors 06:15-06:30 Cleanup attempts: Suspended cronjob, deleted orphaned pods, cleared stale active jobs 06:30-07:00 Restarted kubelite on k8s01, temporary improvement but orphaned job reference persists 07:00-08:00 Created dummy job with stale name and deleted properly, but new jobs still not creating pods 08:00-09:00 User added timeout configuration to ArgoCD manifest (activeDeadlineSeconds: 300, ttlSecondsAfterFinished: 120) 09:00-09:30 ArgoCD synced configuration successfully but cronjob deleted to recreate cleanly 09:30-10:00 ArgoCD failed to auto-recreate deleted cronjob despite OutOfSync status 10:00-10:30 Manually recreated cronjob, but job controller completely wedged (not creating pods for any jobs) 10:30-18:00 Self-healing attempted: waited 1.5 hours for TTL cleanup and active deadline enforcement - failed 18:00-18:05 Jobs created by cronjob but no pods spawned, active deadline not enforced (jobs 85+ min old still Running) 18:05-18:10 TTL cleanup not working (no jobs auto-deleted after completion) 18:10 DECISION: Nuclear option approved - etcd cleanup with cluster restart 18:12-18:15 RESOLUTION 3.1: Stopped MicroK8s on all 3 nodes (k8s01, k8s02, k8s03) 18:15 RESOLUTION 3.2: Backed up etcd/dqlite database to <code>/var/snap/microk8s/common/backup/etcd-backup-20260108-201540</code> 18:15-18:17 RESOLUTION 3.3: Restarted MicroK8s cluster, all nodes returned Ready 18:17-18:18 RESOLUTION 3.4: Force-deleted all stuck jobs and cronjob 18:18-18:19 RESOLUTION 3.5: Triggered ArgoCD sync to recreate cronjob with fresh state 18:19 Cronjob recreated successfully with all timeout settings applied 18:20-18:22 First job (<code>linkace-cronjob-29464462</code>) created successfully, completed in 7 seconds 18:22-18:25 TTL cleanup verified working: completed jobs auto-deleted after 2 minutes 18:25 INCIDENT END: Cronjob fully functional, no orphaned pods, all cleanup mechanisms working"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-4-argocd-application-recovery-2026-01-08-1900-1945","title":"Phase 4: ArgoCD Application Recovery (2026-01-08 ~19:00-19:45)","text":"Time Event 2026-01-08 ~19:00 PHASE 4 START: Investigation of 5 ArgoCD applications stuck OutOfSync or Progressing 19:00-19:05 Identified problematic applications: ci-tools (OutOfSync + Progressing), gharc-runners-pgmac-net-self-hosted (OutOfSync + Healthy), gharc-runners-pgmac-user-self-hosted (Synced + Progressing), hass (Synced + Progressing), linkace (OutOfSync + Healthy) 19:05-19:15 ci-tools investigation: Found child application <code>gharc-runners-pgmac-user-self-hosted</code> stuck with resources \"Pending deletion\" 19:15-19:18 RESOLUTION 4.1: Removed finalizers from 4 stuck resources (AutoscalingRunnerSet, ServiceAccount, Role, RoleBinding) using <code>kubectl patch --type json -p='[{\"op\": \"remove\", \"path\": \"/metadata/finalizers\"}]'</code> 19:18-19:20 Triggered ArgoCD sync for ci-tools, application became Synced + Healthy 19:20-19:25 gharc-runners-pgmac-net-self-hosted investigation: Found old listener resources with hash <code>754b578d</code> needing deletion 19:25-19:28 Deleted 3 old listener resources manually (ServiceAccount, Role, RoleBinding) 19:28-19:30 Discovered 6 runner pods stuck Pending for 44+ minutes (residual from Phase 3 job controller corruption) 19:30-19:32 Force-deleted 6 stuck runner pods: <code>pgmac-renovatebot-*</code> pods with PodScheduled=True but no containers created 19:32-19:33 Application status: OutOfSync + Healthy (acceptable due to ignoreDifferences configuration for AutoscalingListener, Role, RoleBinding) 19:33-19:35 gharc-runners-pgmac-user-self-hosted: Already deleted during ci-tools cleanup 19:35-19:37 hass investigation: Application self-resolved during investigation, showing Synced + Healthy (StatefulSet rollout completed) 19:37-19:40 linkace investigation: Found linkace-cronjob OutOfSync despite application Healthy, ArgoCD attempted 23 auto-heal operations 19:40-19:42 Root cause identified: LinkAce Helm chart doesn't support <code>backoffLimit</code> and <code>resources</code> configuration in cronjob 19:42-19:43 RESOLUTION 4.2: Edited <code>/Users/paulmacdonnell/pgmac/pgk8s/pgmac.net/media/templates/linkace.yaml</code> to remove unsupported fields (backoffLimit, resources block) 19:43 Kept critical timeout settings: startingDeadlineSeconds, activeDeadlineSeconds, ttlSecondsAfterFinished, history limits 19:43-19:44 Committed changes with message \"Remove unsupported LinkAce cronjob configuration\" 19:44 Git push rejected due to remote changes, used <code>git stash &amp;&amp; git pull --rebase &amp;&amp; git stash pop &amp;&amp; git push</code> 19:45 PHASE 4 END: All applications resolved or explained; 2 applications Synced + Healthy (ci-tools, hass), 2 applications OutOfSync + Healthy acceptable (gharc-runners-pgmac-net-self-hosted, linkace), 1 application deleted (gharc-runners-pgmac-user-self-hosted)"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-5-k8s01-container-runtime-corruption-recurrence-2026-01-09-0950-0955","title":"Phase 5: k8s01 Container Runtime Corruption Recurrence (2026-01-09 ~09:50-09:55)","text":"Time Event 2026-01-09 ~09:50 PHASE 5 START: Investigation revealed 4 runner pods in arc-runners namespace stuck in Pending state for 12+ hours 09:50-09:51 Identified all 4 Pending pods assigned to k8s01 node: self-hosted-l52x9-runner-2nnsr, -69qnv, -ls8c2, -w8mcd 09:51 Pod describe showed PodScheduled=True but no container initialization, no events generated (silent failure pattern from Phase 2/3) 09:51-09:52 Verified k8s01 node showing Ready status despite being unable to start new containers 09:52 Root cause identified: Container runtime state corruption on k8s01 (residual from Phase 1-3, not fully cleared by Phase 3 nuclear option) 09:52-09:53 Found 10 EphemeralRunner resources but only 4 pods exist (6 pgmac-slack-scores runners have no pods at all) 09:53 RESOLUTION 5.1: User restarted microk8s on k8s01 (<code>microk8s stop &amp;&amp; microk8s start</code>) 09:55 PHASE 5 END: All 4 Pending pods cleared, container runtime recovered"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#root-causes","title":"Root Causes","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-1-node-and-control-plane-failures","title":"Phase 1: Node and Control Plane Failures","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#11-cascading-node-reboots-primary-trigger","title":"1.1 Cascading Node Reboots (Primary Trigger)","text":"<ul> <li>Issue: All three nodes (k8s01, k8s02, k8s03) experienced unplanned reboots</li> <li>Likely cause: Power event, scheduled maintenance, or infrastructure issue</li> <li>Impact: Triggered cascade of secondary failures during recovery</li> <li>Why it cascaded:</li> <li>Simultaneous reboot prevented graceful pod migration</li> <li>etcd state (via kine) became inconsistent across nodes</li> <li>Container runtime state corrupted on restart</li> <li>Disk pressure accumulated during downtime (logs, audit buffers, orphaned containers)</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#12-k8s01-kubelet-crash-loop-critical","title":"1.2 k8s01 Kubelet Crash Loop (Critical)","text":"<ul> <li>Issue: Kubelet repeatedly crashing within minutes of restart</li> <li>Root causes:</li> <li>Disk exhaustion: 97% usage preventing kubelet operations</li> <li>Audit buffer overload: \"audit buffer queue blocked\" errors in logs</li> <li>Database locks: kine (etcd replacement) showing \"database is locked\" errors</li> <li>Impact: Node oscillating between Ready/NotReady, unable to start/stop pods</li> <li>Why it happened:</li> <li>Container image accumulation from 4+ years of operations</li> <li>Log rotation not keeping pace with audit log generation</li> <li>Kubelet requires &gt;10% free disk to function properly</li> <li>Resolution: Disk cleanup (97% \u2192 87%), container image removal, log pruning</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#13-k8s02-kubelet-process-hang-critical","title":"1.3 k8s02 Kubelet Process Hang (Critical)","text":"<ul> <li>Issue: Kubelet not processing newly assigned pods</li> <li>Symptoms: Pods assigned by scheduler (PodScheduled=True) but never reaching ContainerCreating</li> <li>Root cause: Kubelet process corrupted/hung after node reboot</li> <li>Impact: Entire node unable to start new containers despite reporting Ready status</li> <li>Resolution: kubelite service restart (<code>systemctl restart snap.microk8s.daemon-kubelite</code>)</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#14-k8s03-disk-exhaustion-critical","title":"1.4 k8s03 Disk Exhaustion (Critical)","text":"<ul> <li>Issue: Disk at 100% capacity</li> <li>Symptoms: \"Failed to garbage collect required amount of images. Attempted to free 13GB, but only found 0 bytes eligible to free\"</li> <li>Impact:</li> <li>Prevented container image pulls</li> <li>Blocked new pod scheduling</li> <li>Contributed to 8-9 Jiva replica pods stuck in Pending</li> <li>Resolution: User disk cleanup (100% \u2192 81%)</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#15-github-actions-runner-controller-orphaned-pods-secondary","title":"1.5 GitHub Actions Runner Controller Orphaned Pods (Secondary)","text":"<ul> <li>Issue: 571 orphaned runner pods remained after RunnerDeployment scaled to 0</li> <li>Affected namespace: ci</li> <li>Pod states: 299 Pending, 110 ContainerStatusUnknown, 79 Completed, 58 StartError, 25+ other stuck states</li> <li>Why it happened:</li> <li>RunnerDeployment controller existed but in Error state</li> <li>Pods orphaned from parent controller (reboot disrupted controller finalizers)</li> <li>No automated cleanup triggered despite 0 replicas</li> <li>Impact:</li> <li>Consumed scheduler resources attempting to place Pending pods</li> <li>Consumed API server resources with status updates</li> <li>Contributed to disk pressure (container image layers, logs)</li> <li>Resolution:</li> <li>Deleted RunnerDeployment and HorizontalRunnerAutoscaler</li> <li>Force-deleted 546+ pods in batches using <code>--force --grace-period=0 --wait=false</code></li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#16-openebs-replica-pods-stuck-terminating-secondary","title":"1.6 OpenEBS Replica Pods Stuck Terminating (Secondary)","text":"<ul> <li>Issue: 22 OpenEBS Jiva replica pods stuck in Terminating state</li> <li>Impact: Storage subsystem instability, prevented volume operations</li> <li>Resolution: Force-deleted all 22 pods</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-2-storage-and-ingress-failures","title":"Phase 2: Storage and Ingress Failures","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#21-openebs-jiva-snapshot-accumulation-primary","title":"2.1 OpenEBS Jiva Snapshot Accumulation (Primary)","text":"<ul> <li>Issue: Jiva volumes accumulated 1011 snapshots (threshold: 500)</li> <li>Affected Volumes:</li> <li>Radarr config (pvc-311bef00-1b89-4584-90f0-ae3772e30e09)</li> <li>Sonarr config (pvc-17e6e808-a9fc-4f64-b490-71deffdb81fd)</li> <li>Overseerr config (pvc-05e03b60-3ab7-41a0-9baf-3d0e291eed63)</li> <li>Multiple other Jiva volumes cluster-wide</li> <li>Impact: Radarr volume reached capacity (3GB physical snapshots in 1Gi volume), causing \"No space left on device\" errors</li> <li>Why it happened:</li> <li>Automated cleanup cronjob exists (<code>jiva-snapshot-cleanup</code>) running daily at 2 AM</li> <li>Connection to Phase 1: Node reboots and kubelet failures prevented cronjob pods from running for 2-3 days</li> <li>Snapshot accumulation rate exceeded cleanup frequency</li> <li>1011 snapshots suggests cronjob not executing successfully during Phase 1 disk/kubelet issues</li> <li>Resolution: Manual snapshot cleanup job triggered after node stabilization</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#22-residual-node-container-runtime-issues","title":"2.2 Residual Node Container Runtime Issues","text":"<ul> <li>k8s01 Node (post Phase 1 cleanup): Unable to start new containers despite node reporting Ready status</li> <li>Pods scheduled successfully but containers never initialized</li> <li>Affected: Sonarr pod, linkace-cronjob, cleanup job pod</li> <li>Connection to Phase 1: Kubelet state corruption persisted despite disk cleanup</li> <li> <p>Resolution: Full microk8s restart (not just kubelite)</p> </li> <li> <p>k8s03 Node: Similar container startup issues</p> </li> <li>8-9 Jiva replica pods stuck in Pending with PodScheduled=True but no container creation</li> <li>Connection to Phase 1: Disk exhaustion (100% \u2192 81%) required full cluster restart to clear runtime state</li> <li>Resolution: microk8s restart resolved all Pending pods</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#23-ingress-controller-stale-endpoint-cache","title":"2.3 Ingress Controller Stale Endpoint Cache","text":"<ul> <li>Issue: Nginx ingress controllers retained old pod IP addresses after pod restarts</li> <li>Example: Sonarr old IP 10.1.236.34:8989 vs new IP 10.1.73.92:8989</li> <li>Resulted in 504 Gateway Timeout errors despite healthy backend pods</li> <li>Why it happened: Pod IP changes during Phase 1 chaos not reflected in ingress controller endpoint cache</li> <li>Resolution: Restarting all 3 ingress controller pods refreshed endpoint cache</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#24-radarr-volume-capacity-secondary","title":"2.4 Radarr Volume Capacity (Secondary)","text":"<ul> <li>Issue: Radarr config PVC at 100% capacity (1Gi volume, 958M used)</li> <li>Breakdown:</li> <li>MediaCover directory: 837M (movie posters/artwork)</li> <li>Backups: 49M</li> <li>Database: 33M</li> <li>Other: ~39M</li> <li>Resolution: Cleared old backups freeing 49M (temporary fix to 95% usage)</li> <li>Long-term concern: Volume undersized for media library artwork</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-3-linkace-cronjob-controller-corruption-2026-01-08","title":"Phase 3: LinkAce CronJob Controller Corruption (2026-01-08)","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#31-job-controller-state-corruption-primary-critical","title":"3.1 Job Controller State Corruption (Primary - Critical)","text":"<ul> <li>Issue: Job controller stuck in error loop trying to sync deleted job <code>linkace-cronjob-29463021</code></li> <li>Symptoms:</li> <li>Jobs created by cronjob but pods never spawned</li> <li>Job objects showing \"Running\" with 0 Active/Succeeded/Failed pods</li> <li>No events generated for newly created jobs</li> <li>Controller logs: <code>\"syncing job: tracking status: adding uncounted pods to status: jobs.batch \\\"linkace-cronjob-29463021\\\" not found\"</code></li> <li>Impact: Complete failure of all job creation cluster-wide, not just LinkAce cronjob</li> <li>Why it happened:</li> <li>Connection to Phase 1: Job controller corruption originated from cascading failures on 2026-01-06</li> <li>Stale job reference persisted in controller's in-memory state after Phase 1 reboots</li> <li>Controller unable to clear orphaned reference without full cluster restart</li> <li>MicroK8s dqlite database retained corrupted job metadata</li> <li>Resolution: Nuclear option - cluster restart with dqlite backup</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#32-dqlite-database-state-corruption-primary","title":"3.2 Dqlite Database State Corruption (Primary)","text":"<ul> <li>Issue: MicroK8s dqlite database retained stale job references preventing controller recovery</li> <li>Symptoms:</li> <li>Job controller restart didn't resolve issue (state persisted in database)</li> <li>Manually recreating cronjob didn't clear corruption</li> <li>Creating dummy job with stale name and deleting didn't clear reference</li> <li>Multiple kubelite restarts across all nodes failed to resolve</li> <li>Impact: Self-healing mechanisms completely ineffective</li> <li>Why it happened:</li> <li>Phase 1 cascading failures (disk pressure, kubelet crashes) corrupted dqlite write operations</li> <li>Job deletion operations during Phase 1 chaos didn't complete atomically</li> <li>Dqlite state diverged across 3 nodes during simultaneous kubelet failures</li> <li>Resolution: Stopped cluster, backed up dqlite database, restarted to clear in-memory state</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#33-timeout-configuration-not-enforced-secondary","title":"3.3 Timeout Configuration Not Enforced (Secondary)","text":"<ul> <li>Issue: CronJob timeout settings not enforced despite proper configuration</li> <li>Settings Applied:</li> <li><code>activeDeadlineSeconds: 300</code> (5-minute job timeout)</li> <li><code>ttlSecondsAfterFinished: 120</code> (2-minute cleanup after completion)</li> <li><code>startingDeadlineSeconds: 60</code> (1-minute grace for job creation)</li> <li>Observed Behavior:</li> <li>Jobs remained Running for 85+ minutes despite 5-minute timeout</li> <li>Completed jobs not auto-deleted despite 2-minute TTL</li> <li>No timeout enforcement events generated</li> <li>Root Cause: Job controller corruption prevented processing of any job lifecycle events</li> <li>Impact: Self-healing timeline predictions completely invalid (expected 6-12 hours, actual: infinite)</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#34-argocd-auto-sync-failure-secondary","title":"3.4 ArgoCD Auto-Sync Failure (Secondary)","text":"<ul> <li>Issue: ArgoCD failed to auto-recreate deleted cronjob despite automated sync enabled</li> <li>Symptoms:</li> <li>Application showed OutOfSync status but no sync operation triggered</li> <li>Manual sync attempts via kubectl patch failed</li> <li>Hard refresh annotation didn't trigger recreation</li> <li>Impact: Required manual cronjob creation, delaying recovery</li> <li>Why it happened: ArgoCD controller may have been affected by broader job controller corruption</li> <li>Workaround: Manually created cronjob from template, ArgoCD eventually adopted it</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-4-argocd-application-recovery-2026-01-08","title":"Phase 4: ArgoCD Application Recovery (2026-01-08)","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#41-github-actions-runner-controller-finalizer-issues-primary","title":"4.1 GitHub Actions Runner Controller Finalizer Issues (Primary)","text":"<ul> <li>Issue: Child application <code>gharc-runners-pgmac-user-self-hosted</code> stuck with resources \"Pending deletion\"</li> <li>Affected resources: 4 resources with blocking finalizers</li> <li>AutoscalingRunnerSet: <code>pgmac-slack-scores</code></li> <li>ServiceAccount: <code>pgmac-slack-scores-gha-rs-no-permission</code></li> <li>Role: <code>pgmac-slack-scores-gha-rs-manager</code></li> <li>RoleBinding: <code>pgmac-slack-scores-gha-rs-manager</code></li> <li>Symptoms:</li> <li>Parent application <code>ci-tools</code> stuck OutOfSync + Progressing</li> <li>Resources marked for deletion but finalizers preventing cleanup</li> <li>ArgoCD unable to sync parent application due to child application state</li> <li>Impact: Blocked CI/CD application sync, prevented runner controller updates</li> <li>Why it happened:</li> <li>Connection to Phase 3: Job controller corruption prevented cleanup job from removing finalizers</li> <li>Resources created by parent application but child application deletion failed</li> <li>Finalizers intended to ensure graceful resource cleanup but became stuck</li> <li>Resolution: Removed finalizers manually using <code>kubectl patch --type json -p='[{\"op\": \"remove\", \"path\": \"/metadata/finalizers\"}]'</code></li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#42-github-actions-runner-controller-state-drift-secondary","title":"4.2 GitHub Actions Runner Controller State Drift (Secondary)","text":"<ul> <li>Issue: Old listener resources with hash <code>754b578d</code> remained after controller update</li> <li>Affected resources:</li> <li>ServiceAccount with old hash</li> <li>Role with old hash</li> <li>RoleBinding with old hash</li> <li>Symptoms:</li> <li>Application <code>gharc-runners-pgmac-net-self-hosted</code> showing OutOfSync + Healthy</li> <li>New resources created with different hash but old resources not deleted</li> <li>6 runner pods stuck Pending for 44+ minutes (residual from Phase 3)</li> <li>Impact: Resource accumulation, pod scheduling failures</li> <li>Why it happened:</li> <li>Connection to Phase 3: Pods created during job controller corruption remained stuck</li> <li>ArgoCD <code>ignoreDifferences</code> configuration for AutoscalingListener, Role, RoleBinding masked drift</li> <li>Controller update didn't clean up old resources automatically</li> <li>Resolution: Manually deleted old listener resources, force-deleted 6 stuck runner pods</li> <li>Acceptable state: OutOfSync + Healthy is expected due to ignoreDifferences configuration</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#43-linkace-helm-chart-configuration-drift-primary","title":"4.3 LinkAce Helm Chart Configuration Drift (Primary)","text":"<ul> <li>Issue: LinkAce cronjob showing OutOfSync despite application Healthy, ArgoCD attempted 23 auto-heal operations</li> <li>Unsupported configuration: LinkAce Helm chart doesn't support these cronjob fields:</li> <li><code>backoffLimit: 0</code></li> <li><code>resources</code> block (limits/requests)</li> <li>Symptoms:</li> <li>ArgoCD continuously detecting drift between desired and actual state</li> <li>Auto-heal operations failing to converge</li> <li>Application Healthy but OutOfSync persisting</li> <li>Impact: ArgoCD resource consumption, false positive OutOfSync alerts</li> <li>Why it happened:</li> <li>Upstream Helm chart doesn't expose all CronJob configuration options</li> <li>ArgoCD manifest specified fields not supported by chart templates</li> <li>GitOps configuration drift from Helm chart capabilities</li> <li>Resolution: Removed unsupported fields from ArgoCD manifest, kept critical timeout settings</li> <li>Acceptable state: Minor field drift from Helm chart is expected and acceptable</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#44-home-assistant-application-self-healing-none","title":"4.4 Home Assistant Application Self-Healing (None)","text":"<ul> <li>Issue: Application showed Synced + Progressing initially</li> <li>Root cause: StatefulSet rollout in progress during investigation</li> <li>Resolution: Self-resolved as rollout completed</li> <li>Impact: None, normal operational state</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-5-k8s01-container-runtime-corruption-recurrence-2026-01-09","title":"Phase 5: k8s01 Container Runtime Corruption Recurrence (2026-01-09)","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#51-persistent-container-runtime-corruption-on-k8s01-critical","title":"5.1 Persistent Container Runtime Corruption on k8s01 (Critical)","text":"<ul> <li>Issue: k8s01 node unable to start new containers 48+ hours after Phase 3 nuclear option</li> <li>Symptoms:</li> <li>4 runner pods stuck in Pending state for 12+ hours</li> <li>All 4 pods assigned to k8s01 node</li> <li>Pods showing PodScheduled=True but no container initialization</li> <li>No events generated (silent failure pattern identical to Phase 2)</li> <li>Node reporting Ready status despite being unable to start containers</li> <li>6 additional EphemeralRunner resources with no corresponding pods</li> <li>Impact: Complete inability to start new workloads on k8s01, affecting GitHub Actions runners</li> <li>Why it happened:</li> <li>Connection to Phase 1: Container runtime corruption originated from Phase 1 disk pressure (97% usage) and kubelet crashes</li> <li>Connection to Phase 2: k8s01 required microk8s restart during Phase 2 (Resolution 2.1) for same symptoms</li> <li>Connection to Phase 3: Phase 3 nuclear option (cluster-wide restart) only cleared dqlite/controller state, not node-local container runtime corruption</li> <li>Container runtime (containerd) state diverged from kubelet state</li> <li>Corruption persisted across cluster restarts because it was node-local, not cluster-global</li> <li>12+ hour delay in detection suggests corruption was dormant until new workloads attempted to schedule</li> <li>Resolution: Full microk8s restart on k8s01 (<code>microk8s stop &amp;&amp; microk8s start</code>)</li> <li>Key finding: Nuclear option (cluster restart) insufficient to clear node-local container runtime issues</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#52-detection-gap-for-node-local-failures-secondary","title":"5.2 Detection Gap for Node-Local Failures (Secondary)","text":"<ul> <li>Issue: 12+ hour delay between pod creation and detection of container startup failure</li> <li>Why it happened:</li> <li>No monitoring for silent pod failures (PodScheduled=True but no container creation)</li> <li>No alerts for pods stuck in Pending with node assignment</li> <li>No synthetic pod startup tests on individual nodes</li> <li>Kubernetes node status (Ready) doesn't reflect container runtime health</li> <li>Impact: Extended downtime for affected workloads without visibility</li> <li>Resolution: Manual investigation prompted by user observation</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#impact","title":"Impact","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#services-affected","title":"Services Affected","text":"<p>Phase 1:</p> <ul> <li>Entire cluster: Widespread pod scheduling and lifecycle failures</li> <li>GitHub Actions: Self-hosted runners completely non-functional (571 pods stuck)</li> <li>OpenEBS storage: 22 replica pods unavailable, volume operations degraded</li> <li>All services: Intermittent availability as pods failed to start/stop properly</li> </ul> <p>Phase 2:</p> <ul> <li>Sonarr: Unavailable via https://sonarr.int.pgmac.net/ (504 errors)</li> <li>Radarr: Pod crashing, completely unavailable</li> <li>Overseerr: Intermittent 504 timeout errors</li> </ul> <p>Phase 3:</p> <ul> <li>LinkAce cronjob: Complete failure to execute scheduled tasks (every minute)</li> <li>All Kubernetes Jobs: Job controller corruption affected cluster-wide job creation</li> <li>LinkAce scheduled tasks: Backup creation, link validation, database cleanup not executing</li> <li>ArgoCD: Auto-sync mechanism failed for deleted resources</li> </ul> <p>Phase 4:</p> <ul> <li>CI/CD Applications: ci-tools stuck OutOfSync + Progressing, preventing runner controller updates</li> <li>GitHub Actions Runners: 6 runner pods stuck Pending for 44+ minutes (residual from Phase 3)</li> <li>ArgoCD GitOps: Multiple applications showing false OutOfSync status consuming resources</li> <li>LinkAce Application: 23 failed auto-heal attempts creating noise in ArgoCD</li> </ul> <p>Phase 5:</p> <ul> <li>GitHub Actions Runners: 4 runner pods stuck Pending for 12+ hours on k8s01 (pgmac-slack-scores runners unable to spawn)</li> <li>k8s01 Node: Complete inability to start new containers despite Ready status</li> <li>EphemeralRunner Resources: 6 additional runners with no corresponding pods (silent failure)</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#duration","title":"Duration","text":"<ul> <li>Total incident duration: ~8 hours (09:00 - 17:00 AEST 2026-01-06) + 16.5 hours (Phase 3, 2026-01-08) + 0.75 hours (Phase 4, 2026-01-08) + 12+ hours (Phase 5, 2026-01-09)</li> <li>Phase 1 critical period: ~3.5 hours (09:00 - 12:30 2026-01-06)</li> <li>Phase 2 service outages: ~3 hours (12:30 - 15:35 2026-01-06)</li> <li>Phase 3 cronjob failure: ~16.5 hours (02:00 - 18:25 2026-01-08)</li> <li>Phase 4 ArgoCD recovery: ~45 minutes (19:00 - 19:45 2026-01-08)</li> <li>Phase 5 container runtime corruption: ~5 minutes active recovery (09:50 - 09:55 2026-01-09), but 12+ hours of silent failure</li> <li>Sonarr downtime: ~1.5 hours</li> <li>Radarr downtime: ~2.5 hours</li> <li>Overseerr impact: Intermittent throughout Phase 2</li> <li>GitHub Actions runners: ~8 hours (full Phase 1-2 duration) + 45 minutes (Phase 4) + 12+ hours (Phase 5)</li> <li>LinkAce scheduled tasks: ~16.5 hours (complete failure to execute)</li> <li>CI/CD deployments: Blocked during Phase 4 investigation (~45 minutes) + 12+ hours (Phase 5 k8s01 node failure)</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#scope","title":"Scope","text":"<ul> <li>Infrastructure: All 3 nodes compromised at various points</li> <li>User-facing: All web access to media management services</li> <li>Internal: Home Assistant integrations unable to query service APIs</li> <li>CI/CD: GitHub Actions self-hosted runners completely unavailable</li> <li>Monitoring: Nagios health checks failing across multiple services</li> <li>Storage: OpenEBS volume operations degraded during Phase 1</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#resolution-steps-taken","title":"Resolution Steps Taken","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-1-node-and-kubelet-recovery","title":"Phase 1: Node and Kubelet Recovery","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#1-k8s02-kubelet-restart","title":"1. k8s02 Kubelet Restart","text":"<pre><code># On k8s02 node\nsudo systemctl restart snap.microk8s.daemon-kubelite\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#2-k8s01-disk-cleanup-and-stabilization","title":"2. k8s01 Disk Cleanup and Stabilization","text":"<pre><code># On k8s01 node\n# Removed unused container images\nmicrok8s ctr images rm &lt;image-id&gt;...\n\n# Cleaned container logs (methods vary)\n# Removed old/stopped containers\n# Result: 97% \u2192 87% disk usage\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#3-k8s03-disk-cleanup","title":"3. k8s03 Disk Cleanup","text":"<pre><code># On k8s03 node\n# Similar cleanup process\n# Result: 100% \u2192 81% disk usage\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#4-runner-controller-cleanup","title":"4. Runner Controller Cleanup","text":"<pre><code># Deleted orphaned controller resources\nkubectl delete runnerdeployment pgmac.pgmac-runnerdeploy -n ci --context pvek8s\nkubectl delete horizontalrunnerautoscaler pgmac-pgmac-runnerdeploy-autoscaler -n ci --context pvek8s\n\n# Force-deleted 546+ orphaned runner pods in batches\n# Pending pods (299)\nkubectl get pods -n ci --context pvek8s --no-headers | \\\n  grep \"pgmac.pgmac-runnerdeploy\" | grep \"Pending\" | \\\n  awk '{print $1}' | xargs -I {} kubectl delete pod {} -n ci \\\n  --context pvek8s --force --grace-period=0 --wait=false\n\n# ContainerStatusUnknown (110)\nkubectl get pods -n ci --context pvek8s --no-headers | \\\n  grep \"pgmac.pgmac-runnerdeploy\" | grep \"ContainerStatusUnknown\" | \\\n  awk '{print $1}' | xargs -I {} kubectl delete pod {} -n ci \\\n  --context pvek8s --force --grace-period=0 --wait=false\n\n# StartError/RunContainerError (58)\nkubectl get pods -n ci --context pvek8s --no-headers | \\\n  grep \"pgmac.pgmac-runnerdeploy\" | grep -E \"StartError|RunContainerError|Error\" | \\\n  awk '{print $1}' | xargs -I {} kubectl delete pod {} -n ci \\\n  --context pvek8s --force --grace-period=0 --wait=false\n\n# Completed (79)\nkubectl get pods -n ci --context pvek8s --no-headers | \\\n  grep \"pgmac.pgmac-runnerdeploy\" | grep \"Completed\" | \\\n  awk '{print $1}' | xargs -I {} kubectl delete pod {} -n ci \\\n  --context pvek8s --force --grace-period=0 --wait=false\n\n# Final cleanup batch (247)\nkubectl get pods -n ci --context pvek8s --no-headers | \\\n  grep \"pgmac.pgmac-runnerdeploy\" | grep -v \"Running\" | grep -v \"Terminating\" | \\\n  awk '{print $1}' | xargs -I {} kubectl delete pod {} -n ci \\\n  --context pvek8s --force --grace-period=0 --wait=false\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#5-openebs-replica-pod-cleanup","title":"5. OpenEBS Replica Pod Cleanup","text":"<pre><code># Force-deleted 22 stuck Terminating replica pods\nkubectl get pods -n openebs --context pvek8s | grep Terminating | \\\n  awk '{print $1}' | xargs -I {} kubectl delete pod {} -n openebs \\\n  --context pvek8s --force --grace-period=0\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-2-storage-and-ingress-recovery","title":"Phase 2: Storage and Ingress Recovery","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#6-k8s01-full-restart-residual-issues","title":"6. k8s01 Full Restart (Residual Issues)","text":"<pre><code># On k8s01 node\nmicrok8s stop &amp;&amp; microk8s start\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#7-jiva-snapshot-cleanup","title":"7. Jiva Snapshot Cleanup","text":"<pre><code># Triggered manual cleanup job\nkubectl --context pvek8s create job -n openebs jiva-snapshot-cleanup-manual \\\n  --from=cronjob/jiva-snapshot-cleanup\n\n# Job processed all Jiva volumes sequentially\n# - Rolling restart of 3 replicas per volume\n# - 30-second stabilization period between replicas\n# - Total runtime: ~60 minutes for all volumes\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#8-ingress-controller-refresh","title":"8. Ingress Controller Refresh","text":"<pre><code># Restarted all ingress controllers to clear endpoint cache\nkubectl --context pvek8s delete pod -n ingress \\\n  nginx-ingress-microk8s-controller-2chvz \\\n  nginx-ingress-microk8s-controller-k56gn \\\n  nginx-ingress-microk8s-controller-t56r5\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#9-radarr-volume-emergency-cleanup","title":"9. Radarr Volume Emergency Cleanup","text":"<pre><code># Freed space by removing old backups\nkubectl --context pvek8s exec -n media radarr-&lt;pod&gt; -- \\\n  rm -rf /config/Backups/*\n\n# Result: 100% \u2192 95% usage, sufficient for startup\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#10-k8s03-full-restart-replica-pod-issues","title":"10. k8s03 Full Restart (Replica Pod Issues)","text":"<pre><code># On k8s03 node\nmicrok8s stop &amp;&amp; microk8s start\n\n# All Pending replica pods recreated successfully after restart\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-3-job-controller-and-database-recovery-nuclear-option","title":"Phase 3: Job Controller and Database Recovery (Nuclear Option)","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#11-cluster-wide-restart-with-database-backup","title":"11. Cluster-Wide Restart with Database Backup","text":"<pre><code># Stop MicroK8s on all nodes (prevent database writes during backup)\nssh k8s01 \"sudo snap stop microk8s\"\nssh k8s02 \"sudo snap stop microk8s\"\nssh k8s03 \"sudo snap stop microk8s\"\n\n# Wait for clean shutdown\nsleep 30\n\n# Backup dqlite database (on k8s01 primary node)\nssh k8s01 \"sudo mkdir -p /var/snap/microk8s/common/backup &amp;&amp; \\\n  sudo cp -r /var/snap/microk8s/current/var/kubernetes/backend \\\n  /var/snap/microk8s/common/backup/etcd-backup-20260108-201540\"\n\n# Start MicroK8s on all nodes\nssh k8s01 \"sudo snap start microk8s\"\nssh k8s02 \"sudo snap start microk8s\"\nssh k8s03 \"sudo snap start microk8s\"\n\n# Wait for cluster to be ready\nkubectl --context pvek8s wait --for=condition=Ready nodes --all --timeout=300s\n\n# Verify cluster health\nkubectl --context pvek8s get nodes\nkubectl --context pvek8s get componentstatuses\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#12-clean-job-and-cronjob-state","title":"12. Clean Job and CronJob State","text":"<pre><code># Delete all stuck jobs (85+ jobs accumulated)\nkubectl --context pvek8s delete jobs -n media -l app.kubernetes.io/instance=linkace --all\n\n# Delete cronjob to get fresh state\nkubectl --context pvek8s delete cronjob linkace-cronjob -n media\n\n# Trigger ArgoCD sync to recreate with fresh state\nkubectl --context pvek8s patch application linkace -n argocd \\\n  --type merge -p '{\"operation\":{\"sync\":{\"revision\":\"HEAD\"}}}'\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#13-update-argocd-manifest-with-timeout-configuration","title":"13. Update ArgoCD Manifest with Timeout Configuration","text":"<pre><code># In pgk8s/pgmac.net/media/templates/linkace.yaml (lines 101-114)\ncronjob:\n  startingDeadlineSeconds: 60 # Grace period for job creation\n  activeDeadlineSeconds: 300 # 5-minute job timeout\n  ttlSecondsAfterFinished: 120 # 2-minute cleanup after completion\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 2\n  resources:\n    limits:\n      memory: 512Mi\n      cpu: 500m\n    requests:\n      memory: 256Mi\n      cpu: 100m\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#14-verification","title":"14. Verification","text":"<pre><code># Verify cronjob recreated with correct configuration\nkubectl --context pvek8s get cronjob linkace-cronjob -n media -o yaml\n\n# Wait for next minute and verify job creation\nwatch -n 10 'kubectl --context pvek8s get jobs -n media -l app.kubernetes.io/instance=linkace'\n\n# Verify job completes successfully\nkubectl --context pvek8s wait --for=condition=complete \\\n  job/linkace-cronjob-&lt;generated&gt; -n media --timeout=120s\n\n# Verify TTL cleanup working (jobs deleted after 2 minutes)\n# Monitor job count - should stabilize at 1 successful + max 2 failed\nwatch -n 30 'kubectl --context pvek8s get jobs -n media -l app.kubernetes.io/instance=linkace'\n\n# Check job execution time (should be ~7 seconds)\nkubectl --context pvek8s get job linkace-cronjob-&lt;latest&gt; -n media -o yaml | \\\n  grep -A 5 \"startTime\\|completionTime\"\n\n# Verify no orphaned pods\nkubectl --context pvek8s get pods -n media -l job-name\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-4-argocd-application-and-finalizer-recovery","title":"Phase 4: ArgoCD Application and Finalizer Recovery","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#15-remove-finalizers-from-stuck-resources","title":"15. Remove Finalizers from Stuck Resources","text":"<pre><code># Remove finalizer from AutoscalingRunnerSet\nkubectl --context pvek8s patch autoscalingrunnerset pgmac-slack-scores -n arc-runners \\\n  --type json -p='[{\"op\": \"remove\", \"path\": \"/metadata/finalizers\"}]'\n\n# Remove finalizer from ServiceAccount\nkubectl --context pvek8s patch serviceaccount pgmac-slack-scores-gha-rs-no-permission -n arc-runners \\\n  --type json -p='[{\"op\": \"remove\", \"path\": \"/metadata/finalizers\"}]'\n\n# Remove finalizer from Role\nkubectl --context pvek8s patch role pgmac-slack-scores-gha-rs-manager -n arc-runners \\\n  --type json -p='[{\"op\": \"remove\", \"path\": \"/metadata/finalizers\"}]'\n\n# Remove finalizer from RoleBinding\nkubectl --context pvek8s patch rolebinding pgmac-slack-scores-gha-rs-manager -n arc-runners \\\n  --type json -p='[{\"op\": \"remove\", \"path\": \"/metadata/finalizers\"}]'\n\n# Trigger ArgoCD sync for parent application\nkubectl --context pvek8s patch application ci-tools -n argocd \\\n  --type merge -p '{\"operation\":{\"sync\":{\"revision\":\"HEAD\"}}}'\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#16-clean-up-old-runner-controller-resources","title":"16. Clean Up Old Runner Controller Resources","text":"<pre><code># Delete old listener resources with stale hash\nkubectl --context pvek8s delete serviceaccount \\\n  pgmac-renovatebot-gha-rs-listener-754b578d -n arc-runners\n\nkubectl --context pvek8s delete role \\\n  pgmac-renovatebot-gha-rs-listener-754b578d -n arc-runners\n\nkubectl --context pvek8s delete rolebinding \\\n  pgmac-renovatebot-gha-rs-listener-754b578d -n arc-runners\n\n# Force-delete stuck runner pods (residual from Phase 3)\nkubectl --context pvek8s delete pod pgmac-renovatebot-&lt;pod-id&gt; -n arc-runners \\\n  --force --grace-period=0 --wait=false\n# Repeat for all 6 stuck pods\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#17-fix-linkace-helm-chart-configuration","title":"17. Fix LinkAce Helm Chart Configuration","text":"<pre><code># Edit ArgoCD manifest to remove unsupported fields\n# File: /Users/paulmacdonnell/pgmac/pgk8s/pgmac.net/media/templates/linkace.yaml\n# Removed lines (backoffLimit and resources block):\n#   backoffLimit: 0\n#   resources:\n#     limits:\n#       memory: 512Mi\n#       cpu: 500m\n#     requests:\n#       memory: 256Mi\n#       cpu: 100m\n\n# Kept critical timeout configuration:\n#   startingDeadlineSeconds: 60\n#   activeDeadlineSeconds: 300\n#   ttlSecondsAfterFinished: 120\n#   successfulJobsHistoryLimit: 1\n#   failedJobsHistoryLimit: 2\n\n# Commit changes\ncd /Users/paulmacdonnell/pgmac/pgk8s\ngit add pgmac.net/media/templates/linkace.yaml\ngit commit -m \"Remove unsupported LinkAce cronjob configuration\"\n\n# Handle git push rejection\ngit stash\ngit pull --rebase\ngit stash pop\ngit push\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#18-verification","title":"18. Verification","text":"<pre><code># Verify ci-tools application status\nkubectl --context pvek8s get application ci-tools -n argocd\n\n# Verify gharc-runners-pgmac-net-self-hosted (acceptable OutOfSync + Healthy)\nkubectl --context pvek8s get application gharc-runners-pgmac-net-self-hosted -n argocd\n\n# Verify hass application (should be Synced + Healthy)\nkubectl --context pvek8s get application hass -n argocd\n\n# Verify linkace application (acceptable OutOfSync + Healthy)\nkubectl --context pvek8s get application linkace -n argocd\n\n# Verify no stuck runner pods remain\nkubectl --context pvek8s get pods -n arc-runners | grep Pending\n\n# Verify ArgoCD sync status\nkubectl --context pvek8s get applications -n argocd | grep -E \"OutOfSync|Progressing\"\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-5-k8s01-container-runtime-recovery","title":"Phase 5: k8s01 Container Runtime Recovery","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#19-k8s01-container-runtime-investigation","title":"19. k8s01 Container Runtime Investigation","text":"<pre><code># List all pods in arc-runners namespace\nkubectl --context pvek8s get pods -n arc-runners\n\n# Identified 4 Pending pods (12+ hours old):\n# - self-hosted-l52x9-runner-2nnsr\n# - self-hosted-l52x9-runner-69qnv\n# - self-hosted-l52x9-runner-ls8c2\n# - self-hosted-l52x9-runner-w8mcd\n\n# Describe pod to check status\nkubectl --context pvek8s describe pod self-hosted-l52x9-runner-2nnsr -n arc-runners\n# Observed: PodScheduled=True, assigned to k8s01, no events generated\n\n# Check node status\nkubectl --context pvek8s get nodes\n# k8s01 showing Ready status despite being unable to start containers\n\n# Check pod locations\nkubectl --context pvek8s get pods -n arc-runners -o wide\n# All 4 Pending pods assigned to k8s01 node\n\n# Check EphemeralRunner resources\nkubectl --context pvek8s get ephemeralrunner -n arc-runners\n# Found 10 EphemeralRunner resources but only 4 pods exist\n# 6 pgmac-slack-scores runners have no corresponding pods\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#20-k8s01-microk8s-restart","title":"20. k8s01 MicroK8s Restart","text":"<pre><code># On k8s01 node (user executed)\nmicrok8s stop &amp;&amp; microk8s start\n\n# Wait for node to return Ready\nkubectl --context pvek8s wait --for=condition=Ready node/k8s01 --timeout=300s\n\n# Verify Pending pods cleared\nkubectl --context pvek8s get pods -n arc-runners\n# All 4 Pending pods should be gone, container runtime recovered\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#21-verification","title":"21. Verification","text":"<pre><code># Verify no Pending pods remain in arc-runners namespace\nkubectl --context pvek8s get pods -n arc-runners | grep Pending\n\n# Verify EphemeralRunner resources\nkubectl --context pvek8s get ephemeralrunner -n arc-runners\n\n# Verify k8s01 node health\nkubectl --context pvek8s describe node k8s01\n\n# Check for any new container creation issues\nkubectl --context pvek8s get events -n arc-runners --sort-by='.lastTimestamp'\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#verification","title":"Verification","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#service-health-checks","title":"Service Health Checks","text":"<ul> <li>\u2705 Sonarr: 200 OK responses, RSS sync operational</li> <li>\u2705 Radarr: 200 OK responses, web UI accessible</li> <li>\u2705 Overseerr: 200 OK responses, login page loading</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#infrastructure-health","title":"Infrastructure Health","text":"<ul> <li>\u2705 All 3 nodes (k8s01, k8s02, k8s03): Ready status</li> <li>\u2705 Jiva replicas: 39/40 Running (effectively 100%)</li> <li>\u2705 No Pending, CrashLoop, or Error pods cluster-wide (excluding residual runner pod cleanup)</li> <li>\u2705 Ingress controllers: Routing to correct pod IPs</li> <li>\u2705 Kubelet stable on all nodes: No repeated crashes or hangs</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#volume-replication","title":"Volume Replication","text":"<pre><code>Overseerr:  3/3 replicas Running\nSonarr:     3/3 replicas Running\nRadarr:     3/3 replicas Running\nAll others: 3/3 replicas Running\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#node-disk-status-post-cleanup","title":"Node Disk Status (Post-Cleanup)","text":"<pre><code>k8s01: 87% (down from 97%)\nk8s02: Stable (no initial disk pressure)\nk8s03: 81% (down from 100%)\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#preventive-measures","title":"Preventive Measures","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#immediate-actions-required","title":"Immediate Actions Required","text":"<ol> <li>Implement Node Disk Space Monitoring (Critical Priority)</li> <li>Current: No alerts for disk usage &gt;85%</li> <li>Target: Alert at 80%, critical alert at 90%</li> <li>Actions:<ul> <li>Deploy Prometheus node-exporter on all nodes</li> <li>Configure AlertManager rules for disk pressure</li> <li>Add Nagios checks for disk usage as backup</li> </ul> </li> <li> <p>Rationale: Both k8s01 (97%) and k8s03 (100%) hit critical thresholds without detection</p> </li> <li> <p>Automated Container Image Garbage Collection (High Priority)</p> </li> <li>Current: Manual cleanup required during incident</li> <li>Target: Automated daily cleanup maintaining &lt;75% disk usage</li> <li>Actions:<ul> <li>Configure kubelet <code>imageGCHighThresholdPercent=75</code> (default: 85)</li> <li>Configure kubelet <code>imageGCLowThresholdPercent=70</code> (default: 80)</li> <li>Schedule weekly cleanup cronjob as backup</li> </ul> </li> <li> <p>Rationale: 4+ years of accumulated images contributed to disk exhaustion</p> </li> <li> <p>Audit Log Rotation and Buffer Management (High Priority)</p> </li> <li>Current: Audit buffer overload caused kubelet crashes on k8s01</li> <li>Actions:<ul> <li>Reduce audit log verbosity (current level generating excessive data)</li> <li>Implement aggressive log rotation (hourly vs daily)</li> <li>Configure audit buffer size limits</li> <li>Consider disabling detailed audit logging for non-critical operations</li> </ul> </li> <li> <p>Rationale: \"audit buffer queue blocked\" directly caused kubelet instability</p> </li> <li> <p>Radarr PVC Expansion (High Priority)</p> </li> <li>Current: 1Gi volume at 95% capacity (carried over from Phase 2)</li> <li>Target: 2Gi to accommodate media artwork growth</li> <li>Action: Requires PVC recreation (Jiva doesn't support online expansion)</li> <li> <p>Steps:      <pre><code># 1. Backup Radarr config\n# 2. Create new 2Gi PVC\n# 3. Restore data\n# 4. Update deployment to use new PVC\n</code></pre></p> </li> <li> <p>Jiva Snapshot Cleanup Frequency (High Priority)</p> </li> <li>Current: Daily at 2 AM (threshold: 500 snapshots)</li> <li>Problem: 1011 snapshots accumulated when cronjob couldn't run during Phase 1</li> <li>Actions:<ul> <li>Lower threshold from 500 to 300 snapshots</li> <li>Increase frequency to every 12 hours (2 AM and 2 PM)</li> <li>Add monitoring/alerting for snapshot counts &gt;400</li> <li>Add pod anti-affinity to ensure cleanup job can run on healthy nodes</li> </ul> </li> <li> <p>Rationale: Cronjob failure during Phase 1 directly caused Phase 2 storage issues</p> </li> <li> <p>GitHub Actions Runner Controller Migration (Medium Priority)</p> </li> <li>Current: Orphaned runner pods consumed significant resources</li> <li>Actions:<ul> <li>Migrate to GitHub-hosted runners or alternative self-hosted solution</li> <li>If keeping self-hosted: implement strict <code>maxReplicas</code> limits</li> <li>Add PodDisruptionBudgets to prevent runaway scaling</li> <li>Configure aggressive pod cleanup policies</li> </ul> </li> <li> <p>Rationale: 571 orphaned pods significantly contributed to cluster instability</p> </li> <li> <p>CronJob Timeout Configuration Baseline (High Priority - Added from Phase 3)</p> </li> <li>Current: CronJobs created without timeout settings, allowing infinite hangs</li> <li>Target: All cronjobs have defensive timeout configuration</li> <li>Actions:<ul> <li>Create baseline cronjob template with standard timeouts:</li> <li><code>startingDeadlineSeconds: 60</code> (for minute-frequency jobs)</li> <li><code>activeDeadlineSeconds: &lt;appropriate for task&gt;</code> (e.g., 300 for 5-min tasks)</li> <li><code>ttlSecondsAfterFinished: 120</code> (2-minute cleanup)</li> <li><code>successfulJobsHistoryLimit: 1</code></li> <li><code>failedJobsHistoryLimit: 2</code></li> <li>Audit all existing cronjobs and add timeout configuration</li> <li>Add validation in ArgoCD to require timeout settings</li> </ul> </li> <li> <p>Rationale: Timeout settings proved critical for self-healing, but were missing</p> </li> <li> <p>Job Controller Health Monitoring (Critical Priority - Added from Phase 3)</p> </li> <li>Current: No monitoring for job controller state or corruption</li> <li>Actions:<ul> <li>Add synthetic job creation tests every 5 minutes cluster-wide</li> <li>Monitor job controller logs for \"not found\" errors</li> <li>Alert on jobs with 0 pods after 2 minutes</li> <li>Alert on jobs exceeding activeDeadlineSeconds without termination</li> <li>Monitor dqlite database health and replication lag</li> </ul> </li> <li> <p>Rationale: Job controller corruption went undetected for 16+ hours</p> </li> <li> <p>Dqlite Database Backup Automation (High Priority - Added from Phase 3)</p> </li> <li>Current: Manual backup procedures only</li> <li>Target: Automated hourly backups with 24-hour retention</li> <li>Actions:<ul> <li>Create cronjob to backup dqlite database (requires node-local execution)</li> <li>Store backups on NFS with rotation policy</li> <li>Document and test restoration procedure</li> <li>Add alerts for backup failures</li> </ul> </li> <li>Rationale: Database backup was critical for nuclear option confidence</li> </ol>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#longer-term-improvements","title":"Longer-Term Improvements","text":"<ol> <li> <p>Node Health Synthetic Testing (High Priority)</p> </li> <li> <p>Issue: Nodes reported Ready but couldn't start containers</p> </li> <li>Actions:</li> <li>Deploy synthetic pod startup tests every 5 minutes on each node</li> <li>Monitor microk8s/kubelite service health</li> <li>Alert on pod startup failures or extended ContainerCreating states</li> <li>Consider scheduled microk8s service restarts (monthly maintenance)</li> <li> <p>Rationale: k8s02 and k8s03 both showed \"Ready\" status while unable to start pods</p> </li> <li> <p>Dqlite State Recovery Procedures (Medium Priority - Updated from Phase 3)</p> </li> <li> <p>Issue: Dqlite state corruption from Phase 1 required Phase 3 nuclear option</p> </li> <li>Actions:</li> <li>COMPLETED: Automated dqlite database backups documented (see item 9)</li> <li>Document nuclear option recovery procedure (cluster restart with backup)</li> <li>Test backup restoration in non-production scenario</li> <li>Add monitoring for dqlite replication lag between nodes</li> <li>Consider scheduled preventive cluster restarts (quarterly maintenance)</li> <li> <p>Rationale: Dqlite corruption from Phase 1 persisted for 48+ hours, required nuclear option</p> </li> <li> <p>Node Reboot Resilience Testing (Medium Priority)</p> </li> <li> <p>Issue: Simultaneous node reboots triggered cascading failures</p> </li> <li>Actions:</li> <li>Implement controlled rolling node restarts (monthly maintenance)</li> <li>Document graceful node shutdown/startup procedures</li> <li>Test simultaneous 2-node failure scenarios</li> <li>Add PodDisruptionBudgets for critical services</li> <li>Configure proper pod anti-affinity for redundant services</li> <li> <p>Rationale: Inability to handle simultaneous reboots indicates insufficient resilience</p> </li> <li> <p>Ingress Endpoint Monitoring (Medium Priority)</p> <ul> <li>Add monitoring to detect stale endpoint caching</li> <li>Alert on pod IP changes not reflected in ingress logs</li> <li>Consider automated ingress controller restarts after pod migrations</li> </ul> </li> <li> <p>Volume Capacity Monitoring (High Priority)</p> <ul> <li>Implement alerts for PVC usage &gt;85%</li> <li>Current gap: No visibility into Jiva volume capacity</li> <li>Tool: Consider deploying Prometheus with node-exporter + custom Jiva metrics</li> </ul> </li> <li> <p>Snapshot Management Strategy (Medium Priority)</p> <ul> <li>Investigate snapshot growth rate per volume</li> <li>Document expected snapshot accumulation patterns</li> <li>Consider application-specific snapshot retention policies</li> <li>Evaluate if 3-replica Jiva setup is necessary (vs 2-replica for non-critical data)</li> </ul> </li> <li> <p>MediaCover Cleanup Automation (Low Priority)</p> <ul> <li>Radarr MediaCover directory: 837M of 974M total</li> <li>Implement periodic cleanup of orphaned/old media artwork</li> <li>Consider storing media artwork on NFS instead of Jiva volumes</li> </ul> </li> <li> <p>Runbook Documentation (High Priority - Updated from Phase 3)</p> <ul> <li>Document kubelet/kubelite restart procedures for all nodes</li> <li>Document disk cleanup emergency procedures with target thresholds</li> <li>Document Jiva snapshot cleanup manual trigger process</li> <li>Document ingress controller restart for endpoint refresh</li> <li>Document force-deletion procedures for stuck pods</li> <li>NEW: Document nuclear option procedures (cluster restart with dqlite backup)</li> <li>NEW: Document job controller corruption recovery steps</li> <li>NEW: Document self-healing verification checklist</li> <li>Add to on-call playbook with estimated recovery times</li> <li>Rationale: Multiple manual interventions required across all 3 phases; procedures must be documented</li> <li>Reference: <code>/tmp/linkace-cronjob-nuclear-option.md</code> created during Phase 3</li> </ul> </li> <li> <p>Cluster Architecture Review (Low Priority)</p> <ul> <li>Current: 4+ year old microk8s installation</li> <li>Consider: Upgrade path to newer Kubernetes versions</li> <li>Evaluate: Migration to managed Kubernetes (EKS, GKE, AKS) or alternative distributions</li> <li>Rationale: Age of installation may contribute to accumulated technical debt</li> </ul> </li> </ol>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#lessons-learned","title":"Lessons Learned","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#what-went-well","title":"What Went Well","text":"<ol> <li>Systematic troubleshooting approach: Correctly identified kubelet issues as separate from scheduler problems</li> <li>Node cordoning strategy: Temporarily removing k8s02 from rotation helped isolate the problem</li> <li>Diagnostic tools worked effectively: <code>kubectl</code> commands, <code>journalctl</code>, and custom scripts like <code>check-jiva-volumes.py</code> provided crucial insights</li> <li>Modular architecture: Issues isolated to specific components, preventing total cluster failure</li> <li>Quick node recovery: microk8s restarts resolved kubelet issues within 1-2 minutes</li> <li>Automated cleanup existed: Jiva snapshot cleanup cronjob was already in place, just needed manual trigger</li> <li>Full replication: Jiva 3-replica setup meant volumes remained accessible with 2/3 replicas during issue</li> <li>Force-deletion strategy: Successfully cleared 546+ orphaned pods using batched force-delete commands</li> <li>Phase 3 - Timeout configuration added proactively: ArgoCD manifest updated with defensive timeout settings before nuclear option</li> <li>Phase 3 - Database backup procedures: Successfully backed up dqlite database before nuclear option, providing rollback capability</li> <li>Phase 3 - Nuclear option executed cleanly: Cluster restart resolved all issues within 15 minutes with zero data loss</li> <li>Phase 3 - Verification thoroughness: Systematic verification of job creation, completion, TTL cleanup, and pod lifecycle</li> </ol>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#what-didnt-go-well","title":"What Didn't Go Well","text":"<ol> <li>Cascading failure propagation: Initial node reboot triggered multiple secondary failures across all infrastructure layers</li> <li>No proactive monitoring: Disk usage (97%, 100%) and snapshot accumulation (1011) went undetected</li> <li>Kubelet instability: Disk pressure caused repeated kubelet crashes without clear error messages in pod status</li> <li>Database state corruption: Dqlite database corruption persisted for 48+ hours, spanning Phase 1 \u2192 Phase 3</li> <li>Manual intervention required: Multiple manual steps needed across 8-hour (Phase 1-2) + 16.5-hour (Phase 3) + 12+ hour (Phase 5) periods vs automated recovery</li> <li>Long cleanup duration: 60+ minutes for snapshot cleanup job to process all volumes</li> <li>Ingress endpoint caching: No automatic detection/refresh of stale endpoints</li> <li>Runner controller orphaned pods: 571 pods remained despite controller scaled to 0</li> <li>Capacity planning gap: Radarr volume undersized for actual usage patterns</li> <li>Node Ready status misleading: Nodes reported Ready but couldn't start containers (kubelet vs containerd state mismatch)</li> <li>Cronjob failure during node issues: Snapshot cleanup cronjob couldn't run during Phase 1, directly causing Phase 2 storage issues</li> <li>Phase 3 - Self-healing complete failure: Waited 1.5 hours for timeout-based self-healing that never occurred</li> <li>Phase 3 - Job controller corruption went undetected: 16+ hours of cronjob failures without alerting</li> <li>Phase 3 - Controller restarts ineffective: Multiple kubelite restarts across all nodes failed to clear corruption</li> <li>Phase 3 - ArgoCD auto-sync failed: GitOps automation failed when resources were deleted for clean state</li> <li>Phase 3 - No job controller monitoring: Zero visibility into controller state or processing errors</li> <li>Phase 5 - Nuclear option insufficient: Cluster-wide restart (Phase 3) didn't clear node-local container runtime corruption on k8s01</li> <li>Phase 5 - Silent failure undetected: 12+ hour delay in detecting Pending pods with no container initialization</li> <li>Phase 5 - No node-local runtime monitoring: Zero visibility into container runtime health vs kubelet health</li> </ol>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#surprise-findings","title":"Surprise Findings","text":"<ol> <li>Audit buffer overload: Audit logging directly caused kubelet crashes (not commonly documented failure mode)</li> <li>Dqlite database corruption persistence: Database corruption from Phase 1 persisted for 48+ hours despite multiple controller restarts</li> <li>Kubelet crash without pod warnings: Pods showed \"Pending\" with no indication kubelet was crashing</li> <li>Disk threshold: 97% disk usage was sufficient to crash kubelet despite &gt;3% free space</li> <li>Runner pod accumulation: 571 pods accumulated without triggering any resource quota or alerts</li> <li>Snapshot physical storage: 1011 snapshots consumed 3GB physical space in 1Gi logical volume</li> <li>Media artwork growth: Radarr artwork (837M) exceeded database size (33M) by 25x</li> <li>Cleanup job thoroughness: Job processed ALL Jiva volumes, not just over-threshold volumes</li> <li>Cross-phase dependency: Phase 1 kubelet/disk issues directly prevented Phase 2 cronjobs from running, and Phase 1 database corruption caused Phase 3 job controller failures</li> <li>Phase 3 - Job controller single point of failure: Single corrupted job reference prevented ALL job creation cluster-wide</li> <li>Phase 3 - Timeout settings ignored: Properly configured activeDeadlineSeconds and ttlSecondsAfterFinished completely ignored by corrupted controller</li> <li>Phase 3 - Controller restart insufficient: Restarting kubelite service didn't clear in-memory controller state</li> <li>Phase 3 - Nuclear option effectiveness: Full cluster restart immediately resolved all controller corruption issues</li> <li>Phase 3 - Self-healing timeline invalid: Expected 6-12 hour self-healing never occurred; corruption was permanent without intervention</li> <li>Phase 5 - Nuclear option scope limitation: Cluster restart cleared cluster-global state (dqlite, controllers) but not node-local container runtime corruption</li> <li>Phase 5 - Corruption dormancy: Container runtime corruption from Phase 1 remained dormant for 48+ hours until new workloads attempted to schedule on k8s01</li> <li>Phase 5 - Silent failure persistence: Same silent failure pattern from Phase 2 (PodScheduled=True, no events) persisted despite Phase 3 nuclear option</li> </ol>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#action-items","title":"Action Items","text":"Priority Action Owner Due Date Status Critical Deploy node disk space monitoring with alerts (80%/90% thresholds) SRE 2026-01-08 Open Critical Configure automated container image garbage collection (75% threshold) SRE 2026-01-09 Open Critical Implement job controller health monitoring with synthetic tests SRE 2026-01-09 Open High Implement audit log rotation and reduce verbosity SRE 2026-01-10 Open High Expand Radarr PVC from 1Gi to 2Gi SRE 2026-01-13 Open High Lower snapshot threshold to 300, increase cleanup frequency to 12h SRE 2026-01-08 Open High Audit all cronjobs and add timeout configuration baseline SRE 2026-01-15 Open High Implement automated dqlite database backups (hourly, 24h retention) SRE 2026-01-10 Open High Document nuclear option runbook (cluster restart with dqlite backup) SRE 2026-01-12 Open High Implement synthetic pod startup health checks on all nodes SRE 2026-01-15 Open High Add PVC capacity monitoring and alerting (&gt;85%) SRE 2026-01-20 Open Medium Test dqlite backup restoration in non-production scenario SRE 2026-01-17 Open Medium Add dqlite replication lag monitoring SRE 2026-01-20 Open Medium Migrate GitHub Actions to hosted runners or implement strict limits SRE 2026-01-27 Open Medium Test node reboot resilience with controlled failures SRE 2026-02-03 Open Medium Investigate k8s01/k8s03 kubelet/containerd logs from incident SRE 2026-01-13 Open Medium Add ingress endpoint staleness monitoring SRE 2026-02-10 Open Medium Investigate ArgoCD auto-sync failure for deleted resources SRE 2026-01-20 Open Low Implement Radarr MediaCover cleanup automation Dev 2026-02-03 Open Low Evaluate reducing Jiva replication from 3 to 2 for non-critical data SRE 2026-02-10 Open Low Review cluster architecture and upgrade path SRE 2026-03-01 Open Low Consider scheduled preventive cluster restarts (quarterly) SRE 2026-03-01 Open"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#technical-details","title":"Technical Details","text":""},{"location":"incidents/2026-01-06-cluster-cascade-failure/#environment","title":"Environment","text":"<ul> <li>Cluster: microk8s on 3 nodes (k8s01, k8s02, k8s03)</li> <li>Kubernetes Version: v1.34.3</li> <li>Node Age: 4 years 138 days</li> <li>Storage: OpenEBS Jiva 2.12.1 (openebs-jiva-default storage class)</li> <li>Ingress: nginx-ingress-microk8s-controller (3 replicas)</li> <li>Network: Calico CNI</li> <li>etcd Alternative: kine (microk8s default)</li> <li>Container Runtime: containerd via microk8s</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#affected-resources","title":"Affected Resources","text":"<p>Phase 1:</p> <pre><code>Namespaces: ci, openebs, kube-system, all namespaces (scheduler impact)\nNodes:\n  - k8s01: Kubelet crash loop (disk 97% + audit buffer overload)\n  - k8s02: Kubelet hung (process restart required)\n  - k8s03: Disk 100% full (garbage collection failure)\nPods:\n  - GitHub Actions runners: 571 orphaned (299 Pending, 110 ContainerStatusUnknown, 79 Completed, 58 StartError, 25 other)\n  - OpenEBS replicas: 22 stuck Terminating\n  - Various: Unable to start/stop across all namespaces\n</code></pre> <p>Phase 2:</p> <pre><code>Namespaces: media, openebs, ingress\nPods:\n  - sonarr-7b8f6fcfc4-4wm8m (Pending \u2192 Running)\n  - radarr-5c95c64cff-* (CrashLoopBackOff \u2192 Running, multiple restarts)\n  - overseerr-58cc7d4569-kllz2 (Running, intermittent timeouts)\nPVCs:\n  - radarr-config (pvc-311bef00..., 1Gi, 100% full \u2192 95% after cleanup)\n  - sonarr-config (pvc-17e6e808..., 1Gi, 1011 snapshots)\n  - overseerr-config (pvc-05e03b60..., 1Gi, 1011 snapshots)\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#snapshot-cleanup-job-output","title":"Snapshot Cleanup Job Output","text":"<pre><code>Volumes processed: 13\nVolumes cleaned: 13\nSnapshots consolidated: 1011 \u2192 ~100 per volume (estimated)\nDuration: ~60 minutes\nMethod: Rolling restart of replicas (3 per volume, 30s stabilization between)\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#node-disk-usage-timeline","title":"Node Disk Usage Timeline","text":"<pre><code>k8s01: 97% (critical) \u2192 87% (stable) after cleanup\nk8s02: Stable throughout (no disk pressure)\nk8s03: 100% (critical) \u2192 81% (stable) after cleanup\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#kubelet-error-patterns-phase-1","title":"Kubelet Error Patterns (Phase 1)","text":"<pre><code>k8s01 errors:\n- \"audit buffer queue blocked\"\n- \"database is locked\" (kine)\n- \"Failed to garbage collect required amount of images\"\n- \"Kubelet stopped posting node status\"\n\nk8s02 errors:\n- Pods assigned but never reached ContainerCreating (silent failure)\n\nk8s03 errors:\n- \"Failed to garbage collect required amount of images. Attempted to free 13GB, but only found 0 bytes eligible to free\"\n</code></pre>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#references","title":"References","text":"<ul> <li>Jiva Volume Checker Script: <code>/Users/paulmacdonnell/pgmac/check-jiva-volumes.py</code></li> <li>Snapshot Cleanup Config: <code>/Users/paulmacdonnell/pgmac/pgk8s/pgmac.net/system/templates/jiva-snapshot-cleanup.yaml</code></li> <li>Cleanup Job Logs: <code>kubectl logs -n openebs jiva-snapshot-cleanup-manual-4tv5c</code></li> <li>OpenEBS Jiva Documentation: https://openebs.io/docs/user-guides/jiva</li> <li>microk8s Documentation: https://microk8s.io/docs</li> <li>Kubernetes Kubelet Configuration: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#reviewers","title":"Reviewers","text":"<ul> <li>Prepared by: Claude (AI Assistant)</li> <li>Date: 2026-01-06</li> <li>Review Status: Draft - Pending human review</li> </ul>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#notes","title":"Notes","text":"<p>This incident demonstrated the fragility of a long-running Kubernetes cluster under cascading failure conditions across five distinct phases spanning 2026-01-06 to 2026-01-09. Key takeaways:</p>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#cross-phase-insights","title":"Cross-Phase Insights","text":"<ol> <li>Disk pressure is a critical failure mode: Both 97% and 100% disk usage caused complete kubelet failure, not just degraded performance</li> <li>Audit logging can become a liability: Excessive audit log generation directly caused kubelet crashes via buffer overload</li> <li>Node \"Ready\" status is insufficient: Nodes reported Ready while unable to start containers (kubelet vs containerd state mismatch)</li> <li>Cascading failures span days, not hours: Initial Phase 1 node reboot \u2192 disk pressure \u2192 kubelet failures \u2192 dqlite corruption \u2192 48 hours later \u2192 Phase 3 job controller corruption</li> <li>Automated cleanup jobs are single points of failure: Snapshot cleanup cronjob failure during Phase 1 directly caused Phase 2 storage issues</li> <li>Orphaned pods accumulate silently: 571 runner pods accumulated over time without triggering resource quotas or alerts</li> <li>Force-deletion is sometimes necessary: Normal deletion failed for 546+ pods due to finalizer/controller corruption</li> <li>Database state corruption is persistent: Dqlite corruption persisted for 48+ hours despite multiple controller restarts</li> <li>Multiple layers require monitoring: Node health, disk space, kubelet status, pod lifecycle, storage subsystem, ingress endpoints, controller state</li> <li>Age matters: 4+ year old installation accumulated technical debt (images, logs, state corruption)</li> </ol>"},{"location":"incidents/2026-01-06-cluster-cascade-failure/#phase-3-specific-insights-job-controller-corruption","title":"Phase 3-Specific Insights (Job Controller Corruption)","text":"<ol> <li>Controller corruption is catastrophic: Single corrupted job reference prevented ALL job creation cluster-wide</li> <li>Service restarts don't clear all state: Restarting kubelite service didn't clear in-memory controller state or dqlite database corruption</li> <li>Self-healing has limits: Properly configured timeout settings (activeDeadlineSeconds, ttlSecondsAfterFinished) were completely ignored by corrupted controller</li> <li>Nuclear option is sometimes necessary: Full cluster restart with database backup was the only effective recovery path</li> <li>Timeout configuration is defensive, not curative: Timeout settings prevent runaway resource consumption but don't fix controller corruption</li> <li>Job controller is a single point of failure: No redundancy or failover mechanism for corrupted job controller state</li> <li>GitOps auto-sync can fail: ArgoCD auto-sync failed when resources deleted for clean state, requiring manual intervention</li> <li>Database backups provide confidence: Having dqlite backup before nuclear option provided rollback capability and reduced risk</li> <li>Verification is critical: Systematic verification of job lifecycle (creation \u2192 pod spawn \u2192 completion \u2192 TTL cleanup) necessary after controller recovery</li> <li>Controller monitoring is essential: Zero visibility into job controller processing state delayed detection by 16+ hours</li> </ol> <p>The resolution required comprehensive intervention across all infrastructure layers (compute, storage, networking, control plane, database) demonstrating the interconnected nature of Kubernetes cluster health and the importance of:</p> <ul> <li>Proactive monitoring at multiple levels (nodes, controllers, database)</li> <li>Automated maintenance and cleanup with defensive timeout configuration</li> <li>Graceful degradation under failure (with acknowledgment of hard limits)</li> <li>Clear runbooks for manual intervention including nuclear option procedures</li> <li>Regular resilience testing including controller corruption scenarios</li> <li>Capacity planning and right-sizing</li> <li>Understanding failure cascades and dependencies between systems across multi-day timespans</li> <li>Database backup and recovery procedures for confidence in drastic recovery actions</li> <li>Controller state monitoring and synthetic testing</li> <li>Recognition that some failures require full cluster restart to resolve</li> </ul> <p>Future incidents can be prevented or mitigated through the preventive measures outlined above, particularly:</p> <ul> <li>Phase 1 preventions: Disk space monitoring, automated image cleanup, audit log management</li> <li>Phase 2 preventions: Comprehensive health checks beyond node \"Ready\" status, cronjob reliability monitoring</li> <li>Phase 3 preventions: Job controller health monitoring, synthetic job tests, dqlite backup automation, cronjob timeout baselines, nuclear option documentation</li> </ul> <p>The three-phase nature of this incident (spanning 48+ hours) highlights that cascading failures can have long-term delayed effects requiring sustained vigilance and multiple recovery strategies beyond initial stabilization.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/","title":"Post Incident Review: Radarr Outage Due to OpenEBS Jiva Replica Divergence","text":"<p>Date: 2026-02-22 Duration: ~16h30m silent failure + ~47m active recovery (22:25 AEST 2026-02-21 \u2192 18:48 AEST 2026-02-22) Severity: High (single service outage \u2014 Radarr completely unavailable) Status: Resolved</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#executive-summary","title":"Executive Summary","text":"<p>Radarr became unavailable when its pod failed to start, remaining stuck in <code>ContainerCreating</code> for over 4.5 hours before investigation began. The pod could not start because its persistent volume (<code>radarr-config</code>) could not be mounted. The mount failure was caused by a corrupted ext4 filesystem on the iSCSI block device (<code>/dev/sdi</code>), which itself was caused by all three OpenEBS Jiva storage replicas simultaneously entering a <code>CrashLoopBackOff</code> state with diverged snapshot chains.</p> <p>The Jiva replicas failed because all three had been left in a <code>Rebuilding: true</code> state following an ungraceful shutdown at approximately 22:25 AEST on 2026-02-21 \u2014 roughly 16 hours before the pod failure was detected. Without a healthy replica to serve as a rebuild source, the Jiva controller could not serve a consistent iSCSI target. This left the filesystem journal dirty, which caused <code>fsck -a</code> (run automatically by kubelet before each mount attempt) to fail repeatedly.</p> <p>Two additional PVCs \u2014 <code>overseerr-config</code> and <code>scotchcraft-minecraft-datadir</code> \u2014 were found to have suffered the same underlying Jiva failure but self-recovered because at least one of their replicas remained in a healthy state. They had restart counts of 10-11 and 52-53 respectively indicating significant instability around the same event.</p> <p>Resolution required: scaling down all Jiva deployments, patching <code>volume.meta</code> on one replica to clear the <code>Rebuilding</code> flag, clearing the image data on the other two replicas so they rebuilt from the good source, scaling back up in sequence, and allowing the ext4 journal recovery to complete during the next successful mount.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#timeline-aest-utc10","title":"Timeline (AEST \u2014 UTC+10)","text":"Time Event 2026-02-21 ~22:25 ROOT EVENT: Ungraceful shutdown interrupts an in-progress Jiva rebuild across all 3 replicas. All nodes' <code>revision.counter</code> files share this timestamp. ~22:25 onwards All 3 Jiva replicas for <code>radarr-config</code> enter CrashLoopBackOff. Jiva controller loses all healthy backends. iSCSI LUN (<code>/dev/sdi</code>) becomes unserviceable. ~22:25 onwards Kubelet begins retrying PVC mount for radarr pod (not yet scheduled). Each retry runs <code>fsck -a /dev/sdi</code>, which fails with \"can't read superblock\". 2026-02-22 14:01 Radarr pod <code>radarr-cd6596b59-lbc2v</code> scheduled, enters <code>ContainerCreating</code>. PVC mount failing silently \u2014 pod status gives no indication of storage failure. 14:01 \u2192 18:05 Pod remains in <code>ContainerCreating</code> for 4h4m with no alerting. <code>FailedMount</code> events accumulate in pod describe but are not visible without active investigation. ~18:05 INCIDENT DETECTED: Manual investigation triggered. <code>kubectl describe pod</code> reveals repeated <code>FailedMount</code> events citing <code>can't read superblock on /dev/sdi</code> and <code>fsck found errors but could not correct them</code>. ~18:08 All 3 Jiva replica pods identified in CrashLoopBackOff: <code>rep-1</code> (k8s03), <code>rep-2</code> (k8s02), <code>rep-3</code> (k8s01). Controller running 2/2 but with zero healthy backends. ~18:10 Replica logs reveal fatal error: <code>\"Current replica's checkpoint not present in rwReplica chain, Shutting down...\"</code> ~18:12 All 3 nodes' <code>volume.meta</code> inspected \u2014 all show <code>\"Rebuilding\":true</code> with identical <code>RevisionCounter: 2538385</code> but diverged <code>Parent</code> snapshot chains. Root cause confirmed. ~18:15 RESOLUTION START: All 4 Jiva deployments (controller + 3 replicas) scaled to 0. ~18:17 <code>volume.meta</code> on k8s01 (<code>rep-3</code>) patched: <code>\"Rebuilding\": false</code>. This replica designated as the authoritative source. ~18:19 All <code>.img</code> and <code>.img.meta</code> files moved to <code>.bak</code> directories on k8s02 and k8s03. <code>volume.meta</code> files also moved so those replicas start completely fresh. ~18:21 Jiva controller scaled back to 1. Becomes ready within 80 seconds. ~18:22 <code>rep-3</code> (k8s01, the fixed replica) scaled to 1. Joins controller as RW replica. ~18:24 <code>rep-1</code> and <code>rep-2</code> scaled to 1. Begin rebuilding from <code>rep-3</code>. Jiva correctly serialises \u2014 only one WO rebuild at a time. ~18:29 All 4 Jiva pods <code>Running</code>. Snapshot sync active on <code>rep-2</code>, <code>rep-1</code> queued. ~18:30 Old D-state <code>fsck.ext4</code> process (from prior kubelet retry) clears. <code>/dev/sdi</code> becomes free. ~18:32 Manual <code>fsck -y /dev/sdi</code> attempt fails \u2014 kubelet has already spawned a new <code>fsck -a</code> process, racing for the device. ~18:37 Radarr scaled to 0 to stop kubelet from competing for <code>/dev/sdi</code>. ~18:40 D-state <code>fsck -a</code> process clears. <code>dmesg</code> shows <code>EXT4-fs (sdi): recovery complete</code> \u2014 the kernel's ext4 journal recovery succeeded during a mount attempt after Jiva became healthy. ~18:47 Radarr scaled back to 1. 18:48:28 INCIDENT RESOLVED: Radarr pod <code>radarr-cd6596b59-mlbs6</code> reaches <code>1/1 Running</code>."},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#root-causes","title":"Root Causes","text":""},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#the-infinite-hows-chain","title":"The Infinite How's Chain","text":"<p>\"The infinite how's\" methodology: at each causal step, ask \"how?\" rather than accepting the surface answer. Keep drilling until reaching an actionable, preventable cause.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#how-did-radarr-become-unavailable","title":"How did radarr become unavailable?","text":"<p>The radarr pod entered <code>ContainerCreating</code> and never progressed. The startup probe (TCP socket on port 7878) could not succeed because the container itself never launched.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#how-did-the-container-fail-to-launch","title":"How did the container fail to launch?","text":"<p>Kubelet was unable to mount the <code>radarr-config</code> PVC. Volume mounting is a prerequisite for container creation; without it, the pod is stuck in <code>ContainerCreating</code> indefinitely.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#how-did-the-pvc-mount-fail","title":"How did the PVC mount fail?","text":"<p>Kubelet automatically runs <code>fsck</code> before mounting a block device-backed volume. The fsck reported:</p> <pre><code>/dev/sdi: can't read superblock\n</code></pre> <p>and later:</p> <pre><code>/dev/sdi: UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY.\n(i.e., without -a or -p options)\n</code></pre> <p>The auto-repair flag (<code>-a</code>) is insufficient for this class of journal inconsistency. Kubelet has no mechanism to escalate beyond <code>fsck -a</code>; it simply retries and logs <code>FailedMount</code>.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#how-did-the-ext4-filesystem-on-devsdi-become-inconsistent","title":"How did the ext4 filesystem on <code>/dev/sdi</code> become inconsistent?","text":"<p><code>/dev/sdi</code> is the iSCSI block device provided by the OpenEBS Jiva controller for the <code>radarr-config</code> PVC. When all Jiva replica pods simultaneously entered <code>CrashLoopBackOff</code>, the controller had no healthy backends to service I/O. The iSCSI target remained presented to the host but writes timed out or returned errors. The last radarr write session left the ext4 journal in a dirty/uncommitted state \u2014 which <code>fsck -a</code> cannot repair because the journal's <code>needs_recovery</code> flag was inconsistent with the presence of journal data.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#how-did-all-three-jiva-replica-pods-enter-crashloopbackoff","title":"How did all three Jiva replica pods enter CrashLoopBackOff?","text":"<p>Each replica's log contained:</p> <pre><code>level=fatal msg=\"Failed to add replica to controller, err: Current replica's\ncheckpoint not present in rwReplica chain, Shutting down...\"\n</code></pre> <p>Jiva's safety mechanism: when a replica restarts, it contacts the controller and verifies that its latest snapshot checkpoint exists in the controller's canonical chain. If not, the replica refuses to join (to prevent serving stale or diverged data) and exits. With all three replicas failing this check, the controller has zero healthy backends.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#how-did-all-three-replicas-end-up-with-checkpoints-that-didnt-match-the-controllers-chain","title":"How did all three replicas end up with checkpoints that didn't match the controller's chain?","text":"<p>Inspection of each replica's <code>volume.meta</code> showed:</p> <pre><code>{\n  \"Rebuilding\": true,\n  \"Checkpoint\": \"volume-snap-0fd00bc8-...\",\n  \"RevisionCounter\": 2538385\n}\n</code></pre> <p>All three replicas had identical <code>RevisionCounter</code> values (<code>2538385</code>) and identical <code>Checkpoint</code> UUIDs \u2014 but each had a different <code>Parent</code> snapshot for its head image:</p> Node Head Parent k8s01 <code>volume-snap-3d6f0344...</code> k8s02 <code>volume-snap-b5c23a63...</code> k8s03 <code>volume-snap-af55ce5c...</code> <p>All three were in <code>Rebuilding: true</code>. They had been simultaneously mid-rebuild when something caused a cluster-wide disruption. Each replica had snapshotted at the moment of trying to join (standard Jiva behaviour during rebuild) and those new snapshots were not present in any sibling's chain \u2014 causing the circular checkpoint mismatch.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#how-did-all-three-replicas-end-up-rebuilding-at-the-same-time","title":"How did all three replicas end up rebuilding at the same time?","text":"<p>The <code>revision.counter</code> file on all three nodes bore the same timestamp: 2026-02-21 22:25 AEST. Jiva writes the revision counter file atomically on clean shutdown. The identical timestamp is strong evidence that all three nodes experienced a simultaneous ungraceful shutdown at that moment \u2014 a power event, network partition, or host-level failure causing all three nodes to lose connectivity or restart at the same instant.</p> <p>OpenEBS Jiva only rebuilds one WO (write-only) replica at a time under normal operation. For all three to be in a rebuilding state simultaneously, the disruption must have occurred while a multi-replica rebuild was already in progress \u2014 meaning the system had already been in a partially degraded state before the 22:25 event.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#how-did-a-prior-degraded-state-go-undetected","title":"How did a prior degraded state go undetected?","text":"<p>There is no alerting on:</p> <ul> <li>Jiva replica <code>CrashLoopBackOff</code> or elevated restart counts</li> <li>Jiva replica <code>Rebuilding: true</code> flag persisting beyond a threshold</li> <li>PVC <code>FailedMount</code> events accumulating on pods</li> <li>Pods remaining in <code>ContainerCreating</code> beyond a time threshold</li> </ul> <p>The two other affected PVCs (<code>overseerr-config</code>, <code>scotchcraft-minecraft-datadir</code>) had accumulated 10-53 restarts respectively before self-recovering \u2014 also without triggering any alert.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#how-did-the-radarr-pod-sit-in-containercreating-for-over-4-hours-without-detection","title":"How did the radarr pod sit in <code>ContainerCreating</code> for over 4 hours without detection?","text":"<p>The pod status <code>ContainerCreating</code> is a normal transient state during startup. Kubernetes does not surface <code>FailedMount</code> events prominently in <code>kubectl get pods</code> output \u2014 they are only visible via <code>kubectl describe pod</code>. Without a dashboard widget or alert rule explicitly targeting pods stuck in <code>ContainerCreating</code> beyond a threshold (e.g., 5 minutes), the failure was invisible.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#secondary-findings","title":"Secondary Findings","text":""},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#pvc-05e03b60-overseerr-config-and-pvc-f1888541-scotchcraft-minecraft","title":"pvc-05e03b60 (overseerr-config) and pvc-f1888541 (scotchcraft-minecraft)","text":"<p>Both PVCs were hit by the same underlying Feb 21 22:25 disruption. Both showed the same \"checkpoint not present in rwReplica chain\" fatal error in replica logs. Unlike radarr, at least one replica for each volume had remained in a healthy (non-Rebuilding) state before the disruption, allowing them to self-recover by electing one replica as RW and rebuilding the others from it. Recovery took 50-90 minutes and produced 10-53 container restarts per replica pod \u2014 indicating significant thrashing before convergence.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#impact","title":"Impact","text":""},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#services-affected","title":"Services Affected","text":"<ul> <li>Radarr (<code>https://radarr.int.pgmac.net</code>): Completely unavailable. Pod stuck in <code>ContainerCreating</code>, no web UI, no API, no media management functionality.</li> <li>Overseerr (<code>https://overseerr.int.pgmac.net</code>): Elevated Jiva replica instability but service remained available throughout.</li> <li>Scotchcraft Minecraft: Elevated Jiva replica instability but service remained available throughout.</li> </ul>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#duration","title":"Duration","text":"<ul> <li>Radarr total outage: ~20h47m (from 22:01 AEST 2026-02-21 to 18:48 AEST 2026-02-22)</li> <li>Silent failure period (undetected): ~16h04m (22:25 \u2192 ~14:01 \u2014 pod was not scheduled)</li> <li>Pod stuck in ContainerCreating (undetected): ~4h04m (14:01 \u2192 ~18:05)</li> <li>Active recovery: ~43m (~18:05 \u2192 18:48)</li> <li>Overseerr instability: ~6-8h duration, self-resolved, no user-visible outage confirmed</li> <li>Minecraft instability: ~6-8h duration, self-resolved, no user-visible outage confirmed</li> </ul>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#scope","title":"Scope","text":"<ul> <li>Storage: OpenEBS Jiva storage subsystem for 3 PVCs across 3 namespaces</li> <li>User-facing: Media management (no new media could be tracked or imported via Radarr)</li> <li>Monitoring: No detection for ~16h of silent failure</li> </ul>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#resolution-steps-taken","title":"Resolution Steps Taken","text":""},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#1-create-argocd-syncwindow","title":"1. Create ArgoCD SyncWindow","text":"<p>Create a <code>dney</code> SyncWindow in ArgoCD on all applications to ensure ArgoCD does NOT attempt to auto-sync any changes during the restoration</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#2-scale-down-all-jiva-deployments","title":"2. Scale Down All Jiva Deployments","text":"<pre><code>kubectl scale deployment -n openebs \\\n  pvc-a634b9a3-fdaa-4b45-9dc3-2486e716d755-ctrl \\\n  pvc-a634b9a3-fdaa-4b45-9dc3-2486e716d755-rep-1 \\\n  pvc-a634b9a3-fdaa-4b45-9dc3-2486e716d755-rep-2 \\\n  pvc-a634b9a3-fdaa-4b45-9dc3-2486e716d755-rep-3 \\\n  --replicas=0\n</code></pre>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#3-patch-volumemeta-on-k8s01-rep-3-the-authoritative-source","title":"3. Patch volume.meta on k8s01 (rep-3) \u2014 the Authoritative Source","text":"<pre><code># Backup first\nsudo cp volume.meta volume.meta.bak\n\n# Patch Rebuilding flag to false\nsudo python3 -c \"\nimport json\npath = '/var/snap/microk8s/common/var/openebs/pvc-a634b9a3-.../volume.meta'\nwith open(path) as f:\n    data = json.load(f)\ndata['Rebuilding'] = False\nwith open(path, 'w') as f:\n    json.dump(data, f, separators=(',', ':'))\n\"\n</code></pre>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#4-clear-image-data-on-k8s02-and-k8s03","title":"4. Clear Image Data on k8s02 and k8s03","text":"<pre><code># On k8s02 and k8s03 \u2014 move (not delete) all img files and volume.meta to backup\nsudo mkdir -p /var/snap/microk8s/common/var/openebs/pvc-a634b9a3-....bak\nsudo mv /var/snap/microk8s/common/var/openebs/pvc-a634b9a3-.../*.img \\\n        /var/snap/microk8s/common/var/openebs/pvc-a634b9a3-....bak/\nsudo mv /var/snap/microk8s/common/var/openebs/pvc-a634b9a3-.../*.img.meta \\\n        /var/snap/microk8s/common/var/openebs/pvc-a634b9a3-....bak/\nsudo mv /var/snap/microk8s/common/var/openebs/pvc-a634b9a3-.../volume.meta \\\n        /var/snap/microk8s/common/var/openebs/pvc-a634b9a3-....bak/\n</code></pre>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#5-scale-up-in-sequence","title":"5. Scale Up in Sequence","text":"<pre><code># Controller first\nkubectl scale deployment -n openebs pvc-a634b9a3-...-ctrl --replicas=1\n\n# Wait for controller ready\nkubectl wait --for=condition=ready pod -n openebs \\\n  -l openebs.io/persistent-volume=pvc-a634b9a3-...,openebs.io/controller=jiva-controller \\\n  --timeout=60s\n\n# Good replica (k8s01, rep-3) next\nkubectl scale deployment -n openebs pvc-a634b9a3-...-rep-3 --replicas=1\n\n# Allow rep-3 to establish as RW, then bring up the others\nkubectl scale deployment -n openebs \\\n  pvc-a634b9a3-...-rep-1 \\\n  pvc-a634b9a3-...-rep-2 \\\n  --replicas=1\n</code></pre>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#6-stop-radarr-to-clear-the-mount-race","title":"6. Stop Radarr to Clear the Mount Race","text":"<pre><code># Radarr was generating competing fsck -a processes preventing manual fsck\nkubectl scale deployment -n media radarr --replicas=0\n</code></pre> <p>At this point the kernel's ext4 journal recovery completed automatically during a mount attempt (<code>dmesg</code> showed <code>EXT4-fs (sdi): recovery complete</code> and <code>mounted filesystem with ordered data mode</code>), eliminating the need for a manual <code>fsck -y</code>.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#7-restore-radarr","title":"7. Restore Radarr","text":"<pre><code>kubectl scale deployment -n media radarr --replicas=1\n</code></pre>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#8-cleanup","title":"8. Cleanup","text":"<pre><code># Remove backup directories from all nodes\nssh k8s01 \"sudo rm -f .../volume.meta.bak\"\nssh k8s02 \"sudo rm -rf ...pvc-a634b9a3-....bak\"\nssh k8s03 \"sudo rm -rf ...pvc-a634b9a3-....bak\"\n</code></pre>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#9-remove-syncwindow","title":"9. Remove SyncWindow","text":"<p>Remove the <code>deny</code> SyncWindow in ArgoCD to ensure normal/expected auto-sync operation continues</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#verification","title":"Verification","text":""},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#service-health","title":"Service Health","text":"<ul> <li>\u2705 Radarr: <code>1/1 Running</code>, stable for 8+ check intervals post-recovery</li> <li>\u2705 Overseerr: <code>1/1 Running</code>, no further replica restarts</li> <li>\u2705 Minecraft: <code>1/1 Running</code>, no further replica restarts</li> </ul>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#storage-health","title":"Storage Health","text":"<pre><code>pvc-a634b9a3 (radarr-config):\n  ctrl:  2/2 Running\n  rep-1: 1/1 Running  (rebuilt from rep-3)\n  rep-2: 1/1 Running  (rebuilt from rep-3)\n  rep-3: 1/1 Running  (authoritative source)\n\npvc-05e03b60 (overseerr-config):\n  All replicas: 1/1 Running, Rebuilding=false, shared Checkpoint \u2705\n\npvc-f1888541 (minecraft-datadir):\n  All replicas: 1/1 Running, Rebuilding=false, shared Checkpoint \u2705\n</code></pre>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#volume-metadata-post-recovery","title":"Volume Metadata (Post-Recovery)","text":"<p>All radarr-config replicas confirmed with:</p> <ul> <li><code>\"Rebuilding\": false</code></li> <li>Shared <code>Checkpoint</code> UUID across all 3 nodes</li> <li>Shared <code>Parent</code> snapshot reference</li> <li>Active sync converging <code>RevisionCounter</code> values</li> </ul>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#preventive-measures","title":"Preventive Measures","text":""},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#immediate-actions-required","title":"Immediate Actions Required","text":"<ol> <li>Alert on pods stuck in ContainerCreating &gt; 5 minutes (Critical Priority)</li> <li>Current: No alerting; a pod can sit stuck indefinitely without detection</li> <li>Target: PagerDuty/Slack alert when any pod remains in <code>ContainerCreating</code> beyond 5 minutes</li> <li>Implementation: Prometheus <code>kube_pod_status_phase</code> + duration alert rule</li> <li> <p>Rationale: 4+ hours elapsed before manual detection. This single alert would have reduced radarr's outage from hours to minutes.</p> </li> <li> <p>Alert on Jiva replica CrashLoopBackOff (Critical Priority)</p> </li> <li>Current: No alerting on OpenEBS replica pod failures</li> <li>Target: Immediate alert when any Jiva replica pod enters <code>CrashLoopBackOff</code> or <code>Error</code></li> <li>Implementation: Prometheus <code>kube_pod_container_status_waiting_reason{reason=\"CrashLoopBackOff\"}</code> filtered to <code>openebs</code> namespace</li> <li> <p>Rationale: All 3 replicas were in CrashLoopBackOff for ~16 hours before detection</p> </li> <li> <p>Alert on Jiva replica restart count threshold (High Priority)</p> </li> <li>Current: No alerting; overseerr and minecraft accumulated 10-53 restarts silently</li> <li>Target: Alert when any Jiva replica pod exceeds 5 restarts within 30 minutes</li> <li>Implementation: <code>rate(kube_pod_container_status_restarts_total[30m]) &gt; 0.1</code> filtered to openebs namespace</li> <li> <p>Rationale: The self-recovered PVCs showed the same failure pattern but slightly less severe \u2014 an early restart alert would flag the pattern before it becomes critical</p> </li> <li> <p>Alert on FailedMount events (High Priority)</p> </li> <li>Current: <code>FailedMount</code> events are only visible via <code>kubectl describe</code>; no alerting</li> <li>Target: Alert when a pod generates more than 3 <code>FailedMount</code> events</li> <li>Implementation: Prometheus <code>kube_event_count{reason=\"FailedMount\"}</code> alert rule</li> <li> <p>Rationale: The mount failure was generating repeated events for hours with no visibility</p> </li> <li> <p>Document OpenEBS Jiva replica divergence recovery runbook (High Priority)</p> </li> <li>Current: No documented procedure; recovery required real-time diagnosis</li> <li>Target: Step-by-step runbook covering: identify diverged replicas \u2192 patch volume.meta \u2192 clear image data on non-source replicas \u2192 scale up in sequence</li> <li>Location: <code>incidents/docs/runbooks/openebs-jiva-replica-recovery.md</code></li> <li> <p>Rationale: Recovery took ~43 minutes of active work; a runbook would reduce this significantly and remove the knowledge dependency</p> </li> <li> <p>Investigate and document the Feb 21 22:25 root event (High Priority)</p> </li> <li>Current: The simultaneous all-node disruption at 22:25 AEST is unexplained</li> <li>Target: Identify whether this was a power event, network partition, kernel bug, or other cause</li> <li>Actions:<ul> <li>Review UPS/PDU logs for that timeframe</li> <li>Review node-level system logs (<code>/var/log/syslog</code>) from all 3 nodes around 22:25</li> <li>Check Proxmox/hypervisor logs if nodes are VMs</li> </ul> </li> <li>Rationale: The same unknown event also degraded overseerr and minecraft. If it recurs, all Jiva volumes are at risk of the same failure mode</li> </ol>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#longer-term-improvements","title":"Longer-Term Improvements","text":"<ol> <li>Jiva Rebuilding flag monitoring (Medium Priority)</li> <li>Add a periodic check (every 5 minutes) that inspects <code>volume.meta</code> on all Jiva replica nodes and alerts if <code>Rebuilding: true</code> persists beyond 30 minutes</li> <li>A replica stuck in Rebuilding for &gt;30 minutes indicates a stalled or failed rebuild that requires intervention</li> <li> <p>Implementation: CronJob running a script against node hostPaths, or custom Prometheus exporter</p> </li> <li> <p>Jiva rebuild serialisation guard (Medium Priority)</p> </li> <li>When a cluster-wide disruption leaves all replicas in Rebuilding state simultaneously, Jiva has no self-healing path because no replica can establish as RW</li> <li>Investigate whether OpenEBS Jiva has a recovery mode or operator-level intervention hook that can be automated</li> <li> <p>Consider upgrading OpenEBS if newer versions have improved recovery handling for this scenario</p> </li> <li> <p>Structured review of all Jiva volumes' health state (Medium Priority)</p> </li> <li>Run a periodic job that checks <code>volume.meta</code> on all Jiva replica hostPaths across all nodes</li> <li>Report: revision counter skew between replicas, Rebuilding flag, Dirty flag, snapshot chain depth</li> <li> <p>This would surface partial degradation (e.g., one of three replicas in an unhealthy state) before it becomes a full outage</p> </li> <li> <p>Snapshot chain depth monitoring (Medium Priority)</p> <ul> <li>Referenced from the 2026-01-06 PIR: excessive snapshot accumulation caused Phase 2 storage issues in that incident</li> <li>The radarr-config PVC had 280+ snapshot files on k8s01 at recovery time, indicating the jiva-snapshot-cleanup cronjob may not be running effectively</li> <li>Verify snapshot cleanup cronjob is healthy and its threshold/frequency is appropriate (see 2026-01-06 action items)</li> </ul> </li> </ol>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#lessons-learned","title":"Lessons Learned","text":""},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#what-went-well","title":"What Went Well","text":"<ol> <li>Thorough diagnostic approach: The full causal chain from \"pod not starting\" to \"all replicas Rebuilding simultaneously\" was traced in approximately 10 minutes using <code>kubectl describe</code>, pod logs, node SSH access, and <code>volume.meta</code> inspection</li> <li>Careful recovery sequencing: Scaling down before making filesystem changes, choosing the source replica deliberately, moving (not deleting) backup data before confirming recovery \u2014 all prevented data loss</li> <li>Self-healing worked for two of three affected PVCs: The overseerr and minecraft volumes recovered without intervention, demonstrating that Jiva's rebuild mechanism works correctly when at least one healthy replica survives</li> <li>Backup before touching metadata: <code>volume.meta.bak</code> was created before patching, and image files were moved rather than deleted, preserving rollback options throughout</li> <li>The kernel handled ext4 recovery: Once the Jiva backend was healthy, the kernel's built-in ext4 journal recovery resolved the filesystem corruption without requiring a separate manual <code>fsck</code> \u2014 simplifying the recovery</li> </ol>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#what-didnt-go-well","title":"What Didn't Go Well","text":"<ol> <li>16+ hours of silent failure: The root event occurred at 22:25 AEST; the incident was not detected until ~18:05 the next day \u2014 a detection gap of over 16 hours</li> <li>ContainerCreating is invisible as an error state: The pod appeared \"normal\" to casual inspection; only <code>kubectl describe</code> revealed the FailedMount events</li> <li>No Jiva health alerting whatsoever: Three separate PVCs experienced Jiva replica failures affecting 3 different services, all without any alert being generated</li> <li>fsck -a race condition: Kubelet continuously spawning new <code>fsck -a</code> processes prevented manual <code>fsck -y</code> from acquiring the device, requiring the workaround of scaling radarr to 0</li> <li>The underlying 22:25 disruption remains unexplained: The simultaneous all-replica crash is the true root cause, and without knowing what caused it, the risk of recurrence cannot be assessed or mitigated</li> <li>Jiva has no self-healing path when all replicas diverge: The system had no ability to recover without manual metadata surgery \u2014 this is a fundamental architectural limitation</li> </ol>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#surprise-findings","title":"Surprise Findings","text":"<ol> <li>All 3 replicas can simultaneously enter an unrecoverable state: The assumption that 3-replica Jiva provides resilience only holds if the disruption affects fewer than a quorum. A simultaneous all-node disruption during an active rebuild defeats this assumption entirely.</li> <li>The Rebuilding flag persists across pod restarts: <code>volume.meta</code> is on the node's hostPath, not in the pod. Each time a replica pod restarted, it read <code>Rebuilding: true</code> and immediately failed. The CrashLoopBackOff was not a transient issue \u2014 it would never self-resolve.</li> <li>The revision.counter timestamp as a forensic tool: The identical <code>Feb 21 22:25</code> mtime on all three nodes' <code>revision.counter</code> files provided precise timing of the root event without any application-level logging.</li> <li>ext4 journal recovery as a \"free\" fix: The kernel handled the filesystem repair during the successful mount after Jiva was fixed, avoiding the need for manual <code>fsck -y</code>. The <code>can't read superblock</code> error from the earlier kubelet attempts was due to the total absence of Jiva backends, not permanent disk corruption.</li> <li>Two other services were also impacted but self-recovered: Without checking all Jiva pod restart counts, the full blast radius of the Feb 21 event would have been unknown. The same root event caused instability across minecraft and overseerr.</li> </ol>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#action-items","title":"Action Items","text":"Priority Action Owner Due Date Status Critical Alert: pod stuck in ContainerCreating &gt; 5 minutes SRE 2026-03-01 Open Critical Alert: Jiva replica pod CrashLoopBackOff in openebs namespace SRE 2026-03-01 Open High Alert: Jiva replica restart rate &gt; 5 in 30 minutes SRE 2026-03-08 Open High Alert: FailedMount events &gt; 3 on any pod SRE 2026-03-08 Open High Write OpenEBS Jiva replica recovery runbook SRE 2026-03-08 Open High Investigate Feb 21 22:25 root event (UPS, PDU, hypervisor logs) SRE 2026-03-01 Open Medium Implement Jiva Rebuilding flag monitor (cronjob/exporter) SRE 2026-03-15 Open Medium Investigate Jiva upgrade path or automated rebuild recovery SRE 2026-03-22 Open Medium Periodic Jiva volume health report (revision skew, chain depth) SRE 2026-03-22 Open Medium Verify jiva-snapshot-cleanup cronjob health and thresholds SRE 2026-03-01 Open Low Investigate ci namespace high-restart pods (dependency-track: 176 restarts) SRE 2026-03-15 Open Low Investigate media namespace chronic restarters (metasearch: 34, linkace: 11) SRE 2026-03-22 Open"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#technical-details","title":"Technical Details","text":""},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#environment","title":"Environment","text":"<ul> <li>Cluster: pvek8s (microk8s on 3 nodes: k8s01/172.22.22.6, k8s02, k8s03/172.22.22.9)</li> <li>Storage: OpenEBS Jiva 2.12.1 (<code>openebs-jiva-default</code> storage class)</li> <li>Affected PVC: <code>radarr-config</code> (pvc-a634b9a3-fdaa-4b45-9dc3-2486e716d755), 5Gi RWO</li> <li>iSCSI target: <code>iqn.2016-09.com.openebs.jiva:pvc-a634b9a3-fdaa-4b45-9dc3-2486e716d755</code> at <code>10.152.183.80:3260</code></li> <li>Block device on k8s01: <code>/dev/sdi</code> (ext4 filesystem, 2G)</li> <li>Replica hostPath: <code>/var/snap/microk8s/common/var/openebs/pvc-a634b9a3-fdaa-4b45-9dc3-2486e716d755/</code></li> </ul>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#replica-state-at-discovery","title":"Replica State at Discovery","text":"Node Replica Revision Counter Head Image Rebuilding Status k8s01 rep-3 2538385 volume-head-280.img true CrashLoopBackOff k8s02 rep-2 2538385 volume-head-376.img true CrashLoopBackOff k8s03 rep-1 2538385 volume-head-378.img true CrashLoopBackOff <p>All three had identical <code>Checkpoint: \"volume-snap-0fd00bc8-aaa8-40d1-90c3-1971d4837540.img\"</code> but different <code>Parent</code> snapshot references, confirming divergence during an interrupted multi-replica rebuild.</p>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#key-log-entries","title":"Key Log Entries","text":"<p>Jiva replica fatal error (all 3 replicas):</p> <pre><code>level=fatal msg=\"Failed to add replica to controller, err: Current replica's\ncheckpoint not present in rwReplica chain, Shutting down...\"\n</code></pre> <p>Kubelet mount failure (pod events):</p> <pre><code>Warning  FailedMount  kubelet  MountVolume.MountDevice failed for volume\n\"pvc-a634b9a3-...\" : 'fsck' found errors on device /dev/disk/by-path/...\nbut could not correct them:\n/dev/sdi: recovering journal\n/dev/sdi: Superblock needs_recovery flag is clear, but journal has data.\n/dev/sdi: UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY.\n</code></pre> <p>ext4 recovery success (dmesg on k8s01):</p> <pre><code>[...] EXT4-fs (sdi): recovery complete\n[...] EXT4-fs (sdi): mounted filesystem with ordered data mode. Opts: (null)\n[...] sd 10:0:0:0: [sdi] Synchronizing SCSI cache\n</code></pre>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#other-affected-pvcs-same-root-event","title":"Other Affected PVCs (Same Root Event)","text":"PVC App Max Restarts Self-Recovered Outage pvc-05e03b60 (overseerr-config) Overseerr 11 Yes None confirmed pvc-f1888541 (minecraft-datadir) Scotchcraft Minecraft 53 Yes None confirmed"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#references","title":"References","text":"<ul> <li>Previous incident covering Jiva snapshot accumulation: <code>incidents/docs/incidents/2026-01-06-cluster-cascade-failure.md</code></li> <li>OpenEBS Jiva documentation: https://openebs.io/docs/user-guides/jiva</li> <li>OpenEBS Jiva volume.meta schema: internal replica metadata, not publicly documented</li> </ul>"},{"location":"incidents/2026-02-22-radarr-openebs-jiva-replica-divergence/#reviewers","title":"Reviewers","text":"<ul> <li>Prepared by: Claude (AI Assistant)</li> <li>Date: 2026-02-22</li> <li>Review Status: Draft \u2014 Pending human review</li> </ul>"}]}