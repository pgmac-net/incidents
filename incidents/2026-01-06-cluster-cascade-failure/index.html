
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Internal post-incident review documentation">
      
      
        <meta name="author" content="pgmac">
      
      
        <link rel="canonical" href="https://incidents.pgmac.net.au/incidents/2026-01-06-cluster-cascade-failure/">
      
      
        <link rel="prev" href="../..">
      
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>2026-01-06 Cluster Cascade Failure - Post-Incident Reviews</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#post-incident-review-cascading-kubernetes-cluster-failures" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Post-Incident Reviews" class="md-header__button md-logo" aria-label="Post-Incident Reviews" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Post-Incident Reviews
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2026-01-06 Cluster Cascade Failure
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="red" data-md-color-accent="deep-orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pgmac-net/incidents/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pgmac-net/incidents
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
  
  Incidents

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Post-Incident Reviews" class="md-nav__button md-logo" aria-label="Post-Incident Reviews" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Post-Incident Reviews
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pgmac-net/incidents/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pgmac-net/incidents
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Incidents
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Incidents
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    2026-01-06 Cluster Cascade Failure
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    2026-01-06 Cluster Cascade Failure
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#executive-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Executive Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#timeline-aest-utc10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Timeline (AEST - UTC+10)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Timeline (AEST - UTC+10)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phase-1-cascading-node-and-kubelet-failures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 1: Cascading Node and Kubelet Failures
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-2-storage-and-ingress-service-outages" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 2: Storage and Ingress Service Outages
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cleanup-operations-parallel-with-phase-2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cleanup Operations (Parallel with Phase 2)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-linkace-cronjob-controller-corruption-2026-01-08-0200-1825" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 3: LinkAce CronJob Controller Corruption (2026-01-08 02:00-18:25)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-4-argocd-application-recovery-2026-01-08-1900-1945" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 4: ArgoCD Application Recovery (2026-01-08 ~19:00-19:45)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-5-k8s01-container-runtime-corruption-recurrence-2026-01-09-0950-0955" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 5: k8s01 Container Runtime Corruption Recurrence (2026-01-09 ~09:50-09:55)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#root-causes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Root Causes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Root Causes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phase-1-node-and-control-plane-failures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 1: Node and Control Plane Failures
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 1: Node and Control Plane Failures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-cascading-node-reboots-primary-trigger" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 Cascading Node Reboots (Primary Trigger)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-k8s01-kubelet-crash-loop-critical" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 k8s01 Kubelet Crash Loop (Critical)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-k8s02-kubelet-process-hang-critical" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.3 k8s02 Kubelet Process Hang (Critical)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-k8s03-disk-exhaustion-critical" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.4 k8s03 Disk Exhaustion (Critical)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-github-actions-runner-controller-orphaned-pods-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.5 GitHub Actions Runner Controller Orphaned Pods (Secondary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-openebs-replica-pods-stuck-terminating-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.6 OpenEBS Replica Pods Stuck Terminating (Secondary)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-2-storage-and-ingress-failures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 2: Storage and Ingress Failures
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 2: Storage and Ingress Failures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-openebs-jiva-snapshot-accumulation-primary" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 OpenEBS Jiva Snapshot Accumulation (Primary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-residual-node-container-runtime-issues" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 Residual Node Container Runtime Issues
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-ingress-controller-stale-endpoint-cache" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 Ingress Controller Stale Endpoint Cache
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-radarr-volume-capacity-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4 Radarr Volume Capacity (Secondary)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-linkace-cronjob-controller-corruption-2026-01-08" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 3: LinkAce CronJob Controller Corruption (2026-01-08)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 3: LinkAce CronJob Controller Corruption (2026-01-08)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-job-controller-state-corruption-primary-critical" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 Job Controller State Corruption (Primary - Critical)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-dqlite-database-state-corruption-primary" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 Dqlite Database State Corruption (Primary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-timeout-configuration-not-enforced-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 Timeout Configuration Not Enforced (Secondary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-argocd-auto-sync-failure-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.4 ArgoCD Auto-Sync Failure (Secondary)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-4-argocd-application-recovery-2026-01-08" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 4: ArgoCD Application Recovery (2026-01-08)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 4: ArgoCD Application Recovery (2026-01-08)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-github-actions-runner-controller-finalizer-issues-primary" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 GitHub Actions Runner Controller Finalizer Issues (Primary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-github-actions-runner-controller-state-drift-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 GitHub Actions Runner Controller State Drift (Secondary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-linkace-helm-chart-configuration-drift-primary" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 LinkAce Helm Chart Configuration Drift (Primary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-home-assistant-application-self-healing-none" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.4 Home Assistant Application Self-Healing (None)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-5-k8s01-container-runtime-corruption-recurrence-2026-01-09" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 5: k8s01 Container Runtime Corruption Recurrence (2026-01-09)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 5: k8s01 Container Runtime Corruption Recurrence (2026-01-09)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-persistent-container-runtime-corruption-on-k8s01-critical" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 Persistent Container Runtime Corruption on k8s01 (Critical)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-detection-gap-for-node-local-failures-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 Detection Gap for Node-Local Failures (Secondary)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#impact" class="md-nav__link">
    <span class="md-ellipsis">
      
        Impact
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Impact">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#services-affected" class="md-nav__link">
    <span class="md-ellipsis">
      
        Services Affected
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#duration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Duration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scope" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scope
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resolution-steps-taken" class="md-nav__link">
    <span class="md-ellipsis">
      
        Resolution Steps Taken
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Resolution Steps Taken">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phase-1-node-and-kubelet-recovery" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 1: Node and Kubelet Recovery
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 1: Node and Kubelet Recovery">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-k8s02-kubelet-restart" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. k8s02 Kubelet Restart
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-k8s01-disk-cleanup-and-stabilization" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. k8s01 Disk Cleanup and Stabilization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-k8s03-disk-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. k8s03 Disk Cleanup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-runner-controller-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Runner Controller Cleanup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-openebs-replica-pod-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. OpenEBS Replica Pod Cleanup
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-2-storage-and-ingress-recovery" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 2: Storage and Ingress Recovery
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 2: Storage and Ingress Recovery">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#6-k8s01-full-restart-residual-issues" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. k8s01 Full Restart (Residual Issues)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-jiva-snapshot-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Jiva Snapshot Cleanup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8-ingress-controller-refresh" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Ingress Controller Refresh
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#9-radarr-volume-emergency-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        9. Radarr Volume Emergency Cleanup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#10-k8s03-full-restart-replica-pod-issues" class="md-nav__link">
    <span class="md-ellipsis">
      
        10. k8s03 Full Restart (Replica Pod Issues)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-job-controller-and-database-recovery-nuclear-option" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 3: Job Controller and Database Recovery (Nuclear Option)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 3: Job Controller and Database Recovery (Nuclear Option)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-cluster-wide-restart-with-database-backup" class="md-nav__link">
    <span class="md-ellipsis">
      
        11. Cluster-Wide Restart with Database Backup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-clean-job-and-cronjob-state" class="md-nav__link">
    <span class="md-ellipsis">
      
        12. Clean Job and CronJob State
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-update-argocd-manifest-with-timeout-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      
        13. Update ArgoCD Manifest with Timeout Configuration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-verification" class="md-nav__link">
    <span class="md-ellipsis">
      
        14. Verification
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-4-argocd-application-and-finalizer-recovery" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 4: ArgoCD Application and Finalizer Recovery
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 4: ArgoCD Application and Finalizer Recovery">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#15-remove-finalizers-from-stuck-resources" class="md-nav__link">
    <span class="md-ellipsis">
      
        15. Remove Finalizers from Stuck Resources
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-clean-up-old-runner-controller-resources" class="md-nav__link">
    <span class="md-ellipsis">
      
        16. Clean Up Old Runner Controller Resources
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17-fix-linkace-helm-chart-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      
        17. Fix LinkAce Helm Chart Configuration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#18-verification" class="md-nav__link">
    <span class="md-ellipsis">
      
        18. Verification
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-5-k8s01-container-runtime-recovery" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 5: k8s01 Container Runtime Recovery
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 5: k8s01 Container Runtime Recovery">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#19-k8s01-container-runtime-investigation" class="md-nav__link">
    <span class="md-ellipsis">
      
        19. k8s01 Container Runtime Investigation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#20-k8s01-microk8s-restart" class="md-nav__link">
    <span class="md-ellipsis">
      
        20. k8s01 MicroK8s Restart
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#21-verification" class="md-nav__link">
    <span class="md-ellipsis">
      
        21. Verification
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#verification" class="md-nav__link">
    <span class="md-ellipsis">
      
        Verification
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Verification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#service-health-checks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Service Health Checks
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#infrastructure-health" class="md-nav__link">
    <span class="md-ellipsis">
      
        Infrastructure Health
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#volume-replication" class="md-nav__link">
    <span class="md-ellipsis">
      
        Volume Replication
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#node-disk-status-post-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        Node Disk Status (Post-Cleanup)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#preventive-measures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Preventive Measures
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Preventive Measures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#immediate-actions-required" class="md-nav__link">
    <span class="md-ellipsis">
      
        Immediate Actions Required
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#longer-term-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      
        Longer-Term Improvements
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lessons-learned" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lessons Learned
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lessons Learned">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-went-well" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Went Well
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-didnt-go-well" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Didn't Go Well
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#surprise-findings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Surprise Findings
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#action-items" class="md-nav__link">
    <span class="md-ellipsis">
      
        Action Items
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#technical-details" class="md-nav__link">
    <span class="md-ellipsis">
      
        Technical Details
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Technical Details">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#environment" class="md-nav__link">
    <span class="md-ellipsis">
      
        Environment
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#affected-resources" class="md-nav__link">
    <span class="md-ellipsis">
      
        Affected Resources
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#snapshot-cleanup-job-output" class="md-nav__link">
    <span class="md-ellipsis">
      
        Snapshot Cleanup Job Output
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#node-disk-usage-timeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        Node Disk Usage Timeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kubelet-error-patterns-phase-1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kubelet Error Patterns (Phase 1)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reviewers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reviewers
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Notes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Notes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cross-phase-insights" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cross-Phase Insights
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-specific-insights-job-controller-corruption" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 3-Specific Insights (Job Controller Corruption)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#executive-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Executive Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#timeline-aest-utc10" class="md-nav__link">
    <span class="md-ellipsis">
      
        Timeline (AEST - UTC+10)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Timeline (AEST - UTC+10)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phase-1-cascading-node-and-kubelet-failures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 1: Cascading Node and Kubelet Failures
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-2-storage-and-ingress-service-outages" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 2: Storage and Ingress Service Outages
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cleanup-operations-parallel-with-phase-2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cleanup Operations (Parallel with Phase 2)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-linkace-cronjob-controller-corruption-2026-01-08-0200-1825" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 3: LinkAce CronJob Controller Corruption (2026-01-08 02:00-18:25)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-4-argocd-application-recovery-2026-01-08-1900-1945" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 4: ArgoCD Application Recovery (2026-01-08 ~19:00-19:45)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-5-k8s01-container-runtime-corruption-recurrence-2026-01-09-0950-0955" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 5: k8s01 Container Runtime Corruption Recurrence (2026-01-09 ~09:50-09:55)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#root-causes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Root Causes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Root Causes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phase-1-node-and-control-plane-failures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 1: Node and Control Plane Failures
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 1: Node and Control Plane Failures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-cascading-node-reboots-primary-trigger" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 Cascading Node Reboots (Primary Trigger)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-k8s01-kubelet-crash-loop-critical" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 k8s01 Kubelet Crash Loop (Critical)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-k8s02-kubelet-process-hang-critical" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.3 k8s02 Kubelet Process Hang (Critical)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-k8s03-disk-exhaustion-critical" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.4 k8s03 Disk Exhaustion (Critical)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-github-actions-runner-controller-orphaned-pods-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.5 GitHub Actions Runner Controller Orphaned Pods (Secondary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-openebs-replica-pods-stuck-terminating-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.6 OpenEBS Replica Pods Stuck Terminating (Secondary)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-2-storage-and-ingress-failures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 2: Storage and Ingress Failures
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 2: Storage and Ingress Failures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-openebs-jiva-snapshot-accumulation-primary" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 OpenEBS Jiva Snapshot Accumulation (Primary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-residual-node-container-runtime-issues" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 Residual Node Container Runtime Issues
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-ingress-controller-stale-endpoint-cache" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 Ingress Controller Stale Endpoint Cache
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-radarr-volume-capacity-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4 Radarr Volume Capacity (Secondary)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-linkace-cronjob-controller-corruption-2026-01-08" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 3: LinkAce CronJob Controller Corruption (2026-01-08)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 3: LinkAce CronJob Controller Corruption (2026-01-08)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-job-controller-state-corruption-primary-critical" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 Job Controller State Corruption (Primary - Critical)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-dqlite-database-state-corruption-primary" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 Dqlite Database State Corruption (Primary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-timeout-configuration-not-enforced-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 Timeout Configuration Not Enforced (Secondary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-argocd-auto-sync-failure-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.4 ArgoCD Auto-Sync Failure (Secondary)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-4-argocd-application-recovery-2026-01-08" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 4: ArgoCD Application Recovery (2026-01-08)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 4: ArgoCD Application Recovery (2026-01-08)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-github-actions-runner-controller-finalizer-issues-primary" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 GitHub Actions Runner Controller Finalizer Issues (Primary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-github-actions-runner-controller-state-drift-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 GitHub Actions Runner Controller State Drift (Secondary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-linkace-helm-chart-configuration-drift-primary" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 LinkAce Helm Chart Configuration Drift (Primary)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-home-assistant-application-self-healing-none" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.4 Home Assistant Application Self-Healing (None)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-5-k8s01-container-runtime-corruption-recurrence-2026-01-09" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 5: k8s01 Container Runtime Corruption Recurrence (2026-01-09)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 5: k8s01 Container Runtime Corruption Recurrence (2026-01-09)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-persistent-container-runtime-corruption-on-k8s01-critical" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 Persistent Container Runtime Corruption on k8s01 (Critical)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-detection-gap-for-node-local-failures-secondary" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 Detection Gap for Node-Local Failures (Secondary)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#impact" class="md-nav__link">
    <span class="md-ellipsis">
      
        Impact
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Impact">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#services-affected" class="md-nav__link">
    <span class="md-ellipsis">
      
        Services Affected
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#duration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Duration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scope" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scope
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resolution-steps-taken" class="md-nav__link">
    <span class="md-ellipsis">
      
        Resolution Steps Taken
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Resolution Steps Taken">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#phase-1-node-and-kubelet-recovery" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 1: Node and Kubelet Recovery
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 1: Node and Kubelet Recovery">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-k8s02-kubelet-restart" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. k8s02 Kubelet Restart
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-k8s01-disk-cleanup-and-stabilization" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. k8s01 Disk Cleanup and Stabilization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-k8s03-disk-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. k8s03 Disk Cleanup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-runner-controller-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Runner Controller Cleanup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-openebs-replica-pod-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. OpenEBS Replica Pod Cleanup
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-2-storage-and-ingress-recovery" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 2: Storage and Ingress Recovery
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 2: Storage and Ingress Recovery">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#6-k8s01-full-restart-residual-issues" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. k8s01 Full Restart (Residual Issues)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-jiva-snapshot-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Jiva Snapshot Cleanup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8-ingress-controller-refresh" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Ingress Controller Refresh
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#9-radarr-volume-emergency-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        9. Radarr Volume Emergency Cleanup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#10-k8s03-full-restart-replica-pod-issues" class="md-nav__link">
    <span class="md-ellipsis">
      
        10. k8s03 Full Restart (Replica Pod Issues)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-job-controller-and-database-recovery-nuclear-option" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 3: Job Controller and Database Recovery (Nuclear Option)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 3: Job Controller and Database Recovery (Nuclear Option)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-cluster-wide-restart-with-database-backup" class="md-nav__link">
    <span class="md-ellipsis">
      
        11. Cluster-Wide Restart with Database Backup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-clean-job-and-cronjob-state" class="md-nav__link">
    <span class="md-ellipsis">
      
        12. Clean Job and CronJob State
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-update-argocd-manifest-with-timeout-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      
        13. Update ArgoCD Manifest with Timeout Configuration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-verification" class="md-nav__link">
    <span class="md-ellipsis">
      
        14. Verification
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-4-argocd-application-and-finalizer-recovery" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 4: ArgoCD Application and Finalizer Recovery
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 4: ArgoCD Application and Finalizer Recovery">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#15-remove-finalizers-from-stuck-resources" class="md-nav__link">
    <span class="md-ellipsis">
      
        15. Remove Finalizers from Stuck Resources
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-clean-up-old-runner-controller-resources" class="md-nav__link">
    <span class="md-ellipsis">
      
        16. Clean Up Old Runner Controller Resources
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#17-fix-linkace-helm-chart-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      
        17. Fix LinkAce Helm Chart Configuration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#18-verification" class="md-nav__link">
    <span class="md-ellipsis">
      
        18. Verification
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-5-k8s01-container-runtime-recovery" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 5: k8s01 Container Runtime Recovery
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Phase 5: k8s01 Container Runtime Recovery">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#19-k8s01-container-runtime-investigation" class="md-nav__link">
    <span class="md-ellipsis">
      
        19. k8s01 Container Runtime Investigation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#20-k8s01-microk8s-restart" class="md-nav__link">
    <span class="md-ellipsis">
      
        20. k8s01 MicroK8s Restart
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#21-verification" class="md-nav__link">
    <span class="md-ellipsis">
      
        21. Verification
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#verification" class="md-nav__link">
    <span class="md-ellipsis">
      
        Verification
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Verification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#service-health-checks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Service Health Checks
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#infrastructure-health" class="md-nav__link">
    <span class="md-ellipsis">
      
        Infrastructure Health
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#volume-replication" class="md-nav__link">
    <span class="md-ellipsis">
      
        Volume Replication
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#node-disk-status-post-cleanup" class="md-nav__link">
    <span class="md-ellipsis">
      
        Node Disk Status (Post-Cleanup)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#preventive-measures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Preventive Measures
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Preventive Measures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#immediate-actions-required" class="md-nav__link">
    <span class="md-ellipsis">
      
        Immediate Actions Required
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#longer-term-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      
        Longer-Term Improvements
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lessons-learned" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lessons Learned
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lessons Learned">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-went-well" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Went Well
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-didnt-go-well" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Didn't Go Well
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#surprise-findings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Surprise Findings
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#action-items" class="md-nav__link">
    <span class="md-ellipsis">
      
        Action Items
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#technical-details" class="md-nav__link">
    <span class="md-ellipsis">
      
        Technical Details
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Technical Details">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#environment" class="md-nav__link">
    <span class="md-ellipsis">
      
        Environment
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#affected-resources" class="md-nav__link">
    <span class="md-ellipsis">
      
        Affected Resources
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#snapshot-cleanup-job-output" class="md-nav__link">
    <span class="md-ellipsis">
      
        Snapshot Cleanup Job Output
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#node-disk-usage-timeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        Node Disk Usage Timeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kubelet-error-patterns-phase-1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Kubelet Error Patterns (Phase 1)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reviewers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reviewers
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Notes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Notes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cross-phase-insights" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cross-Phase Insights
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#phase-3-specific-insights-job-controller-corruption" class="md-nav__link">
    <span class="md-ellipsis">
      
        Phase 3-Specific Insights (Job Controller Corruption)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="post-incident-review-cascading-kubernetes-cluster-failures">Post Incident Review: Cascading Kubernetes Cluster Failures<a class="headerlink" href="#post-incident-review-cascading-kubernetes-cluster-failures" title="Permanent link">&para;</a></h1>
<p><strong>Date:</strong> 2026-01-06
<strong>Duration:</strong> ~8 hours (estimated 09:00 - 17:00 AEST)
<strong>Severity:</strong> Critical (Complete cluster instability, multiple service outages)
<strong>Status:</strong> Resolved</p>
<hr />
<h2 id="executive-summary">Executive Summary<a class="headerlink" href="#executive-summary" title="Permanent link">&para;</a></h2>
<p>A cascading failure across the microk8s Kubernetes cluster began with unplanned node reboots, leading to widespread kubelet failures, disk exhaustion, controller corruption, and ultimately service outages. The incident progressed through five distinct phases spanning January 6-9, 2026:</p>
<p><strong>Phase 1 (2026-01-06 09:00-12:30):</strong> Cascading node failures caused kubelet hangs on all three nodes due to disk pressure (97-100% usage), audit buffer overload, and orphaned pod accumulation. The cluster reached a critical state where pods could not be scheduled, started, or terminated. 571 orphaned GitHub Actions runner pods and 22 stuck OpenEBS replica pods contributed to resource exhaustion.</p>
<p><strong>Phase 2 (2026-01-06 12:30-15:35):</strong> After stabilizing node operations, secondary issues emerged: OpenEBS Jiva volume snapshot accumulation (1011+ snapshots), ingress controller endpoint caching failures, and volume capacity exhaustion. Multiple media services (Sonarr, Radarr, Overseerr) became inaccessible.</p>
<p><strong>Phase 3 (2026-01-08 02:00-18:25):</strong> Job controller corruption prevented all cluster-wide job creation for 16.5 hours. Required nuclear option (cluster restart with dqlite backup) to resolve persistent database state corruption originating from Phase 1.</p>
<p><strong>Phase 4 (2026-01-08 19:00-19:45):</strong> ArgoCD application recovery required manual finalizer removal and configuration fixes for GitHub Actions runner controllers and LinkAce cronjob.</p>
<p><strong>Phase 5 (2026-01-09 09:50-09:55):</strong> k8s01 container runtime corruption recurred 48+ hours after Phase 3 nuclear option, demonstrating that cluster restart cleared cluster-global state but not node-local container runtime issues. 4 runner pods stuck Pending for 12+ hours due to silent failure pattern.</p>
<p>Resolution required systematic intervention across multiple infrastructure layers: node recovery, disk cleanup, pod force-deletion, storage subsystem repair, ingress refresh, database backup/restart, and multiple node-local container runtime restarts. All services restored to full functionality with complete volume replication (3/3 replicas).</p>
<hr />
<h2 id="timeline-aest-utc10">Timeline (AEST - UTC+10)<a class="headerlink" href="#timeline-aest-utc10" title="Permanent link">&para;</a></h2>
<h3 id="phase-1-cascading-node-and-kubelet-failures">Phase 1: Cascading Node and Kubelet Failures<a class="headerlink" href="#phase-1-cascading-node-and-kubelet-failures" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Time</th>
<th>Event</th>
</tr>
</thead>
<tbody>
<tr>
<td>~09:00</td>
<td><strong>INCIDENT START</strong>: Unplanned node reboots across k8s01, k8s02, k8s03 (likely power event or scheduled maintenance)</td>
</tr>
<tr>
<td>09:15-09:30</td>
<td>Cluster returns online but exhibits severe instability: pods not scheduling, not starting, not terminating</td>
</tr>
<tr>
<td>09:30-10:00</td>
<td>Initial diagnostics: Control plane components healthy, scheduler functioning, but kubelets not processing assigned pods</td>
</tr>
<tr>
<td>10:00-10:15</td>
<td>Identified k8s02 kubelet hung: pods assigned by scheduler but never reaching ContainerCreating state</td>
</tr>
<tr>
<td>10:15-10:20</td>
<td><strong>RESOLUTION 1.1</strong>: Restarted kubelite on k8s02 (<code>systemctl restart snap.microk8s.daemon-kubelite</code>)</td>
</tr>
<tr>
<td>10:20-10:30</td>
<td>k8s01 kubelet repeatedly crashing: "Kubelet stopped posting node status" within minutes of restart</td>
</tr>
<tr>
<td>10:30-10:45</td>
<td>Root cause analysis k8s01: Disk at 97% usage + audit buffer overload ("audit buffer queue blocked" errors)</td>
</tr>
<tr>
<td>10:45-11:00</td>
<td>Database lock errors in kine (etcd replacement): "database is locked" preventing state updates</td>
</tr>
<tr>
<td>11:00-11:15</td>
<td>k8s03 diagnostics: Disk at 100% capacity with garbage collection failures</td>
</tr>
<tr>
<td>11:15-11:30</td>
<td>Discovered 571 orphaned GitHub Actions runner pods in ci namespace (deployment scaled to 0 but pods remained)</td>
</tr>
<tr>
<td>11:30-11:45</td>
<td><strong>RESOLUTION 1.2</strong>: Disk cleanup on k8s01 (container images, logs) reducing from 97%  87% usage</td>
</tr>
<tr>
<td>11:45-12:00</td>
<td><strong>RESOLUTION 1.3</strong>: Disk cleanup on k8s03 reducing from 100%  81% usage</td>
</tr>
<tr>
<td>12:00-12:15</td>
<td>k8s01 kubelet stabilized after disk cleanup, node maintaining Ready status</td>
</tr>
<tr>
<td>12:15-12:20</td>
<td>Deleted RunnerDeployment and HorizontalRunnerAutoscaler (GitHub Actions runner controller orphaned)</td>
</tr>
<tr>
<td>12:20-12:25</td>
<td>Force-deleted 22 OpenEBS replica pods stuck in Terminating state</td>
</tr>
<tr>
<td>12:25-12:30</td>
<td>Began aggressive force-deletion of 571 runner pods in batches (Pending, ContainerStatusUnknown, StartError, Completed)</td>
</tr>
</tbody>
</table>
<h3 id="phase-2-storage-and-ingress-service-outages">Phase 2: Storage and Ingress Service Outages<a class="headerlink" href="#phase-2-storage-and-ingress-service-outages" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Time</th>
<th>Event</th>
</tr>
</thead>
<tbody>
<tr>
<td>~12:30</td>
<td><strong>PHASE 2 START</strong>: User reports 504 Gateway Timeout errors for Sonarr at https://sonarr.int.pgmac.net/</td>
</tr>
<tr>
<td>12:30-12:45</td>
<td>Initial investigation: Examined ingress controller logs showing upstream timeouts to pods at old IP addresses (10.1.236.34:8989 for Sonarr, etc.)</td>
</tr>
<tr>
<td>12:45-13:00</td>
<td>Root cause analysis: Discovered Radarr pod in CrashLoopBackOff with "No space left on device" error. Sonarr pod Pending on k8s01 node.</td>
</tr>
<tr>
<td>13:00-13:15</td>
<td>Volume analysis: Identified OpenEBS Jiva volumes with excessive snapshots (1011 vs 500 threshold) affecting Radarr, Sonarr, and Overseerr</td>
</tr>
<tr>
<td>13:15-13:30</td>
<td>Node troubleshooting: Identified k8s01 node unable to start new containers despite being healthy (residual kubelet issues from Phase 1)</td>
</tr>
<tr>
<td>13:30-13:35</td>
<td><strong>RESOLUTION 2.1</strong>: Restarted microk8s on k8s01, resolving pod scheduling issues</td>
</tr>
<tr>
<td>13:35-14:00</td>
<td>Snapshot cleanup: Triggered manual Jiva snapshot cleanup job (jiva-snapshot-cleanup-manual)</td>
</tr>
<tr>
<td>13:52</td>
<td>Cleanup job started processing Overseerr volume (pvc-05e03b60)</td>
</tr>
<tr>
<td>13:57</td>
<td>Sonarr volume (pvc-17e6e808) cleanup completed</td>
</tr>
<tr>
<td>14:10</td>
<td><strong>RESOLUTION 2.2</strong>: Restarted all 3 ingress controller pods to clear stale endpoint cache</td>
</tr>
<tr>
<td>14:11</td>
<td><strong>SERVICE RESTORED</strong>: Sonarr accessible at https://sonarr.int.pgmac.net/ (200 OK responses)</td>
</tr>
<tr>
<td>14:15</td>
<td>Overseerr confirmed accessible (200 OK responses)</td>
</tr>
<tr>
<td>14:20</td>
<td>Radarr volume (pvc-311bef00) cleanup completed</td>
</tr>
<tr>
<td>14:25</td>
<td>Radarr pod still crashing: volume at 100% capacity (958M/974M used)</td>
</tr>
<tr>
<td>14:28</td>
<td><strong>RESOLUTION 2.3</strong>: Cleared 49M of old backups from Radarr volume, reducing to 95% usage</td>
</tr>
<tr>
<td>14:30</td>
<td><strong>SERVICE RESTORED</strong>: Radarr accessible at https://radarr.int.pgmac.net/</td>
</tr>
<tr>
<td>14:35</td>
<td>Identified 8-9 Jiva replica pods stuck in Pending state on k8s03 (residual from Phase 1)</td>
</tr>
<tr>
<td>~15:30</td>
<td><strong>RESOLUTION 2.4</strong>: Restarted microk8s on k8s03, resolving all Pending replica pods</td>
</tr>
<tr>
<td>15:35</td>
<td><strong>INCIDENT END</strong>: All services operational, all replicas running (3/3), no problematic pods</td>
</tr>
</tbody>
</table>
<h3 id="cleanup-operations-parallel-with-phase-2">Cleanup Operations (Parallel with Phase 2)<a class="headerlink" href="#cleanup-operations-parallel-with-phase-2" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Time</th>
<th>Event</th>
</tr>
</thead>
<tbody>
<tr>
<td>12:30-12:45</td>
<td>Force-deleted 299 Pending runner pods</td>
</tr>
<tr>
<td>12:45-13:00</td>
<td>Force-deleted 110 ContainerStatusUnknown runner pods</td>
</tr>
<tr>
<td>13:00-13:15</td>
<td>Force-deleted 58 StartError/RunContainerError runner pods</td>
</tr>
<tr>
<td>13:15-13:30</td>
<td>Force-deleted 79 Completed runner pods</td>
</tr>
<tr>
<td>13:30-14:00</td>
<td>Force-deleted final batch of 247 non-Running/non-Terminating runner pods</td>
</tr>
<tr>
<td>14:00</td>
<td>Runner pod cleanup substantially complete: 393 pods remain (128 Terminating, 18 Running, 247 deleted)</td>
</tr>
</tbody>
</table>
<h3 id="phase-3-linkace-cronjob-controller-corruption-2026-01-08-0200-1825">Phase 3: LinkAce CronJob Controller Corruption (2026-01-08 02:00-18:25)<a class="headerlink" href="#phase-3-linkace-cronjob-controller-corruption-2026-01-08-0200-1825" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Time</th>
<th>Event</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>2026-01-08 ~02:00</strong></td>
<td><strong>PHASE 3 START</strong>: LinkAce cronjob (<code>* * * * *</code> schedule) begins failing to create jobs successfully</td>
</tr>
<tr>
<td>02:00-05:00</td>
<td>Cronjob creates job objects but pods orphaned (parent job deleted before pod creation)</td>
</tr>
<tr>
<td>05:00-05:30</td>
<td>44+ jobs stuck in Running state (0/1 completions, 6min-11h old), 24+ pods Pending</td>
</tr>
<tr>
<td>05:30-06:00</td>
<td>Investigation reveals job controller stuck syncing deleted job <code>linkace-cronjob-29463021</code></td>
</tr>
<tr>
<td>06:00-06:15</td>
<td>Job controller logs: "syncing job: tracking status: jobs.batch not found" errors</td>
</tr>
<tr>
<td>06:15-06:30</td>
<td>Cleanup attempts: Suspended cronjob, deleted orphaned pods, cleared stale active jobs</td>
</tr>
<tr>
<td>06:30-07:00</td>
<td>Restarted kubelite on k8s01, temporary improvement but orphaned job reference persists</td>
</tr>
<tr>
<td>07:00-08:00</td>
<td>Created dummy job with stale name and deleted properly, but new jobs still not creating pods</td>
</tr>
<tr>
<td>08:00-09:00</td>
<td>User added timeout configuration to ArgoCD manifest (activeDeadlineSeconds: 300, ttlSecondsAfterFinished: 120)</td>
</tr>
<tr>
<td>09:00-09:30</td>
<td>ArgoCD synced configuration successfully but cronjob deleted to recreate cleanly</td>
</tr>
<tr>
<td>09:30-10:00</td>
<td>ArgoCD failed to auto-recreate deleted cronjob despite OutOfSync status</td>
</tr>
<tr>
<td>10:00-10:30</td>
<td>Manually recreated cronjob, but job controller completely wedged (not creating pods for any jobs)</td>
</tr>
<tr>
<td>10:30-18:00</td>
<td>Self-healing attempted: waited 1.5 hours for TTL cleanup and active deadline enforcement - <strong>failed</strong></td>
</tr>
<tr>
<td>18:00-18:05</td>
<td>Jobs created by cronjob but no pods spawned, active deadline not enforced (jobs 85+ min old still Running)</td>
</tr>
<tr>
<td>18:05-18:10</td>
<td>TTL cleanup not working (no jobs auto-deleted after completion)</td>
</tr>
<tr>
<td>18:10</td>
<td><strong>DECISION</strong>: Nuclear option approved - etcd cleanup with cluster restart</td>
</tr>
<tr>
<td>18:12-18:15</td>
<td><strong>RESOLUTION 3.1</strong>: Stopped MicroK8s on all 3 nodes (k8s01, k8s02, k8s03)</td>
</tr>
<tr>
<td>18:15</td>
<td><strong>RESOLUTION 3.2</strong>: Backed up etcd/dqlite database to <code>/var/snap/microk8s/common/backup/etcd-backup-20260108-201540</code></td>
</tr>
<tr>
<td>18:15-18:17</td>
<td><strong>RESOLUTION 3.3</strong>: Restarted MicroK8s cluster, all nodes returned Ready</td>
</tr>
<tr>
<td>18:17-18:18</td>
<td><strong>RESOLUTION 3.4</strong>: Force-deleted all stuck jobs and cronjob</td>
</tr>
<tr>
<td>18:18-18:19</td>
<td><strong>RESOLUTION 3.5</strong>: Triggered ArgoCD sync to recreate cronjob with fresh state</td>
</tr>
<tr>
<td>18:19</td>
<td>Cronjob recreated successfully with all timeout settings applied</td>
</tr>
<tr>
<td>18:20-18:22</td>
<td>First job (<code>linkace-cronjob-29464462</code>) created successfully, completed in 7 seconds</td>
</tr>
<tr>
<td>18:22-18:25</td>
<td>TTL cleanup verified working: completed jobs auto-deleted after 2 minutes</td>
</tr>
<tr>
<td>18:25</td>
<td><strong>INCIDENT END</strong>: Cronjob fully functional, no orphaned pods, all cleanup mechanisms working</td>
</tr>
</tbody>
</table>
<h3 id="phase-4-argocd-application-recovery-2026-01-08-1900-1945">Phase 4: ArgoCD Application Recovery (2026-01-08 ~19:00-19:45)<a class="headerlink" href="#phase-4-argocd-application-recovery-2026-01-08-1900-1945" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Time</th>
<th>Event</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>2026-01-08 ~19:00</strong></td>
<td><strong>PHASE 4 START</strong>: Investigation of 5 ArgoCD applications stuck OutOfSync or Progressing</td>
</tr>
<tr>
<td>19:00-19:05</td>
<td>Identified problematic applications: ci-tools (OutOfSync + Progressing), gharc-runners-pgmac-net-self-hosted (OutOfSync + Healthy), gharc-runners-pgmac-user-self-hosted (Synced + Progressing), hass (Synced + Progressing), linkace (OutOfSync + Healthy)</td>
</tr>
<tr>
<td>19:05-19:15</td>
<td><strong>ci-tools investigation</strong>: Found child application <code>gharc-runners-pgmac-user-self-hosted</code> stuck with resources "Pending deletion"</td>
</tr>
<tr>
<td>19:15-19:18</td>
<td><strong>RESOLUTION 4.1</strong>: Removed finalizers from 4 stuck resources (AutoscalingRunnerSet, ServiceAccount, Role, RoleBinding) using <code>kubectl patch --type json -p='[{"op": "remove", "path": "/metadata/finalizers"}]'</code></td>
</tr>
<tr>
<td>19:18-19:20</td>
<td>Triggered ArgoCD sync for ci-tools, application became Synced + Healthy</td>
</tr>
<tr>
<td>19:20-19:25</td>
<td><strong>gharc-runners-pgmac-net-self-hosted investigation</strong>: Found old listener resources with hash <code>754b578d</code> needing deletion</td>
</tr>
<tr>
<td>19:25-19:28</td>
<td>Deleted 3 old listener resources manually (ServiceAccount, Role, RoleBinding)</td>
</tr>
<tr>
<td>19:28-19:30</td>
<td>Discovered 6 runner pods stuck Pending for 44+ minutes (residual from Phase 3 job controller corruption)</td>
</tr>
<tr>
<td>19:30-19:32</td>
<td>Force-deleted 6 stuck runner pods: <code>pgmac-renovatebot-*</code> pods with PodScheduled=True but no containers created</td>
</tr>
<tr>
<td>19:32-19:33</td>
<td>Application status: OutOfSync + Healthy (acceptable due to ignoreDifferences configuration for AutoscalingListener, Role, RoleBinding)</td>
</tr>
<tr>
<td>19:33-19:35</td>
<td><strong>gharc-runners-pgmac-user-self-hosted</strong>: Already deleted during ci-tools cleanup</td>
</tr>
<tr>
<td>19:35-19:37</td>
<td><strong>hass investigation</strong>: Application self-resolved during investigation, showing Synced + Healthy (StatefulSet rollout completed)</td>
</tr>
<tr>
<td>19:37-19:40</td>
<td><strong>linkace investigation</strong>: Found linkace-cronjob OutOfSync despite application Healthy, ArgoCD attempted 23 auto-heal operations</td>
</tr>
<tr>
<td>19:40-19:42</td>
<td>Root cause identified: LinkAce Helm chart doesn't support <code>backoffLimit</code> and <code>resources</code> configuration in cronjob</td>
</tr>
<tr>
<td>19:42-19:43</td>
<td><strong>RESOLUTION 4.2</strong>: Edited <code>/Users/paulmacdonnell/pgmac/pgk8s/pgmac.net/media/templates/linkace.yaml</code> to remove unsupported fields (backoffLimit, resources block)</td>
</tr>
<tr>
<td>19:43</td>
<td>Kept critical timeout settings: startingDeadlineSeconds, activeDeadlineSeconds, ttlSecondsAfterFinished, history limits</td>
</tr>
<tr>
<td>19:43-19:44</td>
<td>Committed changes with message "Remove unsupported LinkAce cronjob configuration"</td>
</tr>
<tr>
<td>19:44</td>
<td>Git push rejected due to remote changes, used <code>git stash &amp;&amp; git pull --rebase &amp;&amp; git stash pop &amp;&amp; git push</code></td>
</tr>
<tr>
<td>19:45</td>
<td><strong>PHASE 4 END</strong>: All applications resolved or explained; 2 applications Synced + Healthy (ci-tools, hass), 2 applications OutOfSync + Healthy acceptable (gharc-runners-pgmac-net-self-hosted, linkace), 1 application deleted (gharc-runners-pgmac-user-self-hosted)</td>
</tr>
</tbody>
</table>
<h3 id="phase-5-k8s01-container-runtime-corruption-recurrence-2026-01-09-0950-0955">Phase 5: k8s01 Container Runtime Corruption Recurrence (2026-01-09 ~09:50-09:55)<a class="headerlink" href="#phase-5-k8s01-container-runtime-corruption-recurrence-2026-01-09-0950-0955" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Time</th>
<th>Event</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>2026-01-09 ~09:50</strong></td>
<td><strong>PHASE 5 START</strong>: Investigation revealed 4 runner pods in arc-runners namespace stuck in Pending state for 12+ hours</td>
</tr>
<tr>
<td>09:50-09:51</td>
<td>Identified all 4 Pending pods assigned to k8s01 node: self-hosted-l52x9-runner-2nnsr, -69qnv, -ls8c2, -w8mcd</td>
</tr>
<tr>
<td>09:51</td>
<td>Pod describe showed PodScheduled=True but no container initialization, no events generated (silent failure pattern from Phase 2/3)</td>
</tr>
<tr>
<td>09:51-09:52</td>
<td>Verified k8s01 node showing Ready status despite being unable to start new containers</td>
</tr>
<tr>
<td>09:52</td>
<td><strong>Root cause identified</strong>: Container runtime state corruption on k8s01 (residual from Phase 1-3, not fully cleared by Phase 3 nuclear option)</td>
</tr>
<tr>
<td>09:52-09:53</td>
<td>Found 10 EphemeralRunner resources but only 4 pods exist (6 pgmac-slack-scores runners have no pods at all)</td>
</tr>
<tr>
<td>09:53</td>
<td><strong>RESOLUTION 5.1</strong>: User restarted microk8s on k8s01 (<code>microk8s stop &amp;&amp; microk8s start</code>)</td>
</tr>
<tr>
<td>09:55</td>
<td><strong>PHASE 5 END</strong>: All 4 Pending pods cleared, container runtime recovered</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="root-causes">Root Causes<a class="headerlink" href="#root-causes" title="Permanent link">&para;</a></h2>
<h3 id="phase-1-node-and-control-plane-failures">Phase 1: Node and Control Plane Failures<a class="headerlink" href="#phase-1-node-and-control-plane-failures" title="Permanent link">&para;</a></h3>
<h4 id="11-cascading-node-reboots-primary-trigger">1.1 Cascading Node Reboots (Primary Trigger)<a class="headerlink" href="#11-cascading-node-reboots-primary-trigger" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: All three nodes (k8s01, k8s02, k8s03) experienced unplanned reboots</li>
<li><strong>Likely cause</strong>: Power event, scheduled maintenance, or infrastructure issue</li>
<li><strong>Impact</strong>: Triggered cascade of secondary failures during recovery</li>
<li><strong>Why it cascaded</strong>:</li>
<li>Simultaneous reboot prevented graceful pod migration</li>
<li>etcd state (via kine) became inconsistent across nodes</li>
<li>Container runtime state corrupted on restart</li>
<li>Disk pressure accumulated during downtime (logs, audit buffers, orphaned containers)</li>
</ul>
<h4 id="12-k8s01-kubelet-crash-loop-critical">1.2 k8s01 Kubelet Crash Loop (Critical)<a class="headerlink" href="#12-k8s01-kubelet-crash-loop-critical" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: Kubelet repeatedly crashing within minutes of restart</li>
<li><strong>Root causes</strong>:</li>
<li><strong>Disk exhaustion</strong>: 97% usage preventing kubelet operations</li>
<li><strong>Audit buffer overload</strong>: "audit buffer queue blocked" errors in logs</li>
<li><strong>Database locks</strong>: kine (etcd replacement) showing "database is locked" errors</li>
<li><strong>Impact</strong>: Node oscillating between Ready/NotReady, unable to start/stop pods</li>
<li><strong>Why it happened</strong>:</li>
<li>Container image accumulation from 4+ years of operations</li>
<li>Log rotation not keeping pace with audit log generation</li>
<li>Kubelet requires &gt;10% free disk to function properly</li>
<li><strong>Resolution</strong>: Disk cleanup (97%  87%), container image removal, log pruning</li>
</ul>
<h4 id="13-k8s02-kubelet-process-hang-critical">1.3 k8s02 Kubelet Process Hang (Critical)<a class="headerlink" href="#13-k8s02-kubelet-process-hang-critical" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: Kubelet not processing newly assigned pods</li>
<li><strong>Symptoms</strong>: Pods assigned by scheduler (PodScheduled=True) but never reaching ContainerCreating</li>
<li><strong>Root cause</strong>: Kubelet process corrupted/hung after node reboot</li>
<li><strong>Impact</strong>: Entire node unable to start new containers despite reporting Ready status</li>
<li><strong>Resolution</strong>: kubelite service restart (<code>systemctl restart snap.microk8s.daemon-kubelite</code>)</li>
</ul>
<h4 id="14-k8s03-disk-exhaustion-critical">1.4 k8s03 Disk Exhaustion (Critical)<a class="headerlink" href="#14-k8s03-disk-exhaustion-critical" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: Disk at 100% capacity</li>
<li><strong>Symptoms</strong>: "Failed to garbage collect required amount of images. Attempted to free 13GB, but only found 0 bytes eligible to free"</li>
<li><strong>Impact</strong>:</li>
<li>Prevented container image pulls</li>
<li>Blocked new pod scheduling</li>
<li>Contributed to 8-9 Jiva replica pods stuck in Pending</li>
<li><strong>Resolution</strong>: User disk cleanup (100%  81%)</li>
</ul>
<h4 id="15-github-actions-runner-controller-orphaned-pods-secondary">1.5 GitHub Actions Runner Controller Orphaned Pods (Secondary)<a class="headerlink" href="#15-github-actions-runner-controller-orphaned-pods-secondary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: 571 orphaned runner pods remained after RunnerDeployment scaled to 0</li>
<li><strong>Affected namespace</strong>: ci</li>
<li><strong>Pod states</strong>: 299 Pending, 110 ContainerStatusUnknown, 79 Completed, 58 StartError, 25+ other stuck states</li>
<li><strong>Why it happened</strong>:</li>
<li>RunnerDeployment controller existed but in Error state</li>
<li>Pods orphaned from parent controller (reboot disrupted controller finalizers)</li>
<li>No automated cleanup triggered despite 0 replicas</li>
<li><strong>Impact</strong>:</li>
<li>Consumed scheduler resources attempting to place Pending pods</li>
<li>Consumed API server resources with status updates</li>
<li>Contributed to disk pressure (container image layers, logs)</li>
<li><strong>Resolution</strong>:</li>
<li>Deleted RunnerDeployment and HorizontalRunnerAutoscaler</li>
<li>Force-deleted 546+ pods in batches using <code>--force --grace-period=0 --wait=false</code></li>
</ul>
<h4 id="16-openebs-replica-pods-stuck-terminating-secondary">1.6 OpenEBS Replica Pods Stuck Terminating (Secondary)<a class="headerlink" href="#16-openebs-replica-pods-stuck-terminating-secondary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: 22 OpenEBS Jiva replica pods stuck in Terminating state</li>
<li><strong>Impact</strong>: Storage subsystem instability, prevented volume operations</li>
<li><strong>Resolution</strong>: Force-deleted all 22 pods</li>
</ul>
<h3 id="phase-2-storage-and-ingress-failures">Phase 2: Storage and Ingress Failures<a class="headerlink" href="#phase-2-storage-and-ingress-failures" title="Permanent link">&para;</a></h3>
<h4 id="21-openebs-jiva-snapshot-accumulation-primary">2.1 OpenEBS Jiva Snapshot Accumulation (Primary)<a class="headerlink" href="#21-openebs-jiva-snapshot-accumulation-primary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: Jiva volumes accumulated 1011 snapshots (threshold: 500)</li>
<li><strong>Affected Volumes</strong>:</li>
<li>Radarr config (pvc-311bef00-1b89-4584-90f0-ae3772e30e09)</li>
<li>Sonarr config (pvc-17e6e808-a9fc-4f64-b490-71deffdb81fd)</li>
<li>Overseerr config (pvc-05e03b60-3ab7-41a0-9baf-3d0e291eed63)</li>
<li>Multiple other Jiva volumes cluster-wide</li>
<li><strong>Impact</strong>: Radarr volume reached capacity (3GB physical snapshots in 1Gi volume), causing "No space left on device" errors</li>
<li><strong>Why it happened</strong>:</li>
<li>Automated cleanup cronjob exists (<code>jiva-snapshot-cleanup</code>) running daily at 2 AM</li>
<li><strong>Connection to Phase 1</strong>: Node reboots and kubelet failures prevented cronjob pods from running for 2-3 days</li>
<li>Snapshot accumulation rate exceeded cleanup frequency</li>
<li>1011 snapshots suggests cronjob not executing successfully during Phase 1 disk/kubelet issues</li>
<li><strong>Resolution</strong>: Manual snapshot cleanup job triggered after node stabilization</li>
</ul>
<h4 id="22-residual-node-container-runtime-issues">2.2 Residual Node Container Runtime Issues<a class="headerlink" href="#22-residual-node-container-runtime-issues" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>k8s01 Node</strong> (post Phase 1 cleanup): Unable to start new containers despite node reporting Ready status</li>
<li>Pods scheduled successfully but containers never initialized</li>
<li>Affected: Sonarr pod, linkace-cronjob, cleanup job pod</li>
<li><strong>Connection to Phase 1</strong>: Kubelet state corruption persisted despite disk cleanup</li>
<li>
<p>Resolution: Full microk8s restart (not just kubelite)</p>
</li>
<li>
<p><strong>k8s03 Node</strong>: Similar container startup issues</p>
</li>
<li>8-9 Jiva replica pods stuck in Pending with PodScheduled=True but no container creation</li>
<li><strong>Connection to Phase 1</strong>: Disk exhaustion (100%  81%) required full cluster restart to clear runtime state</li>
<li>Resolution: microk8s restart resolved all Pending pods</li>
</ul>
<h4 id="23-ingress-controller-stale-endpoint-cache">2.3 Ingress Controller Stale Endpoint Cache<a class="headerlink" href="#23-ingress-controller-stale-endpoint-cache" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: Nginx ingress controllers retained old pod IP addresses after pod restarts</li>
<li>Example: Sonarr old IP 10.1.236.34:8989 vs new IP 10.1.73.92:8989</li>
<li>Resulted in 504 Gateway Timeout errors despite healthy backend pods</li>
<li><strong>Why it happened</strong>: Pod IP changes during Phase 1 chaos not reflected in ingress controller endpoint cache</li>
<li><strong>Resolution</strong>: Restarting all 3 ingress controller pods refreshed endpoint cache</li>
</ul>
<h4 id="24-radarr-volume-capacity-secondary">2.4 Radarr Volume Capacity (Secondary)<a class="headerlink" href="#24-radarr-volume-capacity-secondary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: Radarr config PVC at 100% capacity (1Gi volume, 958M used)</li>
<li><strong>Breakdown</strong>:</li>
<li>MediaCover directory: 837M (movie posters/artwork)</li>
<li>Backups: 49M</li>
<li>Database: 33M</li>
<li>Other: ~39M</li>
<li><strong>Resolution</strong>: Cleared old backups freeing 49M (temporary fix to 95% usage)</li>
<li><strong>Long-term concern</strong>: Volume undersized for media library artwork</li>
</ul>
<h3 id="phase-3-linkace-cronjob-controller-corruption-2026-01-08">Phase 3: LinkAce CronJob Controller Corruption (2026-01-08)<a class="headerlink" href="#phase-3-linkace-cronjob-controller-corruption-2026-01-08" title="Permanent link">&para;</a></h3>
<h4 id="31-job-controller-state-corruption-primary-critical">3.1 Job Controller State Corruption (Primary - Critical)<a class="headerlink" href="#31-job-controller-state-corruption-primary-critical" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: Job controller stuck in error loop trying to sync deleted job <code>linkace-cronjob-29463021</code></li>
<li><strong>Symptoms</strong>:</li>
<li>Jobs created by cronjob but pods never spawned</li>
<li>Job objects showing "Running" with 0 Active/Succeeded/Failed pods</li>
<li>No events generated for newly created jobs</li>
<li>Controller logs: <code>"syncing job: tracking status: adding uncounted pods to status: jobs.batch \"linkace-cronjob-29463021\" not found"</code></li>
<li><strong>Impact</strong>: Complete failure of all job creation cluster-wide, not just LinkAce cronjob</li>
<li><strong>Why it happened</strong>:</li>
<li><strong>Connection to Phase 1</strong>: Job controller corruption originated from cascading failures on 2026-01-06</li>
<li>Stale job reference persisted in controller's in-memory state after Phase 1 reboots</li>
<li>Controller unable to clear orphaned reference without full cluster restart</li>
<li>MicroK8s dqlite database retained corrupted job metadata</li>
<li><strong>Resolution</strong>: Nuclear option - cluster restart with dqlite backup</li>
</ul>
<h4 id="32-dqlite-database-state-corruption-primary">3.2 Dqlite Database State Corruption (Primary)<a class="headerlink" href="#32-dqlite-database-state-corruption-primary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: MicroK8s dqlite database retained stale job references preventing controller recovery</li>
<li><strong>Symptoms</strong>:</li>
<li>Job controller restart didn't resolve issue (state persisted in database)</li>
<li>Manually recreating cronjob didn't clear corruption</li>
<li>Creating dummy job with stale name and deleting didn't clear reference</li>
<li>Multiple kubelite restarts across all nodes failed to resolve</li>
<li><strong>Impact</strong>: Self-healing mechanisms completely ineffective</li>
<li><strong>Why it happened</strong>:</li>
<li>Phase 1 cascading failures (disk pressure, kubelet crashes) corrupted dqlite write operations</li>
<li>Job deletion operations during Phase 1 chaos didn't complete atomically</li>
<li>Dqlite state diverged across 3 nodes during simultaneous kubelet failures</li>
<li><strong>Resolution</strong>: Stopped cluster, backed up dqlite database, restarted to clear in-memory state</li>
</ul>
<h4 id="33-timeout-configuration-not-enforced-secondary">3.3 Timeout Configuration Not Enforced (Secondary)<a class="headerlink" href="#33-timeout-configuration-not-enforced-secondary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: CronJob timeout settings not enforced despite proper configuration</li>
<li><strong>Settings Applied</strong>:</li>
<li><code>activeDeadlineSeconds: 300</code> (5-minute job timeout)</li>
<li><code>ttlSecondsAfterFinished: 120</code> (2-minute cleanup after completion)</li>
<li><code>startingDeadlineSeconds: 60</code> (1-minute grace for job creation)</li>
<li><strong>Observed Behavior</strong>:</li>
<li>Jobs remained Running for 85+ minutes despite 5-minute timeout</li>
<li>Completed jobs not auto-deleted despite 2-minute TTL</li>
<li>No timeout enforcement events generated</li>
<li><strong>Root Cause</strong>: Job controller corruption prevented processing of any job lifecycle events</li>
<li><strong>Impact</strong>: Self-healing timeline predictions completely invalid (expected 6-12 hours, actual: infinite)</li>
</ul>
<h4 id="34-argocd-auto-sync-failure-secondary">3.4 ArgoCD Auto-Sync Failure (Secondary)<a class="headerlink" href="#34-argocd-auto-sync-failure-secondary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: ArgoCD failed to auto-recreate deleted cronjob despite automated sync enabled</li>
<li><strong>Symptoms</strong>:</li>
<li>Application showed OutOfSync status but no sync operation triggered</li>
<li>Manual sync attempts via kubectl patch failed</li>
<li>Hard refresh annotation didn't trigger recreation</li>
<li><strong>Impact</strong>: Required manual cronjob creation, delaying recovery</li>
<li><strong>Why it happened</strong>: ArgoCD controller may have been affected by broader job controller corruption</li>
<li><strong>Workaround</strong>: Manually created cronjob from template, ArgoCD eventually adopted it</li>
</ul>
<h3 id="phase-4-argocd-application-recovery-2026-01-08">Phase 4: ArgoCD Application Recovery (2026-01-08)<a class="headerlink" href="#phase-4-argocd-application-recovery-2026-01-08" title="Permanent link">&para;</a></h3>
<h4 id="41-github-actions-runner-controller-finalizer-issues-primary">4.1 GitHub Actions Runner Controller Finalizer Issues (Primary)<a class="headerlink" href="#41-github-actions-runner-controller-finalizer-issues-primary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: Child application <code>gharc-runners-pgmac-user-self-hosted</code> stuck with resources "Pending deletion"</li>
<li><strong>Affected resources</strong>: 4 resources with blocking finalizers</li>
<li>AutoscalingRunnerSet: <code>pgmac-slack-scores</code></li>
<li>ServiceAccount: <code>pgmac-slack-scores-gha-rs-no-permission</code></li>
<li>Role: <code>pgmac-slack-scores-gha-rs-manager</code></li>
<li>RoleBinding: <code>pgmac-slack-scores-gha-rs-manager</code></li>
<li><strong>Symptoms</strong>:</li>
<li>Parent application <code>ci-tools</code> stuck OutOfSync + Progressing</li>
<li>Resources marked for deletion but finalizers preventing cleanup</li>
<li>ArgoCD unable to sync parent application due to child application state</li>
<li><strong>Impact</strong>: Blocked CI/CD application sync, prevented runner controller updates</li>
<li><strong>Why it happened</strong>:</li>
<li><strong>Connection to Phase 3</strong>: Job controller corruption prevented cleanup job from removing finalizers</li>
<li>Resources created by parent application but child application deletion failed</li>
<li>Finalizers intended to ensure graceful resource cleanup but became stuck</li>
<li><strong>Resolution</strong>: Removed finalizers manually using <code>kubectl patch --type json -p='[{"op": "remove", "path": "/metadata/finalizers"}]'</code></li>
</ul>
<h4 id="42-github-actions-runner-controller-state-drift-secondary">4.2 GitHub Actions Runner Controller State Drift (Secondary)<a class="headerlink" href="#42-github-actions-runner-controller-state-drift-secondary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: Old listener resources with hash <code>754b578d</code> remained after controller update</li>
<li><strong>Affected resources</strong>:</li>
<li>ServiceAccount with old hash</li>
<li>Role with old hash</li>
<li>RoleBinding with old hash</li>
<li><strong>Symptoms</strong>:</li>
<li>Application <code>gharc-runners-pgmac-net-self-hosted</code> showing OutOfSync + Healthy</li>
<li>New resources created with different hash but old resources not deleted</li>
<li>6 runner pods stuck Pending for 44+ minutes (residual from Phase 3)</li>
<li><strong>Impact</strong>: Resource accumulation, pod scheduling failures</li>
<li><strong>Why it happened</strong>:</li>
<li><strong>Connection to Phase 3</strong>: Pods created during job controller corruption remained stuck</li>
<li>ArgoCD <code>ignoreDifferences</code> configuration for AutoscalingListener, Role, RoleBinding masked drift</li>
<li>Controller update didn't clean up old resources automatically</li>
<li><strong>Resolution</strong>: Manually deleted old listener resources, force-deleted 6 stuck runner pods</li>
<li><strong>Acceptable state</strong>: OutOfSync + Healthy is expected due to ignoreDifferences configuration</li>
</ul>
<h4 id="43-linkace-helm-chart-configuration-drift-primary">4.3 LinkAce Helm Chart Configuration Drift (Primary)<a class="headerlink" href="#43-linkace-helm-chart-configuration-drift-primary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: LinkAce cronjob showing OutOfSync despite application Healthy, ArgoCD attempted 23 auto-heal operations</li>
<li><strong>Unsupported configuration</strong>: LinkAce Helm chart doesn't support these cronjob fields:</li>
<li><code>backoffLimit: 0</code></li>
<li><code>resources</code> block (limits/requests)</li>
<li><strong>Symptoms</strong>:</li>
<li>ArgoCD continuously detecting drift between desired and actual state</li>
<li>Auto-heal operations failing to converge</li>
<li>Application Healthy but OutOfSync persisting</li>
<li><strong>Impact</strong>: ArgoCD resource consumption, false positive OutOfSync alerts</li>
<li><strong>Why it happened</strong>:</li>
<li>Upstream Helm chart doesn't expose all CronJob configuration options</li>
<li>ArgoCD manifest specified fields not supported by chart templates</li>
<li>GitOps configuration drift from Helm chart capabilities</li>
<li><strong>Resolution</strong>: Removed unsupported fields from ArgoCD manifest, kept critical timeout settings</li>
<li><strong>Acceptable state</strong>: Minor field drift from Helm chart is expected and acceptable</li>
</ul>
<h4 id="44-home-assistant-application-self-healing-none">4.4 Home Assistant Application Self-Healing (None)<a class="headerlink" href="#44-home-assistant-application-self-healing-none" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: Application showed Synced + Progressing initially</li>
<li><strong>Root cause</strong>: StatefulSet rollout in progress during investigation</li>
<li><strong>Resolution</strong>: Self-resolved as rollout completed</li>
<li><strong>Impact</strong>: None, normal operational state</li>
</ul>
<h3 id="phase-5-k8s01-container-runtime-corruption-recurrence-2026-01-09">Phase 5: k8s01 Container Runtime Corruption Recurrence (2026-01-09)<a class="headerlink" href="#phase-5-k8s01-container-runtime-corruption-recurrence-2026-01-09" title="Permanent link">&para;</a></h3>
<h4 id="51-persistent-container-runtime-corruption-on-k8s01-critical">5.1 Persistent Container Runtime Corruption on k8s01 (Critical)<a class="headerlink" href="#51-persistent-container-runtime-corruption-on-k8s01-critical" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: k8s01 node unable to start new containers 48+ hours after Phase 3 nuclear option</li>
<li><strong>Symptoms</strong>:</li>
<li>4 runner pods stuck in Pending state for 12+ hours</li>
<li>All 4 pods assigned to k8s01 node</li>
<li>Pods showing PodScheduled=True but no container initialization</li>
<li>No events generated (silent failure pattern identical to Phase 2)</li>
<li>Node reporting Ready status despite being unable to start containers</li>
<li>6 additional EphemeralRunner resources with no corresponding pods</li>
<li><strong>Impact</strong>: Complete inability to start new workloads on k8s01, affecting GitHub Actions runners</li>
<li><strong>Why it happened</strong>:</li>
<li><strong>Connection to Phase 1</strong>: Container runtime corruption originated from Phase 1 disk pressure (97% usage) and kubelet crashes</li>
<li><strong>Connection to Phase 2</strong>: k8s01 required microk8s restart during Phase 2 (Resolution 2.1) for same symptoms</li>
<li><strong>Connection to Phase 3</strong>: Phase 3 nuclear option (cluster-wide restart) only cleared dqlite/controller state, not node-local container runtime corruption</li>
<li>Container runtime (containerd) state diverged from kubelet state</li>
<li>Corruption persisted across cluster restarts because it was node-local, not cluster-global</li>
<li>12+ hour delay in detection suggests corruption was dormant until new workloads attempted to schedule</li>
<li><strong>Resolution</strong>: Full microk8s restart on k8s01 (<code>microk8s stop &amp;&amp; microk8s start</code>)</li>
<li><strong>Key finding</strong>: Nuclear option (cluster restart) insufficient to clear node-local container runtime issues</li>
</ul>
<h4 id="52-detection-gap-for-node-local-failures-secondary">5.2 Detection Gap for Node-Local Failures (Secondary)<a class="headerlink" href="#52-detection-gap-for-node-local-failures-secondary" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Issue</strong>: 12+ hour delay between pod creation and detection of container startup failure</li>
<li><strong>Why it happened</strong>:</li>
<li>No monitoring for silent pod failures (PodScheduled=True but no container creation)</li>
<li>No alerts for pods stuck in Pending with node assignment</li>
<li>No synthetic pod startup tests on individual nodes</li>
<li>Kubernetes node status (Ready) doesn't reflect container runtime health</li>
<li><strong>Impact</strong>: Extended downtime for affected workloads without visibility</li>
<li><strong>Resolution</strong>: Manual investigation prompted by user observation</li>
</ul>
<hr />
<h2 id="impact">Impact<a class="headerlink" href="#impact" title="Permanent link">&para;</a></h2>
<h3 id="services-affected">Services Affected<a class="headerlink" href="#services-affected" title="Permanent link">&para;</a></h3>
<p><strong>Phase 1:</strong></p>
<ul>
<li><strong>Entire cluster</strong>: Widespread pod scheduling and lifecycle failures</li>
<li><strong>GitHub Actions</strong>: Self-hosted runners completely non-functional (571 pods stuck)</li>
<li><strong>OpenEBS storage</strong>: 22 replica pods unavailable, volume operations degraded</li>
<li><strong>All services</strong>: Intermittent availability as pods failed to start/stop properly</li>
</ul>
<p><strong>Phase 2:</strong></p>
<ul>
<li><strong>Sonarr</strong>: Unavailable via https://sonarr.int.pgmac.net/ (504 errors)</li>
<li><strong>Radarr</strong>: Pod crashing, completely unavailable</li>
<li><strong>Overseerr</strong>: Intermittent 504 timeout errors</li>
</ul>
<p><strong>Phase 3:</strong></p>
<ul>
<li><strong>LinkAce cronjob</strong>: Complete failure to execute scheduled tasks (every minute)</li>
<li><strong>All Kubernetes Jobs</strong>: Job controller corruption affected cluster-wide job creation</li>
<li><strong>LinkAce scheduled tasks</strong>: Backup creation, link validation, database cleanup not executing</li>
<li><strong>ArgoCD</strong>: Auto-sync mechanism failed for deleted resources</li>
</ul>
<p><strong>Phase 4:</strong></p>
<ul>
<li><strong>CI/CD Applications</strong>: ci-tools stuck OutOfSync + Progressing, preventing runner controller updates</li>
<li><strong>GitHub Actions Runners</strong>: 6 runner pods stuck Pending for 44+ minutes (residual from Phase 3)</li>
<li><strong>ArgoCD GitOps</strong>: Multiple applications showing false OutOfSync status consuming resources</li>
<li><strong>LinkAce Application</strong>: 23 failed auto-heal attempts creating noise in ArgoCD</li>
</ul>
<p><strong>Phase 5:</strong></p>
<ul>
<li><strong>GitHub Actions Runners</strong>: 4 runner pods stuck Pending for 12+ hours on k8s01 (pgmac-slack-scores runners unable to spawn)</li>
<li><strong>k8s01 Node</strong>: Complete inability to start new containers despite Ready status</li>
<li><strong>EphemeralRunner Resources</strong>: 6 additional runners with no corresponding pods (silent failure)</li>
</ul>
<h3 id="duration">Duration<a class="headerlink" href="#duration" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Total incident duration</strong>: ~8 hours (09:00 - 17:00 AEST 2026-01-06) + 16.5 hours (Phase 3, 2026-01-08) + 0.75 hours (Phase 4, 2026-01-08) + 12+ hours (Phase 5, 2026-01-09)</li>
<li><strong>Phase 1 critical period</strong>: ~3.5 hours (09:00 - 12:30 2026-01-06)</li>
<li><strong>Phase 2 service outages</strong>: ~3 hours (12:30 - 15:35 2026-01-06)</li>
<li><strong>Phase 3 cronjob failure</strong>: ~16.5 hours (02:00 - 18:25 2026-01-08)</li>
<li><strong>Phase 4 ArgoCD recovery</strong>: ~45 minutes (19:00 - 19:45 2026-01-08)</li>
<li><strong>Phase 5 container runtime corruption</strong>: ~5 minutes active recovery (09:50 - 09:55 2026-01-09), but 12+ hours of silent failure</li>
<li><strong>Sonarr downtime</strong>: ~1.5 hours</li>
<li><strong>Radarr downtime</strong>: ~2.5 hours</li>
<li><strong>Overseerr impact</strong>: Intermittent throughout Phase 2</li>
<li><strong>GitHub Actions runners</strong>: ~8 hours (full Phase 1-2 duration) + 45 minutes (Phase 4) + 12+ hours (Phase 5)</li>
<li><strong>LinkAce scheduled tasks</strong>: ~16.5 hours (complete failure to execute)</li>
<li><strong>CI/CD deployments</strong>: Blocked during Phase 4 investigation (~45 minutes) + 12+ hours (Phase 5 k8s01 node failure)</li>
</ul>
<h3 id="scope">Scope<a class="headerlink" href="#scope" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Infrastructure</strong>: All 3 nodes compromised at various points</li>
<li><strong>User-facing</strong>: All web access to media management services</li>
<li><strong>Internal</strong>: Home Assistant integrations unable to query service APIs</li>
<li><strong>CI/CD</strong>: GitHub Actions self-hosted runners completely unavailable</li>
<li><strong>Monitoring</strong>: Nagios health checks failing across multiple services</li>
<li><strong>Storage</strong>: OpenEBS volume operations degraded during Phase 1</li>
</ul>
<hr />
<h2 id="resolution-steps-taken">Resolution Steps Taken<a class="headerlink" href="#resolution-steps-taken" title="Permanent link">&para;</a></h2>
<h3 id="phase-1-node-and-kubelet-recovery">Phase 1: Node and Kubelet Recovery<a class="headerlink" href="#phase-1-node-and-kubelet-recovery" title="Permanent link">&para;</a></h3>
<h4 id="1-k8s02-kubelet-restart">1. k8s02 Kubelet Restart<a class="headerlink" href="#1-k8s02-kubelet-restart" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="c1"># On k8s02 node</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>snap.microk8s.daemon-kubelite
</code></pre></div>
<h4 id="2-k8s01-disk-cleanup-and-stabilization">2. k8s01 Disk Cleanup and Stabilization<a class="headerlink" href="#2-k8s01-disk-cleanup-and-stabilization" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="c1"># On k8s01 node</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="c1"># Removed unused container images</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>microk8s<span class="w"> </span>ctr<span class="w"> </span>images<span class="w"> </span>rm<span class="w"> </span>&lt;image-id&gt;...
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="c1"># Cleaned container logs (methods vary)</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span class="c1"># Removed old/stopped containers</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="c1"># Result: 97%  87% disk usage</span>
</code></pre></div>
<h4 id="3-k8s03-disk-cleanup">3. k8s03 Disk Cleanup<a class="headerlink" href="#3-k8s03-disk-cleanup" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="c1"># On k8s03 node</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="c1"># Similar cleanup process</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="c1"># Result: 100%  81% disk usage</span>
</code></pre></div>
<h4 id="4-runner-controller-cleanup">4. Runner Controller Cleanup<a class="headerlink" href="#4-runner-controller-cleanup" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="c1"># Deleted orphaned controller resources</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>kubectl<span class="w"> </span>delete<span class="w"> </span>runnerdeployment<span class="w"> </span>pgmac.pgmac-runnerdeploy<span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span>--context<span class="w"> </span>pvek8s
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>kubectl<span class="w"> </span>delete<span class="w"> </span>horizontalrunnerautoscaler<span class="w"> </span>pgmac-pgmac-runnerdeploy-autoscaler<span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span>--context<span class="w"> </span>pvek8s
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="c1"># Force-deleted 546+ orphaned runner pods in batches</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a><span class="c1"># Pending pods (299)</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--no-headers<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="w">  </span>grep<span class="w"> </span><span class="s2">&quot;pgmac.pgmac-runnerdeploy&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">&quot;Pending&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a><span class="w">  </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>-I<span class="w"> </span><span class="o">{}</span><span class="w"> </span>kubectl<span class="w"> </span>delete<span class="w"> </span>pod<span class="w"> </span><span class="o">{}</span><span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a><span class="w">  </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--force<span class="w"> </span>--grace-period<span class="o">=</span><span class="m">0</span><span class="w"> </span>--wait<span class="o">=</span><span class="nb">false</span>
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a><span class="c1"># ContainerStatusUnknown (110)</span>
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--no-headers<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a><span class="w">  </span>grep<span class="w"> </span><span class="s2">&quot;pgmac.pgmac-runnerdeploy&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">&quot;ContainerStatusUnknown&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a><span class="w">  </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>-I<span class="w"> </span><span class="o">{}</span><span class="w"> </span>kubectl<span class="w"> </span>delete<span class="w"> </span>pod<span class="w"> </span><span class="o">{}</span><span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a><span class="w">  </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--force<span class="w"> </span>--grace-period<span class="o">=</span><span class="m">0</span><span class="w"> </span>--wait<span class="o">=</span><span class="nb">false</span>
<a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a>
<a id="__codelineno-3-18" name="__codelineno-3-18" href="#__codelineno-3-18"></a><span class="c1"># StartError/RunContainerError (58)</span>
<a id="__codelineno-3-19" name="__codelineno-3-19" href="#__codelineno-3-19"></a>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--no-headers<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-20" name="__codelineno-3-20" href="#__codelineno-3-20"></a><span class="w">  </span>grep<span class="w"> </span><span class="s2">&quot;pgmac.pgmac-runnerdeploy&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-E<span class="w"> </span><span class="s2">&quot;StartError|RunContainerError|Error&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-21" name="__codelineno-3-21" href="#__codelineno-3-21"></a><span class="w">  </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>-I<span class="w"> </span><span class="o">{}</span><span class="w"> </span>kubectl<span class="w"> </span>delete<span class="w"> </span>pod<span class="w"> </span><span class="o">{}</span><span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-22" name="__codelineno-3-22" href="#__codelineno-3-22"></a><span class="w">  </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--force<span class="w"> </span>--grace-period<span class="o">=</span><span class="m">0</span><span class="w"> </span>--wait<span class="o">=</span><span class="nb">false</span>
<a id="__codelineno-3-23" name="__codelineno-3-23" href="#__codelineno-3-23"></a>
<a id="__codelineno-3-24" name="__codelineno-3-24" href="#__codelineno-3-24"></a><span class="c1"># Completed (79)</span>
<a id="__codelineno-3-25" name="__codelineno-3-25" href="#__codelineno-3-25"></a>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--no-headers<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-26" name="__codelineno-3-26" href="#__codelineno-3-26"></a><span class="w">  </span>grep<span class="w"> </span><span class="s2">&quot;pgmac.pgmac-runnerdeploy&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span><span class="s2">&quot;Completed&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-27" name="__codelineno-3-27" href="#__codelineno-3-27"></a><span class="w">  </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>-I<span class="w"> </span><span class="o">{}</span><span class="w"> </span>kubectl<span class="w"> </span>delete<span class="w"> </span>pod<span class="w"> </span><span class="o">{}</span><span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-28" name="__codelineno-3-28" href="#__codelineno-3-28"></a><span class="w">  </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--force<span class="w"> </span>--grace-period<span class="o">=</span><span class="m">0</span><span class="w"> </span>--wait<span class="o">=</span><span class="nb">false</span>
<a id="__codelineno-3-29" name="__codelineno-3-29" href="#__codelineno-3-29"></a>
<a id="__codelineno-3-30" name="__codelineno-3-30" href="#__codelineno-3-30"></a><span class="c1"># Final cleanup batch (247)</span>
<a id="__codelineno-3-31" name="__codelineno-3-31" href="#__codelineno-3-31"></a>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--no-headers<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-32" name="__codelineno-3-32" href="#__codelineno-3-32"></a><span class="w">  </span>grep<span class="w"> </span><span class="s2">&quot;pgmac.pgmac-runnerdeploy&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-v<span class="w"> </span><span class="s2">&quot;Running&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-v<span class="w"> </span><span class="s2">&quot;Terminating&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-33" name="__codelineno-3-33" href="#__codelineno-3-33"></a><span class="w">  </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>-I<span class="w"> </span><span class="o">{}</span><span class="w"> </span>kubectl<span class="w"> </span>delete<span class="w"> </span>pod<span class="w"> </span><span class="o">{}</span><span class="w"> </span>-n<span class="w"> </span>ci<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-3-34" name="__codelineno-3-34" href="#__codelineno-3-34"></a><span class="w">  </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--force<span class="w"> </span>--grace-period<span class="o">=</span><span class="m">0</span><span class="w"> </span>--wait<span class="o">=</span><span class="nb">false</span>
</code></pre></div>
<h4 id="5-openebs-replica-pod-cleanup">5. OpenEBS Replica Pod Cleanup<a class="headerlink" href="#5-openebs-replica-pod-cleanup" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="c1"># Force-deleted 22 stuck Terminating replica pods</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>kubectl<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>openebs<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>Terminating<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="w">  </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>-I<span class="w"> </span><span class="o">{}</span><span class="w"> </span>kubectl<span class="w"> </span>delete<span class="w"> </span>pod<span class="w"> </span><span class="o">{}</span><span class="w"> </span>-n<span class="w"> </span>openebs<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="w">  </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>--force<span class="w"> </span>--grace-period<span class="o">=</span><span class="m">0</span>
</code></pre></div>
<h3 id="phase-2-storage-and-ingress-recovery">Phase 2: Storage and Ingress Recovery<a class="headerlink" href="#phase-2-storage-and-ingress-recovery" title="Permanent link">&para;</a></h3>
<h4 id="6-k8s01-full-restart-residual-issues">6. k8s01 Full Restart (Residual Issues)<a class="headerlink" href="#6-k8s01-full-restart-residual-issues" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="c1"># On k8s01 node</span>
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>microk8s<span class="w"> </span>stop<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>microk8s<span class="w"> </span>start
</code></pre></div>
<h4 id="7-jiva-snapshot-cleanup">7. Jiva Snapshot Cleanup<a class="headerlink" href="#7-jiva-snapshot-cleanup" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="c1"># Triggered manual cleanup job</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>create<span class="w"> </span>job<span class="w"> </span>-n<span class="w"> </span>openebs<span class="w"> </span>jiva-snapshot-cleanup-manual<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="w">  </span>--from<span class="o">=</span>cronjob/jiva-snapshot-cleanup
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a><span class="c1"># Job processed all Jiva volumes sequentially</span>
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a><span class="c1"># - Rolling restart of 3 replicas per volume</span>
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a><span class="c1"># - 30-second stabilization period between replicas</span>
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a><span class="c1"># - Total runtime: ~60 minutes for all volumes</span>
</code></pre></div>
<h4 id="8-ingress-controller-refresh">8. Ingress Controller Refresh<a class="headerlink" href="#8-ingress-controller-refresh" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="c1"># Restarted all ingress controllers to clear endpoint cache</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>delete<span class="w"> </span>pod<span class="w"> </span>-n<span class="w"> </span>ingress<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a><span class="w">  </span>nginx-ingress-microk8s-controller-2chvz<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a><span class="w">  </span>nginx-ingress-microk8s-controller-k56gn<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a><span class="w">  </span>nginx-ingress-microk8s-controller-t56r5
</code></pre></div>
<h4 id="9-radarr-volume-emergency-cleanup">9. Radarr Volume Emergency Cleanup<a class="headerlink" href="#9-radarr-volume-emergency-cleanup" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="c1"># Freed space by removing old backups</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-n<span class="w"> </span>media<span class="w"> </span>radarr-&lt;pod&gt;<span class="w"> </span>--<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="w">  </span>rm<span class="w"> </span>-rf<span class="w"> </span>/config/Backups/*
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a><span class="c1"># Result: 100%  95% usage, sufficient for startup</span>
</code></pre></div>
<h4 id="10-k8s03-full-restart-replica-pod-issues">10. k8s03 Full Restart (Replica Pod Issues)<a class="headerlink" href="#10-k8s03-full-restart-replica-pod-issues" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="c1"># On k8s03 node</span>
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>microk8s<span class="w"> </span>stop<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>microk8s<span class="w"> </span>start
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a><span class="c1"># All Pending replica pods recreated successfully after restart</span>
</code></pre></div>
<h3 id="phase-3-job-controller-and-database-recovery-nuclear-option">Phase 3: Job Controller and Database Recovery (Nuclear Option)<a class="headerlink" href="#phase-3-job-controller-and-database-recovery-nuclear-option" title="Permanent link">&para;</a></h3>
<h4 id="11-cluster-wide-restart-with-database-backup">11. Cluster-Wide Restart with Database Backup<a class="headerlink" href="#11-cluster-wide-restart-with-database-backup" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="c1"># Stop MicroK8s on all nodes (prevent database writes during backup)</span>
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>ssh<span class="w"> </span>k8s01<span class="w"> </span><span class="s2">&quot;sudo snap stop microk8s&quot;</span>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>ssh<span class="w"> </span>k8s02<span class="w"> </span><span class="s2">&quot;sudo snap stop microk8s&quot;</span>
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>ssh<span class="w"> </span>k8s03<span class="w"> </span><span class="s2">&quot;sudo snap stop microk8s&quot;</span>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a><span class="c1"># Wait for clean shutdown</span>
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>sleep<span class="w"> </span><span class="m">30</span>
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a><span class="c1"># Backup dqlite database (on k8s01 primary node)</span>
<a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a>ssh<span class="w"> </span>k8s01<span class="w"> </span><span class="s2">&quot;sudo mkdir -p /var/snap/microk8s/common/backup &amp;&amp; \</span>
<a id="__codelineno-10-11" name="__codelineno-10-11" href="#__codelineno-10-11"></a><span class="s2">  sudo cp -r /var/snap/microk8s/current/var/kubernetes/backend \</span>
<a id="__codelineno-10-12" name="__codelineno-10-12" href="#__codelineno-10-12"></a><span class="s2">  /var/snap/microk8s/common/backup/etcd-backup-20260108-201540&quot;</span>
<a id="__codelineno-10-13" name="__codelineno-10-13" href="#__codelineno-10-13"></a>
<a id="__codelineno-10-14" name="__codelineno-10-14" href="#__codelineno-10-14"></a><span class="c1"># Start MicroK8s on all nodes</span>
<a id="__codelineno-10-15" name="__codelineno-10-15" href="#__codelineno-10-15"></a>ssh<span class="w"> </span>k8s01<span class="w"> </span><span class="s2">&quot;sudo snap start microk8s&quot;</span>
<a id="__codelineno-10-16" name="__codelineno-10-16" href="#__codelineno-10-16"></a>ssh<span class="w"> </span>k8s02<span class="w"> </span><span class="s2">&quot;sudo snap start microk8s&quot;</span>
<a id="__codelineno-10-17" name="__codelineno-10-17" href="#__codelineno-10-17"></a>ssh<span class="w"> </span>k8s03<span class="w"> </span><span class="s2">&quot;sudo snap start microk8s&quot;</span>
<a id="__codelineno-10-18" name="__codelineno-10-18" href="#__codelineno-10-18"></a>
<a id="__codelineno-10-19" name="__codelineno-10-19" href="#__codelineno-10-19"></a><span class="c1"># Wait for cluster to be ready</span>
<a id="__codelineno-10-20" name="__codelineno-10-20" href="#__codelineno-10-20"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span><span class="nb">wait</span><span class="w"> </span>--for<span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Ready<span class="w"> </span>nodes<span class="w"> </span>--all<span class="w"> </span>--timeout<span class="o">=</span>300s
<a id="__codelineno-10-21" name="__codelineno-10-21" href="#__codelineno-10-21"></a>
<a id="__codelineno-10-22" name="__codelineno-10-22" href="#__codelineno-10-22"></a><span class="c1"># Verify cluster health</span>
<a id="__codelineno-10-23" name="__codelineno-10-23" href="#__codelineno-10-23"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>nodes
<a id="__codelineno-10-24" name="__codelineno-10-24" href="#__codelineno-10-24"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>componentstatuses
</code></pre></div>
<h4 id="12-clean-job-and-cronjob-state">12. Clean Job and CronJob State<a class="headerlink" href="#12-clean-job-and-cronjob-state" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="c1"># Delete all stuck jobs (85+ jobs accumulated)</span>
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>delete<span class="w"> </span><span class="nb">jobs</span><span class="w"> </span>-n<span class="w"> </span>media<span class="w"> </span>-l<span class="w"> </span>app.kubernetes.io/instance<span class="o">=</span>linkace<span class="w"> </span>--all
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a><span class="c1"># Delete cronjob to get fresh state</span>
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>delete<span class="w"> </span>cronjob<span class="w"> </span>linkace-cronjob<span class="w"> </span>-n<span class="w"> </span>media
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a>
<a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a><span class="c1"># Trigger ArgoCD sync to recreate with fresh state</span>
<a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>patch<span class="w"> </span>application<span class="w"> </span>linkace<span class="w"> </span>-n<span class="w"> </span>argocd<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a><span class="w">  </span>--type<span class="w"> </span>merge<span class="w"> </span>-p<span class="w"> </span><span class="s1">&#39;{&quot;operation&quot;:{&quot;sync&quot;:{&quot;revision&quot;:&quot;HEAD&quot;}}}&#39;</span>
</code></pre></div>
<h4 id="13-update-argocd-manifest-with-timeout-configuration">13. Update ArgoCD Manifest with Timeout Configuration<a class="headerlink" href="#13-update-argocd-manifest-with-timeout-configuration" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="c1"># In pgk8s/pgmac.net/media/templates/linkace.yaml (lines 101-114)</span>
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="nt">cronjob</span><span class="p">:</span>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a><span class="w">  </span><span class="nt">startingDeadlineSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60</span><span class="w"> </span><span class="c1"># Grace period for job creation</span>
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a><span class="w">  </span><span class="nt">activeDeadlineSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">300</span><span class="w"> </span><span class="c1"># 5-minute job timeout</span>
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a><span class="w">  </span><span class="nt">ttlSecondsAfterFinished</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">120</span><span class="w"> </span><span class="c1"># 2-minute cleanup after completion</span>
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a><span class="w">  </span><span class="nt">successfulJobsHistoryLimit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a><span class="w">  </span><span class="nt">failedJobsHistoryLimit</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a><span class="w">  </span><span class="nt">resources</span><span class="p">:</span>
<a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a><span class="w">    </span><span class="nt">limits</span><span class="p">:</span>
<a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a><span class="w">      </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">512Mi</span>
<a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a><span class="w">      </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">500m</span>
<a id="__codelineno-12-12" name="__codelineno-12-12" href="#__codelineno-12-12"></a><span class="w">    </span><span class="nt">requests</span><span class="p">:</span>
<a id="__codelineno-12-13" name="__codelineno-12-13" href="#__codelineno-12-13"></a><span class="w">      </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256Mi</span>
<a id="__codelineno-12-14" name="__codelineno-12-14" href="#__codelineno-12-14"></a><span class="w">      </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100m</span>
</code></pre></div>
<h4 id="14-verification">14. Verification<a class="headerlink" href="#14-verification" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="c1"># Verify cronjob recreated with correct configuration</span>
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>cronjob<span class="w"> </span>linkace-cronjob<span class="w"> </span>-n<span class="w"> </span>media<span class="w"> </span>-o<span class="w"> </span>yaml
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a><span class="c1"># Wait for next minute and verify job creation</span>
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>watch<span class="w"> </span>-n<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="s1">&#39;kubectl --context pvek8s get jobs -n media -l app.kubernetes.io/instance=linkace&#39;</span>
<a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a>
<a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a><span class="c1"># Verify job completes successfully</span>
<a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span><span class="nb">wait</span><span class="w"> </span>--for<span class="o">=</span><span class="nv">condition</span><span class="o">=</span><span class="nb">complete</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-13-9" name="__codelineno-13-9" href="#__codelineno-13-9"></a><span class="w">  </span>job/linkace-cronjob-&lt;generated&gt;<span class="w"> </span>-n<span class="w"> </span>media<span class="w"> </span>--timeout<span class="o">=</span>120s
<a id="__codelineno-13-10" name="__codelineno-13-10" href="#__codelineno-13-10"></a>
<a id="__codelineno-13-11" name="__codelineno-13-11" href="#__codelineno-13-11"></a><span class="c1"># Verify TTL cleanup working (jobs deleted after 2 minutes)</span>
<a id="__codelineno-13-12" name="__codelineno-13-12" href="#__codelineno-13-12"></a><span class="c1"># Monitor job count - should stabilize at 1 successful + max 2 failed</span>
<a id="__codelineno-13-13" name="__codelineno-13-13" href="#__codelineno-13-13"></a>watch<span class="w"> </span>-n<span class="w"> </span><span class="m">30</span><span class="w"> </span><span class="s1">&#39;kubectl --context pvek8s get jobs -n media -l app.kubernetes.io/instance=linkace&#39;</span>
<a id="__codelineno-13-14" name="__codelineno-13-14" href="#__codelineno-13-14"></a>
<a id="__codelineno-13-15" name="__codelineno-13-15" href="#__codelineno-13-15"></a><span class="c1"># Check job execution time (should be ~7 seconds)</span>
<a id="__codelineno-13-16" name="__codelineno-13-16" href="#__codelineno-13-16"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>job<span class="w"> </span>linkace-cronjob-&lt;latest&gt;<span class="w"> </span>-n<span class="w"> </span>media<span class="w"> </span>-o<span class="w"> </span>yaml<span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<a id="__codelineno-13-17" name="__codelineno-13-17" href="#__codelineno-13-17"></a><span class="w">  </span>grep<span class="w"> </span>-A<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="s2">&quot;startTime\|completionTime&quot;</span>
<a id="__codelineno-13-18" name="__codelineno-13-18" href="#__codelineno-13-18"></a>
<a id="__codelineno-13-19" name="__codelineno-13-19" href="#__codelineno-13-19"></a><span class="c1"># Verify no orphaned pods</span>
<a id="__codelineno-13-20" name="__codelineno-13-20" href="#__codelineno-13-20"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>media<span class="w"> </span>-l<span class="w"> </span>job-name
</code></pre></div>
<h3 id="phase-4-argocd-application-and-finalizer-recovery">Phase 4: ArgoCD Application and Finalizer Recovery<a class="headerlink" href="#phase-4-argocd-application-and-finalizer-recovery" title="Permanent link">&para;</a></h3>
<h4 id="15-remove-finalizers-from-stuck-resources">15. Remove Finalizers from Stuck Resources<a class="headerlink" href="#15-remove-finalizers-from-stuck-resources" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="c1"># Remove finalizer from AutoscalingRunnerSet</span>
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>patch<span class="w"> </span>autoscalingrunnerset<span class="w"> </span>pgmac-slack-scores<span class="w"> </span>-n<span class="w"> </span>arc-runners<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="w">  </span>--type<span class="w"> </span>json<span class="w"> </span>-p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/metadata/finalizers&quot;}]&#39;</span>
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a>
<a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="c1"># Remove finalizer from ServiceAccount</span>
<a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>patch<span class="w"> </span>serviceaccount<span class="w"> </span>pgmac-slack-scores-gha-rs-no-permission<span class="w"> </span>-n<span class="w"> </span>arc-runners<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a><span class="w">  </span>--type<span class="w"> </span>json<span class="w"> </span>-p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/metadata/finalizers&quot;}]&#39;</span>
<a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a>
<a id="__codelineno-14-9" name="__codelineno-14-9" href="#__codelineno-14-9"></a><span class="c1"># Remove finalizer from Role</span>
<a id="__codelineno-14-10" name="__codelineno-14-10" href="#__codelineno-14-10"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>patch<span class="w"> </span>role<span class="w"> </span>pgmac-slack-scores-gha-rs-manager<span class="w"> </span>-n<span class="w"> </span>arc-runners<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-14-11" name="__codelineno-14-11" href="#__codelineno-14-11"></a><span class="w">  </span>--type<span class="w"> </span>json<span class="w"> </span>-p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/metadata/finalizers&quot;}]&#39;</span>
<a id="__codelineno-14-12" name="__codelineno-14-12" href="#__codelineno-14-12"></a>
<a id="__codelineno-14-13" name="__codelineno-14-13" href="#__codelineno-14-13"></a><span class="c1"># Remove finalizer from RoleBinding</span>
<a id="__codelineno-14-14" name="__codelineno-14-14" href="#__codelineno-14-14"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>patch<span class="w"> </span>rolebinding<span class="w"> </span>pgmac-slack-scores-gha-rs-manager<span class="w"> </span>-n<span class="w"> </span>arc-runners<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-14-15" name="__codelineno-14-15" href="#__codelineno-14-15"></a><span class="w">  </span>--type<span class="w"> </span>json<span class="w"> </span>-p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/metadata/finalizers&quot;}]&#39;</span>
<a id="__codelineno-14-16" name="__codelineno-14-16" href="#__codelineno-14-16"></a>
<a id="__codelineno-14-17" name="__codelineno-14-17" href="#__codelineno-14-17"></a><span class="c1"># Trigger ArgoCD sync for parent application</span>
<a id="__codelineno-14-18" name="__codelineno-14-18" href="#__codelineno-14-18"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>patch<span class="w"> </span>application<span class="w"> </span>ci-tools<span class="w"> </span>-n<span class="w"> </span>argocd<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-14-19" name="__codelineno-14-19" href="#__codelineno-14-19"></a><span class="w">  </span>--type<span class="w"> </span>merge<span class="w"> </span>-p<span class="w"> </span><span class="s1">&#39;{&quot;operation&quot;:{&quot;sync&quot;:{&quot;revision&quot;:&quot;HEAD&quot;}}}&#39;</span>
</code></pre></div>
<h4 id="16-clean-up-old-runner-controller-resources">16. Clean Up Old Runner Controller Resources<a class="headerlink" href="#16-clean-up-old-runner-controller-resources" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="c1"># Delete old listener resources with stale hash</span>
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>delete<span class="w"> </span>serviceaccount<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="w">  </span>pgmac-renovatebot-gha-rs-listener-754b578d<span class="w"> </span>-n<span class="w"> </span>arc-runners
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>delete<span class="w"> </span>role<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a><span class="w">  </span>pgmac-renovatebot-gha-rs-listener-754b578d<span class="w"> </span>-n<span class="w"> </span>arc-runners
<a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a>
<a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>delete<span class="w"> </span>rolebinding<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a><span class="w">  </span>pgmac-renovatebot-gha-rs-listener-754b578d<span class="w"> </span>-n<span class="w"> </span>arc-runners
<a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a>
<a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a><span class="c1"># Force-delete stuck runner pods (residual from Phase 3)</span>
<a id="__codelineno-15-12" name="__codelineno-15-12" href="#__codelineno-15-12"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>delete<span class="w"> </span>pod<span class="w"> </span>pgmac-renovatebot-&lt;pod-id&gt;<span class="w"> </span>-n<span class="w"> </span>arc-runners<span class="w"> </span><span class="se">\</span>
<a id="__codelineno-15-13" name="__codelineno-15-13" href="#__codelineno-15-13"></a><span class="w">  </span>--force<span class="w"> </span>--grace-period<span class="o">=</span><span class="m">0</span><span class="w"> </span>--wait<span class="o">=</span><span class="nb">false</span>
<a id="__codelineno-15-14" name="__codelineno-15-14" href="#__codelineno-15-14"></a><span class="c1"># Repeat for all 6 stuck pods</span>
</code></pre></div>
<h4 id="17-fix-linkace-helm-chart-configuration">17. Fix LinkAce Helm Chart Configuration<a class="headerlink" href="#17-fix-linkace-helm-chart-configuration" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="c1"># Edit ArgoCD manifest to remove unsupported fields</span>
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="c1"># File: /Users/paulmacdonnell/pgmac/pgk8s/pgmac.net/media/templates/linkace.yaml</span>
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a><span class="c1"># Removed lines (backoffLimit and resources block):</span>
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="c1">#   backoffLimit: 0</span>
<a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a><span class="c1">#   resources:</span>
<a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a><span class="c1">#     limits:</span>
<a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a><span class="c1">#       memory: 512Mi</span>
<a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a><span class="c1">#       cpu: 500m</span>
<a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a><span class="c1">#     requests:</span>
<a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a><span class="c1">#       memory: 256Mi</span>
<a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a><span class="c1">#       cpu: 100m</span>
<a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a>
<a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a><span class="c1"># Kept critical timeout configuration:</span>
<a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a><span class="c1">#   startingDeadlineSeconds: 60</span>
<a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a><span class="c1">#   activeDeadlineSeconds: 300</span>
<a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a><span class="c1">#   ttlSecondsAfterFinished: 120</span>
<a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a><span class="c1">#   successfulJobsHistoryLimit: 1</span>
<a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a><span class="c1">#   failedJobsHistoryLimit: 2</span>
<a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a>
<a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a><span class="c1"># Commit changes</span>
<a id="__codelineno-16-21" name="__codelineno-16-21" href="#__codelineno-16-21"></a><span class="nb">cd</span><span class="w"> </span>/Users/paulmacdonnell/pgmac/pgk8s
<a id="__codelineno-16-22" name="__codelineno-16-22" href="#__codelineno-16-22"></a>git<span class="w"> </span>add<span class="w"> </span>pgmac.net/media/templates/linkace.yaml
<a id="__codelineno-16-23" name="__codelineno-16-23" href="#__codelineno-16-23"></a>git<span class="w"> </span>commit<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;Remove unsupported LinkAce cronjob configuration&quot;</span>
<a id="__codelineno-16-24" name="__codelineno-16-24" href="#__codelineno-16-24"></a>
<a id="__codelineno-16-25" name="__codelineno-16-25" href="#__codelineno-16-25"></a><span class="c1"># Handle git push rejection</span>
<a id="__codelineno-16-26" name="__codelineno-16-26" href="#__codelineno-16-26"></a>git<span class="w"> </span>stash
<a id="__codelineno-16-27" name="__codelineno-16-27" href="#__codelineno-16-27"></a>git<span class="w"> </span>pull<span class="w"> </span>--rebase
<a id="__codelineno-16-28" name="__codelineno-16-28" href="#__codelineno-16-28"></a>git<span class="w"> </span>stash<span class="w"> </span>pop
<a id="__codelineno-16-29" name="__codelineno-16-29" href="#__codelineno-16-29"></a>git<span class="w"> </span>push
</code></pre></div>
<h4 id="18-verification">18. Verification<a class="headerlink" href="#18-verification" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="c1"># Verify ci-tools application status</span>
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>application<span class="w"> </span>ci-tools<span class="w"> </span>-n<span class="w"> </span>argocd
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a>
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a><span class="c1"># Verify gharc-runners-pgmac-net-self-hosted (acceptable OutOfSync + Healthy)</span>
<a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>application<span class="w"> </span>gharc-runners-pgmac-net-self-hosted<span class="w"> </span>-n<span class="w"> </span>argocd
<a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a>
<a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a><span class="c1"># Verify hass application (should be Synced + Healthy)</span>
<a id="__codelineno-17-8" name="__codelineno-17-8" href="#__codelineno-17-8"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>application<span class="w"> </span>hass<span class="w"> </span>-n<span class="w"> </span>argocd
<a id="__codelineno-17-9" name="__codelineno-17-9" href="#__codelineno-17-9"></a>
<a id="__codelineno-17-10" name="__codelineno-17-10" href="#__codelineno-17-10"></a><span class="c1"># Verify linkace application (acceptable OutOfSync + Healthy)</span>
<a id="__codelineno-17-11" name="__codelineno-17-11" href="#__codelineno-17-11"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>application<span class="w"> </span>linkace<span class="w"> </span>-n<span class="w"> </span>argocd
<a id="__codelineno-17-12" name="__codelineno-17-12" href="#__codelineno-17-12"></a>
<a id="__codelineno-17-13" name="__codelineno-17-13" href="#__codelineno-17-13"></a><span class="c1"># Verify no stuck runner pods remain</span>
<a id="__codelineno-17-14" name="__codelineno-17-14" href="#__codelineno-17-14"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>arc-runners<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>Pending
<a id="__codelineno-17-15" name="__codelineno-17-15" href="#__codelineno-17-15"></a>
<a id="__codelineno-17-16" name="__codelineno-17-16" href="#__codelineno-17-16"></a><span class="c1"># Verify ArgoCD sync status</span>
<a id="__codelineno-17-17" name="__codelineno-17-17" href="#__codelineno-17-17"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>applications<span class="w"> </span>-n<span class="w"> </span>argocd<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-E<span class="w"> </span><span class="s2">&quot;OutOfSync|Progressing&quot;</span>
</code></pre></div>
<h3 id="phase-5-k8s01-container-runtime-recovery">Phase 5: k8s01 Container Runtime Recovery<a class="headerlink" href="#phase-5-k8s01-container-runtime-recovery" title="Permanent link">&para;</a></h3>
<h4 id="19-k8s01-container-runtime-investigation">19. k8s01 Container Runtime Investigation<a class="headerlink" href="#19-k8s01-container-runtime-investigation" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="c1"># List all pods in arc-runners namespace</span>
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>arc-runners
<a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>
<a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a><span class="c1"># Identified 4 Pending pods (12+ hours old):</span>
<a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a><span class="c1"># - self-hosted-l52x9-runner-2nnsr</span>
<a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a><span class="c1"># - self-hosted-l52x9-runner-69qnv</span>
<a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a><span class="c1"># - self-hosted-l52x9-runner-ls8c2</span>
<a id="__codelineno-18-8" name="__codelineno-18-8" href="#__codelineno-18-8"></a><span class="c1"># - self-hosted-l52x9-runner-w8mcd</span>
<a id="__codelineno-18-9" name="__codelineno-18-9" href="#__codelineno-18-9"></a>
<a id="__codelineno-18-10" name="__codelineno-18-10" href="#__codelineno-18-10"></a><span class="c1"># Describe pod to check status</span>
<a id="__codelineno-18-11" name="__codelineno-18-11" href="#__codelineno-18-11"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>describe<span class="w"> </span>pod<span class="w"> </span>self-hosted-l52x9-runner-2nnsr<span class="w"> </span>-n<span class="w"> </span>arc-runners
<a id="__codelineno-18-12" name="__codelineno-18-12" href="#__codelineno-18-12"></a><span class="c1"># Observed: PodScheduled=True, assigned to k8s01, no events generated</span>
<a id="__codelineno-18-13" name="__codelineno-18-13" href="#__codelineno-18-13"></a>
<a id="__codelineno-18-14" name="__codelineno-18-14" href="#__codelineno-18-14"></a><span class="c1"># Check node status</span>
<a id="__codelineno-18-15" name="__codelineno-18-15" href="#__codelineno-18-15"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>nodes
<a id="__codelineno-18-16" name="__codelineno-18-16" href="#__codelineno-18-16"></a><span class="c1"># k8s01 showing Ready status despite being unable to start containers</span>
<a id="__codelineno-18-17" name="__codelineno-18-17" href="#__codelineno-18-17"></a>
<a id="__codelineno-18-18" name="__codelineno-18-18" href="#__codelineno-18-18"></a><span class="c1"># Check pod locations</span>
<a id="__codelineno-18-19" name="__codelineno-18-19" href="#__codelineno-18-19"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>arc-runners<span class="w"> </span>-o<span class="w"> </span>wide
<a id="__codelineno-18-20" name="__codelineno-18-20" href="#__codelineno-18-20"></a><span class="c1"># All 4 Pending pods assigned to k8s01 node</span>
<a id="__codelineno-18-21" name="__codelineno-18-21" href="#__codelineno-18-21"></a>
<a id="__codelineno-18-22" name="__codelineno-18-22" href="#__codelineno-18-22"></a><span class="c1"># Check EphemeralRunner resources</span>
<a id="__codelineno-18-23" name="__codelineno-18-23" href="#__codelineno-18-23"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>ephemeralrunner<span class="w"> </span>-n<span class="w"> </span>arc-runners
<a id="__codelineno-18-24" name="__codelineno-18-24" href="#__codelineno-18-24"></a><span class="c1"># Found 10 EphemeralRunner resources but only 4 pods exist</span>
<a id="__codelineno-18-25" name="__codelineno-18-25" href="#__codelineno-18-25"></a><span class="c1"># 6 pgmac-slack-scores runners have no corresponding pods</span>
</code></pre></div>
<h4 id="20-k8s01-microk8s-restart">20. k8s01 MicroK8s Restart<a class="headerlink" href="#20-k8s01-microk8s-restart" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="c1"># On k8s01 node (user executed)</span>
<a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a>microk8s<span class="w"> </span>stop<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>microk8s<span class="w"> </span>start
<a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a>
<a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a><span class="c1"># Wait for node to return Ready</span>
<a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span><span class="nb">wait</span><span class="w"> </span>--for<span class="o">=</span><span class="nv">condition</span><span class="o">=</span>Ready<span class="w"> </span>node/k8s01<span class="w"> </span>--timeout<span class="o">=</span>300s
<a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a>
<a id="__codelineno-19-7" name="__codelineno-19-7" href="#__codelineno-19-7"></a><span class="c1"># Verify Pending pods cleared</span>
<a id="__codelineno-19-8" name="__codelineno-19-8" href="#__codelineno-19-8"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>arc-runners
<a id="__codelineno-19-9" name="__codelineno-19-9" href="#__codelineno-19-9"></a><span class="c1"># All 4 Pending pods should be gone, container runtime recovered</span>
</code></pre></div>
<h4 id="21-verification">21. Verification<a class="headerlink" href="#21-verification" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="c1"># Verify no Pending pods remain in arc-runners namespace</span>
<a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>pods<span class="w"> </span>-n<span class="w"> </span>arc-runners<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>Pending
<a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a>
<a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a><span class="c1"># Verify EphemeralRunner resources</span>
<a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>ephemeralrunner<span class="w"> </span>-n<span class="w"> </span>arc-runners
<a id="__codelineno-20-6" name="__codelineno-20-6" href="#__codelineno-20-6"></a>
<a id="__codelineno-20-7" name="__codelineno-20-7" href="#__codelineno-20-7"></a><span class="c1"># Verify k8s01 node health</span>
<a id="__codelineno-20-8" name="__codelineno-20-8" href="#__codelineno-20-8"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>describe<span class="w"> </span>node<span class="w"> </span>k8s01
<a id="__codelineno-20-9" name="__codelineno-20-9" href="#__codelineno-20-9"></a>
<a id="__codelineno-20-10" name="__codelineno-20-10" href="#__codelineno-20-10"></a><span class="c1"># Check for any new container creation issues</span>
<a id="__codelineno-20-11" name="__codelineno-20-11" href="#__codelineno-20-11"></a>kubectl<span class="w"> </span>--context<span class="w"> </span>pvek8s<span class="w"> </span>get<span class="w"> </span>events<span class="w"> </span>-n<span class="w"> </span>arc-runners<span class="w"> </span>--sort-by<span class="o">=</span><span class="s1">&#39;.lastTimestamp&#39;</span>
</code></pre></div>
<hr />
<h2 id="verification">Verification<a class="headerlink" href="#verification" title="Permanent link">&para;</a></h2>
<h3 id="service-health-checks">Service Health Checks<a class="headerlink" href="#service-health-checks" title="Permanent link">&para;</a></h3>
<ul>
<li> Sonarr: 200 OK responses, RSS sync operational</li>
<li> Radarr: 200 OK responses, web UI accessible</li>
<li> Overseerr: 200 OK responses, login page loading</li>
</ul>
<h3 id="infrastructure-health">Infrastructure Health<a class="headerlink" href="#infrastructure-health" title="Permanent link">&para;</a></h3>
<ul>
<li> All 3 nodes (k8s01, k8s02, k8s03): Ready status</li>
<li> Jiva replicas: 39/40 Running (effectively 100%)</li>
<li> No Pending, CrashLoop, or Error pods cluster-wide (excluding residual runner pod cleanup)</li>
<li> Ingress controllers: Routing to correct pod IPs</li>
<li> Kubelet stable on all nodes: No repeated crashes or hangs</li>
</ul>
<h3 id="volume-replication">Volume Replication<a class="headerlink" href="#volume-replication" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a>Overseerr:  3/3 replicas Running
<a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a>Sonarr:     3/3 replicas Running
<a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a>Radarr:     3/3 replicas Running
<a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a>All others: 3/3 replicas Running
</code></pre></div>
<h3 id="node-disk-status-post-cleanup">Node Disk Status (Post-Cleanup)<a class="headerlink" href="#node-disk-status-post-cleanup" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a>k8s01: 87% (down from 97%)
<a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a>k8s02: Stable (no initial disk pressure)
<a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a>k8s03: 81% (down from 100%)
</code></pre></div>
<hr />
<h2 id="preventive-measures">Preventive Measures<a class="headerlink" href="#preventive-measures" title="Permanent link">&para;</a></h2>
<h3 id="immediate-actions-required">Immediate Actions Required<a class="headerlink" href="#immediate-actions-required" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Implement Node Disk Space Monitoring</strong> (Critical Priority)</li>
<li>Current: No alerts for disk usage &gt;85%</li>
<li>Target: Alert at 80%, critical alert at 90%</li>
<li>Actions:<ul>
<li>Deploy Prometheus node-exporter on all nodes</li>
<li>Configure AlertManager rules for disk pressure</li>
<li>Add Nagios checks for disk usage as backup</li>
</ul>
</li>
<li>
<p><strong>Rationale</strong>: Both k8s01 (97%) and k8s03 (100%) hit critical thresholds without detection</p>
</li>
<li>
<p><strong>Automated Container Image Garbage Collection</strong> (High Priority)</p>
</li>
<li>Current: Manual cleanup required during incident</li>
<li>Target: Automated daily cleanup maintaining &lt;75% disk usage</li>
<li>Actions:<ul>
<li>Configure kubelet <code>imageGCHighThresholdPercent=75</code> (default: 85)</li>
<li>Configure kubelet <code>imageGCLowThresholdPercent=70</code> (default: 80)</li>
<li>Schedule weekly cleanup cronjob as backup</li>
</ul>
</li>
<li>
<p><strong>Rationale</strong>: 4+ years of accumulated images contributed to disk exhaustion</p>
</li>
<li>
<p><strong>Audit Log Rotation and Buffer Management</strong> (High Priority)</p>
</li>
<li>Current: Audit buffer overload caused kubelet crashes on k8s01</li>
<li>Actions:<ul>
<li>Reduce audit log verbosity (current level generating excessive data)</li>
<li>Implement aggressive log rotation (hourly vs daily)</li>
<li>Configure audit buffer size limits</li>
<li>Consider disabling detailed audit logging for non-critical operations</li>
</ul>
</li>
<li>
<p><strong>Rationale</strong>: "audit buffer queue blocked" directly caused kubelet instability</p>
</li>
<li>
<p><strong>Radarr PVC Expansion</strong> (High Priority)</p>
</li>
<li>Current: 1Gi volume at 95% capacity (carried over from Phase 2)</li>
<li>Target: 2Gi to accommodate media artwork growth</li>
<li>Action: Requires PVC recreation (Jiva doesn't support online expansion)</li>
<li>
<p>Steps:
     <div class="highlight"><pre><span></span><code><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a><span class="c1"># 1. Backup Radarr config</span>
<a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a><span class="c1"># 2. Create new 2Gi PVC</span>
<a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a><span class="c1"># 3. Restore data</span>
<a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a><span class="c1"># 4. Update deployment to use new PVC</span>
</code></pre></div></p>
</li>
<li>
<p><strong>Jiva Snapshot Cleanup Frequency</strong> (High Priority)</p>
</li>
<li>Current: Daily at 2 AM (threshold: 500 snapshots)</li>
<li>Problem: 1011 snapshots accumulated when cronjob couldn't run during Phase 1</li>
<li>Actions:<ul>
<li>Lower threshold from 500 to 300 snapshots</li>
<li>Increase frequency to every 12 hours (2 AM and 2 PM)</li>
<li>Add monitoring/alerting for snapshot counts &gt;400</li>
<li>Add pod anti-affinity to ensure cleanup job can run on healthy nodes</li>
</ul>
</li>
<li>
<p><strong>Rationale</strong>: Cronjob failure during Phase 1 directly caused Phase 2 storage issues</p>
</li>
<li>
<p><strong>GitHub Actions Runner Controller Migration</strong> (Medium Priority)</p>
</li>
<li>Current: Orphaned runner pods consumed significant resources</li>
<li>Actions:<ul>
<li>Migrate to GitHub-hosted runners or alternative self-hosted solution</li>
<li>If keeping self-hosted: implement strict <code>maxReplicas</code> limits</li>
<li>Add PodDisruptionBudgets to prevent runaway scaling</li>
<li>Configure aggressive pod cleanup policies</li>
</ul>
</li>
<li>
<p><strong>Rationale</strong>: 571 orphaned pods significantly contributed to cluster instability</p>
</li>
<li>
<p><strong>CronJob Timeout Configuration Baseline</strong> (High Priority - Added from Phase 3)</p>
</li>
<li>Current: CronJobs created without timeout settings, allowing infinite hangs</li>
<li>Target: All cronjobs have defensive timeout configuration</li>
<li>Actions:<ul>
<li>Create baseline cronjob template with standard timeouts:</li>
<li><code>startingDeadlineSeconds: 60</code> (for minute-frequency jobs)</li>
<li><code>activeDeadlineSeconds: &lt;appropriate for task&gt;</code> (e.g., 300 for 5-min tasks)</li>
<li><code>ttlSecondsAfterFinished: 120</code> (2-minute cleanup)</li>
<li><code>successfulJobsHistoryLimit: 1</code></li>
<li><code>failedJobsHistoryLimit: 2</code></li>
<li>Audit all existing cronjobs and add timeout configuration</li>
<li>Add validation in ArgoCD to require timeout settings</li>
</ul>
</li>
<li>
<p><strong>Rationale</strong>: Timeout settings proved critical for self-healing, but were missing</p>
</li>
<li>
<p><strong>Job Controller Health Monitoring</strong> (Critical Priority - Added from Phase 3)</p>
</li>
<li>Current: No monitoring for job controller state or corruption</li>
<li>Actions:<ul>
<li>Add synthetic job creation tests every 5 minutes cluster-wide</li>
<li>Monitor job controller logs for "not found" errors</li>
<li>Alert on jobs with 0 pods after 2 minutes</li>
<li>Alert on jobs exceeding activeDeadlineSeconds without termination</li>
<li>Monitor dqlite database health and replication lag</li>
</ul>
</li>
<li>
<p><strong>Rationale</strong>: Job controller corruption went undetected for 16+ hours</p>
</li>
<li>
<p><strong>Dqlite Database Backup Automation</strong> (High Priority - Added from Phase 3)</p>
</li>
<li>Current: Manual backup procedures only</li>
<li>Target: Automated hourly backups with 24-hour retention</li>
<li>Actions:<ul>
<li>Create cronjob to backup dqlite database (requires node-local execution)</li>
<li>Store backups on NFS with rotation policy</li>
<li>Document and test restoration procedure</li>
<li>Add alerts for backup failures</li>
</ul>
</li>
<li><strong>Rationale</strong>: Database backup was critical for nuclear option confidence</li>
</ol>
<h3 id="longer-term-improvements">Longer-Term Improvements<a class="headerlink" href="#longer-term-improvements" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p><strong>Node Health Synthetic Testing</strong> (High Priority)</p>
</li>
<li>
<p>Issue: Nodes reported Ready but couldn't start containers</p>
</li>
<li>Actions:</li>
<li>Deploy synthetic pod startup tests every 5 minutes on each node</li>
<li>Monitor microk8s/kubelite service health</li>
<li>Alert on pod startup failures or extended ContainerCreating states</li>
<li>Consider scheduled microk8s service restarts (monthly maintenance)</li>
<li>
<p><strong>Rationale</strong>: k8s02 and k8s03 both showed "Ready" status while unable to start pods</p>
</li>
<li>
<p><strong>Dqlite State Recovery Procedures</strong> (Medium Priority - Updated from Phase 3)</p>
</li>
<li>
<p>Issue: Dqlite state corruption from Phase 1 required Phase 3 nuclear option</p>
</li>
<li>Actions:</li>
<li><strong>COMPLETED</strong>: Automated dqlite database backups documented (see item 9)</li>
<li>Document nuclear option recovery procedure (cluster restart with backup)</li>
<li>Test backup restoration in non-production scenario</li>
<li>Add monitoring for dqlite replication lag between nodes</li>
<li>Consider scheduled preventive cluster restarts (quarterly maintenance)</li>
<li>
<p><strong>Rationale</strong>: Dqlite corruption from Phase 1 persisted for 48+ hours, required nuclear option</p>
</li>
<li>
<p><strong>Node Reboot Resilience Testing</strong> (Medium Priority)</p>
</li>
<li>
<p>Issue: Simultaneous node reboots triggered cascading failures</p>
</li>
<li>Actions:</li>
<li>Implement controlled rolling node restarts (monthly maintenance)</li>
<li>Document graceful node shutdown/startup procedures</li>
<li>Test simultaneous 2-node failure scenarios</li>
<li>Add PodDisruptionBudgets for critical services</li>
<li>Configure proper pod anti-affinity for redundant services</li>
<li>
<p><strong>Rationale</strong>: Inability to handle simultaneous reboots indicates insufficient resilience</p>
</li>
<li>
<p><strong>Ingress Endpoint Monitoring</strong> (Medium Priority)</p>
<ul>
<li>Add monitoring to detect stale endpoint caching</li>
<li>Alert on pod IP changes not reflected in ingress logs</li>
<li>Consider automated ingress controller restarts after pod migrations</li>
</ul>
</li>
<li>
<p><strong>Volume Capacity Monitoring</strong> (High Priority)</p>
<ul>
<li>Implement alerts for PVC usage &gt;85%</li>
<li>Current gap: No visibility into Jiva volume capacity</li>
<li>Tool: Consider deploying Prometheus with node-exporter + custom Jiva metrics</li>
</ul>
</li>
<li>
<p><strong>Snapshot Management Strategy</strong> (Medium Priority)</p>
<ul>
<li>Investigate snapshot growth rate per volume</li>
<li>Document expected snapshot accumulation patterns</li>
<li>Consider application-specific snapshot retention policies</li>
<li>Evaluate if 3-replica Jiva setup is necessary (vs 2-replica for non-critical data)</li>
</ul>
</li>
<li>
<p><strong>MediaCover Cleanup Automation</strong> (Low Priority)</p>
<ul>
<li>Radarr MediaCover directory: 837M of 974M total</li>
<li>Implement periodic cleanup of orphaned/old media artwork</li>
<li>Consider storing media artwork on NFS instead of Jiva volumes</li>
</ul>
</li>
<li>
<p><strong>Runbook Documentation</strong> (High Priority - Updated from Phase 3)</p>
<ul>
<li>Document kubelet/kubelite restart procedures for all nodes</li>
<li>Document disk cleanup emergency procedures with target thresholds</li>
<li>Document Jiva snapshot cleanup manual trigger process</li>
<li>Document ingress controller restart for endpoint refresh</li>
<li>Document force-deletion procedures for stuck pods</li>
<li><strong>NEW</strong>: Document nuclear option procedures (cluster restart with dqlite backup)</li>
<li><strong>NEW</strong>: Document job controller corruption recovery steps</li>
<li><strong>NEW</strong>: Document self-healing verification checklist</li>
<li>Add to on-call playbook with estimated recovery times</li>
<li><strong>Rationale</strong>: Multiple manual interventions required across all 3 phases; procedures must be documented</li>
<li><strong>Reference</strong>: <code>/tmp/linkace-cronjob-nuclear-option.md</code> created during Phase 3</li>
</ul>
</li>
<li>
<p><strong>Cluster Architecture Review</strong> (Low Priority)</p>
<ul>
<li>Current: 4+ year old microk8s installation</li>
<li>Consider: Upgrade path to newer Kubernetes versions</li>
<li>Evaluate: Migration to managed Kubernetes (EKS, GKE, AKS) or alternative distributions</li>
<li><strong>Rationale</strong>: Age of installation may contribute to accumulated technical debt</li>
</ul>
</li>
</ol>
<hr />
<h2 id="lessons-learned">Lessons Learned<a class="headerlink" href="#lessons-learned" title="Permanent link">&para;</a></h2>
<h3 id="what-went-well">What Went Well<a class="headerlink" href="#what-went-well" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Systematic troubleshooting approach</strong>: Correctly identified kubelet issues as separate from scheduler problems</li>
<li><strong>Node cordoning strategy</strong>: Temporarily removing k8s02 from rotation helped isolate the problem</li>
<li><strong>Diagnostic tools worked effectively</strong>: <code>kubectl</code> commands, <code>journalctl</code>, and custom scripts like <code>check-jiva-volumes.py</code> provided crucial insights</li>
<li><strong>Modular architecture</strong>: Issues isolated to specific components, preventing total cluster failure</li>
<li><strong>Quick node recovery</strong>: microk8s restarts resolved kubelet issues within 1-2 minutes</li>
<li><strong>Automated cleanup existed</strong>: Jiva snapshot cleanup cronjob was already in place, just needed manual trigger</li>
<li><strong>Full replication</strong>: Jiva 3-replica setup meant volumes remained accessible with 2/3 replicas during issue</li>
<li><strong>Force-deletion strategy</strong>: Successfully cleared 546+ orphaned pods using batched force-delete commands</li>
<li><strong>Phase 3 - Timeout configuration added proactively</strong>: ArgoCD manifest updated with defensive timeout settings before nuclear option</li>
<li><strong>Phase 3 - Database backup procedures</strong>: Successfully backed up dqlite database before nuclear option, providing rollback capability</li>
<li><strong>Phase 3 - Nuclear option executed cleanly</strong>: Cluster restart resolved all issues within 15 minutes with zero data loss</li>
<li><strong>Phase 3 - Verification thoroughness</strong>: Systematic verification of job creation, completion, TTL cleanup, and pod lifecycle</li>
</ol>
<h3 id="what-didnt-go-well">What Didn't Go Well<a class="headerlink" href="#what-didnt-go-well" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Cascading failure propagation</strong>: Initial node reboot triggered multiple secondary failures across all infrastructure layers</li>
<li><strong>No proactive monitoring</strong>: Disk usage (97%, 100%) and snapshot accumulation (1011) went undetected</li>
<li><strong>Kubelet instability</strong>: Disk pressure caused repeated kubelet crashes without clear error messages in pod status</li>
<li><strong>Database state corruption</strong>: Dqlite database corruption persisted for 48+ hours, spanning Phase 1  Phase 3</li>
<li><strong>Manual intervention required</strong>: Multiple manual steps needed across 8-hour (Phase 1-2) + 16.5-hour (Phase 3) + 12+ hour (Phase 5) periods vs automated recovery</li>
<li><strong>Long cleanup duration</strong>: 60+ minutes for snapshot cleanup job to process all volumes</li>
<li><strong>Ingress endpoint caching</strong>: No automatic detection/refresh of stale endpoints</li>
<li><strong>Runner controller orphaned pods</strong>: 571 pods remained despite controller scaled to 0</li>
<li><strong>Capacity planning gap</strong>: Radarr volume undersized for actual usage patterns</li>
<li><strong>Node Ready status misleading</strong>: Nodes reported Ready but couldn't start containers (kubelet vs containerd state mismatch)</li>
<li><strong>Cronjob failure during node issues</strong>: Snapshot cleanup cronjob couldn't run during Phase 1, directly causing Phase 2 storage issues</li>
<li><strong>Phase 3 - Self-healing complete failure</strong>: Waited 1.5 hours for timeout-based self-healing that never occurred</li>
<li><strong>Phase 3 - Job controller corruption went undetected</strong>: 16+ hours of cronjob failures without alerting</li>
<li><strong>Phase 3 - Controller restarts ineffective</strong>: Multiple kubelite restarts across all nodes failed to clear corruption</li>
<li><strong>Phase 3 - ArgoCD auto-sync failed</strong>: GitOps automation failed when resources were deleted for clean state</li>
<li><strong>Phase 3 - No job controller monitoring</strong>: Zero visibility into controller state or processing errors</li>
<li><strong>Phase 5 - Nuclear option insufficient</strong>: Cluster-wide restart (Phase 3) didn't clear node-local container runtime corruption on k8s01</li>
<li><strong>Phase 5 - Silent failure undetected</strong>: 12+ hour delay in detecting Pending pods with no container initialization</li>
<li><strong>Phase 5 - No node-local runtime monitoring</strong>: Zero visibility into container runtime health vs kubelet health</li>
</ol>
<h3 id="surprise-findings">Surprise Findings<a class="headerlink" href="#surprise-findings" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Audit buffer overload</strong>: Audit logging directly caused kubelet crashes (not commonly documented failure mode)</li>
<li><strong>Dqlite database corruption persistence</strong>: Database corruption from Phase 1 persisted for 48+ hours despite multiple controller restarts</li>
<li><strong>Kubelet crash without pod warnings</strong>: Pods showed "Pending" with no indication kubelet was crashing</li>
<li><strong>Disk threshold</strong>: 97% disk usage was sufficient to crash kubelet despite &gt;3% free space</li>
<li><strong>Runner pod accumulation</strong>: 571 pods accumulated without triggering any resource quota or alerts</li>
<li><strong>Snapshot physical storage</strong>: 1011 snapshots consumed 3GB physical space in 1Gi logical volume</li>
<li><strong>Media artwork growth</strong>: Radarr artwork (837M) exceeded database size (33M) by 25x</li>
<li><strong>Cleanup job thoroughness</strong>: Job processed ALL Jiva volumes, not just over-threshold volumes</li>
<li><strong>Cross-phase dependency</strong>: Phase 1 kubelet/disk issues directly prevented Phase 2 cronjobs from running, and Phase 1 database corruption caused Phase 3 job controller failures</li>
<li><strong>Phase 3 - Job controller single point of failure</strong>: Single corrupted job reference prevented ALL job creation cluster-wide</li>
<li><strong>Phase 3 - Timeout settings ignored</strong>: Properly configured activeDeadlineSeconds and ttlSecondsAfterFinished completely ignored by corrupted controller</li>
<li><strong>Phase 3 - Controller restart insufficient</strong>: Restarting kubelite service didn't clear in-memory controller state</li>
<li><strong>Phase 3 - Nuclear option effectiveness</strong>: Full cluster restart immediately resolved all controller corruption issues</li>
<li><strong>Phase 3 - Self-healing timeline invalid</strong>: Expected 6-12 hour self-healing never occurred; corruption was permanent without intervention</li>
<li><strong>Phase 5 - Nuclear option scope limitation</strong>: Cluster restart cleared cluster-global state (dqlite, controllers) but not node-local container runtime corruption</li>
<li><strong>Phase 5 - Corruption dormancy</strong>: Container runtime corruption from Phase 1 remained dormant for 48+ hours until new workloads attempted to schedule on k8s01</li>
<li><strong>Phase 5 - Silent failure persistence</strong>: Same silent failure pattern from Phase 2 (PodScheduled=True, no events) persisted despite Phase 3 nuclear option</li>
</ol>
<hr />
<h2 id="action-items">Action Items<a class="headerlink" href="#action-items" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Priority</th>
<th>Action</th>
<th>Owner</th>
<th>Due Date</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Critical</td>
<td>Deploy node disk space monitoring with alerts (80%/90% thresholds)</td>
<td>SRE</td>
<td>2026-01-08</td>
<td>Open</td>
</tr>
<tr>
<td>Critical</td>
<td>Configure automated container image garbage collection (75% threshold)</td>
<td>SRE</td>
<td>2026-01-09</td>
<td>Open</td>
</tr>
<tr>
<td>Critical</td>
<td>Implement job controller health monitoring with synthetic tests</td>
<td>SRE</td>
<td>2026-01-09</td>
<td>Open</td>
</tr>
<tr>
<td>High</td>
<td>Implement audit log rotation and reduce verbosity</td>
<td>SRE</td>
<td>2026-01-10</td>
<td>Open</td>
</tr>
<tr>
<td>High</td>
<td>Expand Radarr PVC from 1Gi to 2Gi</td>
<td>SRE</td>
<td>2026-01-13</td>
<td>Open</td>
</tr>
<tr>
<td>High</td>
<td>Lower snapshot threshold to 300, increase cleanup frequency to 12h</td>
<td>SRE</td>
<td>2026-01-08</td>
<td>Open</td>
</tr>
<tr>
<td>High</td>
<td>Audit all cronjobs and add timeout configuration baseline</td>
<td>SRE</td>
<td>2026-01-15</td>
<td>Open</td>
</tr>
<tr>
<td>High</td>
<td>Implement automated dqlite database backups (hourly, 24h retention)</td>
<td>SRE</td>
<td>2026-01-10</td>
<td>Open</td>
</tr>
<tr>
<td>High</td>
<td>Document nuclear option runbook (cluster restart with dqlite backup)</td>
<td>SRE</td>
<td>2026-01-12</td>
<td>Open</td>
</tr>
<tr>
<td>High</td>
<td>Implement synthetic pod startup health checks on all nodes</td>
<td>SRE</td>
<td>2026-01-15</td>
<td>Open</td>
</tr>
<tr>
<td>High</td>
<td>Add PVC capacity monitoring and alerting (&gt;85%)</td>
<td>SRE</td>
<td>2026-01-20</td>
<td>Open</td>
</tr>
<tr>
<td>Medium</td>
<td>Test dqlite backup restoration in non-production scenario</td>
<td>SRE</td>
<td>2026-01-17</td>
<td>Open</td>
</tr>
<tr>
<td>Medium</td>
<td>Add dqlite replication lag monitoring</td>
<td>SRE</td>
<td>2026-01-20</td>
<td>Open</td>
</tr>
<tr>
<td>Medium</td>
<td>Migrate GitHub Actions to hosted runners or implement strict limits</td>
<td>SRE</td>
<td>2026-01-27</td>
<td>Open</td>
</tr>
<tr>
<td>Medium</td>
<td>Test node reboot resilience with controlled failures</td>
<td>SRE</td>
<td>2026-02-03</td>
<td>Open</td>
</tr>
<tr>
<td>Medium</td>
<td>Investigate k8s01/k8s03 kubelet/containerd logs from incident</td>
<td>SRE</td>
<td>2026-01-13</td>
<td>Open</td>
</tr>
<tr>
<td>Medium</td>
<td>Add ingress endpoint staleness monitoring</td>
<td>SRE</td>
<td>2026-02-10</td>
<td>Open</td>
</tr>
<tr>
<td>Medium</td>
<td>Investigate ArgoCD auto-sync failure for deleted resources</td>
<td>SRE</td>
<td>2026-01-20</td>
<td>Open</td>
</tr>
<tr>
<td>Low</td>
<td>Implement Radarr MediaCover cleanup automation</td>
<td>Dev</td>
<td>2026-02-03</td>
<td>Open</td>
</tr>
<tr>
<td>Low</td>
<td>Evaluate reducing Jiva replication from 3 to 2 for non-critical data</td>
<td>SRE</td>
<td>2026-02-10</td>
<td>Open</td>
</tr>
<tr>
<td>Low</td>
<td>Review cluster architecture and upgrade path</td>
<td>SRE</td>
<td>2026-03-01</td>
<td>Open</td>
</tr>
<tr>
<td>Low</td>
<td>Consider scheduled preventive cluster restarts (quarterly)</td>
<td>SRE</td>
<td>2026-03-01</td>
<td>Open</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="technical-details">Technical Details<a class="headerlink" href="#technical-details" title="Permanent link">&para;</a></h2>
<h3 id="environment">Environment<a class="headerlink" href="#environment" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Cluster</strong>: microk8s on 3 nodes (k8s01, k8s02, k8s03)</li>
<li><strong>Kubernetes Version</strong>: v1.34.3</li>
<li><strong>Node Age</strong>: 4 years 138 days</li>
<li><strong>Storage</strong>: OpenEBS Jiva 2.12.1 (openebs-jiva-default storage class)</li>
<li><strong>Ingress</strong>: nginx-ingress-microk8s-controller (3 replicas)</li>
<li><strong>Network</strong>: Calico CNI</li>
<li><strong>etcd Alternative</strong>: kine (microk8s default)</li>
<li><strong>Container Runtime</strong>: containerd via microk8s</li>
</ul>
<h3 id="affected-resources">Affected Resources<a class="headerlink" href="#affected-resources" title="Permanent link">&para;</a></h3>
<p><strong>Phase 1:</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a><span class="nt">Namespaces</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ci, openebs, kube-system, all namespaces (scheduler impact)</span>
<a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a><span class="nt">Nodes</span><span class="p">:</span>
<a id="__codelineno-24-3" name="__codelineno-24-3" href="#__codelineno-24-3"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">k8s01</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Kubelet crash loop (disk 97% + audit buffer overload)</span>
<a id="__codelineno-24-4" name="__codelineno-24-4" href="#__codelineno-24-4"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">k8s02</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Kubelet hung (process restart required)</span>
<a id="__codelineno-24-5" name="__codelineno-24-5" href="#__codelineno-24-5"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">k8s03</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Disk 100% full (garbage collection failure)</span>
<a id="__codelineno-24-6" name="__codelineno-24-6" href="#__codelineno-24-6"></a><span class="nt">Pods</span><span class="p">:</span>
<a id="__codelineno-24-7" name="__codelineno-24-7" href="#__codelineno-24-7"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">GitHub Actions runners</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">571 orphaned (299 Pending, 110 ContainerStatusUnknown, 79 Completed, 58 StartError, 25 other)</span>
<a id="__codelineno-24-8" name="__codelineno-24-8" href="#__codelineno-24-8"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">OpenEBS replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">22 stuck Terminating</span>
<a id="__codelineno-24-9" name="__codelineno-24-9" href="#__codelineno-24-9"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">Various</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Unable to start/stop across all namespaces</span>
</code></pre></div>
<p><strong>Phase 2:</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a><span class="nt">Namespaces</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">media, openebs, ingress</span>
<a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a><span class="nt">Pods</span><span class="p">:</span>
<a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sonarr-7b8f6fcfc4-4wm8m (Pending  Running)</span>
<a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">radarr-5c95c64cff-* (CrashLoopBackOff  Running, multiple restarts)</span>
<a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">overseerr-58cc7d4569-kllz2 (Running, intermittent timeouts)</span>
<a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a><span class="nt">PVCs</span><span class="p">:</span>
<a id="__codelineno-25-7" name="__codelineno-25-7" href="#__codelineno-25-7"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">radarr-config (pvc-311bef00..., 1Gi, 100% full  95% after cleanup)</span>
<a id="__codelineno-25-8" name="__codelineno-25-8" href="#__codelineno-25-8"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">sonarr-config (pvc-17e6e808..., 1Gi, 1011 snapshots)</span>
<a id="__codelineno-25-9" name="__codelineno-25-9" href="#__codelineno-25-9"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">overseerr-config (pvc-05e03b60..., 1Gi, 1011 snapshots)</span>
</code></pre></div>
<h3 id="snapshot-cleanup-job-output">Snapshot Cleanup Job Output<a class="headerlink" href="#snapshot-cleanup-job-output" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a>Volumes processed: 13
<a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a>Volumes cleaned: 13
<a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a>Snapshots consolidated: 1011  ~100 per volume (estimated)
<a id="__codelineno-26-4" name="__codelineno-26-4" href="#__codelineno-26-4"></a>Duration: ~60 minutes
<a id="__codelineno-26-5" name="__codelineno-26-5" href="#__codelineno-26-5"></a>Method: Rolling restart of replicas (3 per volume, 30s stabilization between)
</code></pre></div>
<h3 id="node-disk-usage-timeline">Node Disk Usage Timeline<a class="headerlink" href="#node-disk-usage-timeline" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a>k8s01: 97% (critical)  87% (stable) after cleanup
<a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a>k8s02: Stable throughout (no disk pressure)
<a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a>k8s03: 100% (critical)  81% (stable) after cleanup
</code></pre></div>
<h3 id="kubelet-error-patterns-phase-1">Kubelet Error Patterns (Phase 1)<a class="headerlink" href="#kubelet-error-patterns-phase-1" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a>k8s01 errors:
<a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a>- &quot;audit buffer queue blocked&quot;
<a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a>- &quot;database is locked&quot; (kine)
<a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a>- &quot;Failed to garbage collect required amount of images&quot;
<a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a>- &quot;Kubelet stopped posting node status&quot;
<a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a>
<a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a>k8s02 errors:
<a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a>- Pods assigned but never reached ContainerCreating (silent failure)
<a id="__codelineno-28-9" name="__codelineno-28-9" href="#__codelineno-28-9"></a>
<a id="__codelineno-28-10" name="__codelineno-28-10" href="#__codelineno-28-10"></a>k8s03 errors:
<a id="__codelineno-28-11" name="__codelineno-28-11" href="#__codelineno-28-11"></a>- &quot;Failed to garbage collect required amount of images. Attempted to free 13GB, but only found 0 bytes eligible to free&quot;
</code></pre></div>
<hr />
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<ul>
<li>Jiva Volume Checker Script: <code>/Users/paulmacdonnell/pgmac/check-jiva-volumes.py</code></li>
<li>Snapshot Cleanup Config: <code>/Users/paulmacdonnell/pgmac/pgk8s/pgmac.net/system/templates/jiva-snapshot-cleanup.yaml</code></li>
<li>Cleanup Job Logs: <code>kubectl logs -n openebs jiva-snapshot-cleanup-manual-4tv5c</code></li>
<li>OpenEBS Jiva Documentation: https://openebs.io/docs/user-guides/jiva</li>
<li>microk8s Documentation: https://microk8s.io/docs</li>
<li>Kubernetes Kubelet Configuration: https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/</li>
</ul>
<hr />
<h2 id="reviewers">Reviewers<a class="headerlink" href="#reviewers" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Prepared by</strong>: Claude (AI Assistant)</li>
<li><strong>Date</strong>: 2026-01-06</li>
<li><strong>Review Status</strong>: Draft - Pending human review</li>
</ul>
<hr />
<h2 id="notes">Notes<a class="headerlink" href="#notes" title="Permanent link">&para;</a></h2>
<p>This incident demonstrated the fragility of a long-running Kubernetes cluster under cascading failure conditions across five distinct phases spanning 2026-01-06 to 2026-01-09. Key takeaways:</p>
<h3 id="cross-phase-insights">Cross-Phase Insights<a class="headerlink" href="#cross-phase-insights" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Disk pressure is a critical failure mode</strong>: Both 97% and 100% disk usage caused complete kubelet failure, not just degraded performance</li>
<li><strong>Audit logging can become a liability</strong>: Excessive audit log generation directly caused kubelet crashes via buffer overload</li>
<li><strong>Node "Ready" status is insufficient</strong>: Nodes reported Ready while unable to start containers (kubelet vs containerd state mismatch)</li>
<li><strong>Cascading failures span days, not hours</strong>: Initial Phase 1 node reboot  disk pressure  kubelet failures  dqlite corruption  <strong>48 hours later</strong>  Phase 3 job controller corruption</li>
<li><strong>Automated cleanup jobs are single points of failure</strong>: Snapshot cleanup cronjob failure during Phase 1 directly caused Phase 2 storage issues</li>
<li><strong>Orphaned pods accumulate silently</strong>: 571 runner pods accumulated over time without triggering resource quotas or alerts</li>
<li><strong>Force-deletion is sometimes necessary</strong>: Normal deletion failed for 546+ pods due to finalizer/controller corruption</li>
<li><strong>Database state corruption is persistent</strong>: Dqlite corruption persisted for 48+ hours despite multiple controller restarts</li>
<li><strong>Multiple layers require monitoring</strong>: Node health, disk space, kubelet status, pod lifecycle, storage subsystem, ingress endpoints, <strong>controller state</strong></li>
<li><strong>Age matters</strong>: 4+ year old installation accumulated technical debt (images, logs, state corruption)</li>
</ol>
<h3 id="phase-3-specific-insights-job-controller-corruption">Phase 3-Specific Insights (Job Controller Corruption)<a class="headerlink" href="#phase-3-specific-insights-job-controller-corruption" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Controller corruption is catastrophic</strong>: Single corrupted job reference prevented ALL job creation cluster-wide</li>
<li><strong>Service restarts don't clear all state</strong>: Restarting kubelite service didn't clear in-memory controller state or dqlite database corruption</li>
<li><strong>Self-healing has limits</strong>: Properly configured timeout settings (activeDeadlineSeconds, ttlSecondsAfterFinished) were completely ignored by corrupted controller</li>
<li><strong>Nuclear option is sometimes necessary</strong>: Full cluster restart with database backup was the only effective recovery path</li>
<li><strong>Timeout configuration is defensive, not curative</strong>: Timeout settings prevent runaway resource consumption but don't fix controller corruption</li>
<li><strong>Job controller is a single point of failure</strong>: No redundancy or failover mechanism for corrupted job controller state</li>
<li><strong>GitOps auto-sync can fail</strong>: ArgoCD auto-sync failed when resources deleted for clean state, requiring manual intervention</li>
<li><strong>Database backups provide confidence</strong>: Having dqlite backup before nuclear option provided rollback capability and reduced risk</li>
<li><strong>Verification is critical</strong>: Systematic verification of job lifecycle (creation  pod spawn  completion  TTL cleanup) necessary after controller recovery</li>
<li><strong>Controller monitoring is essential</strong>: Zero visibility into job controller processing state delayed detection by 16+ hours</li>
</ol>
<p>The resolution required comprehensive intervention across all infrastructure layers (compute, storage, networking, control plane, <strong>database</strong>) demonstrating the interconnected nature of Kubernetes cluster health and the importance of:</p>
<ul>
<li>Proactive monitoring at multiple levels (nodes, controllers, database)</li>
<li>Automated maintenance and cleanup with defensive timeout configuration</li>
<li>Graceful degradation under failure (with acknowledgment of hard limits)</li>
<li>Clear runbooks for manual intervention <strong>including nuclear option procedures</strong></li>
<li>Regular resilience testing <strong>including controller corruption scenarios</strong></li>
<li>Capacity planning and right-sizing</li>
<li>Understanding failure cascades and dependencies between systems <strong>across multi-day timespans</strong></li>
<li>Database backup and recovery procedures for confidence in drastic recovery actions</li>
<li>Controller state monitoring and synthetic testing</li>
<li>Recognition that some failures require full cluster restart to resolve</li>
</ul>
<p>Future incidents can be prevented or mitigated through the preventive measures outlined above, particularly:</p>
<ul>
<li><strong>Phase 1 preventions</strong>: Disk space monitoring, automated image cleanup, audit log management</li>
<li><strong>Phase 2 preventions</strong>: Comprehensive health checks beyond node "Ready" status, cronjob reliability monitoring</li>
<li><strong>Phase 3 preventions</strong>: Job controller health monitoring, synthetic job tests, dqlite backup automation, cronjob timeout baselines, nuclear option documentation</li>
</ul>
<p>The three-phase nature of this incident (spanning 48+ hours) highlights that cascading failures can have <strong>long-term delayed effects</strong> requiring sustained vigilance and multiple recovery strategies beyond initial stabilization.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/pgmac" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.code.annotate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>